{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff962462",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Save target-pathway lists as spark dfs and filter out non-gene targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf81c38",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "Input folder: Pathwaganda/data/GSEA_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "796fbd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "import shutil\n",
    "from pyspark.sql.functions import split, explode, collect_list, col, concat_ws, input_file_name, regexp_extract, lit\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e640365a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/21 15:37:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/08/21 15:37:43 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81a99f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = spark.read.parquet(\"/Users/polina/Pathwaganda/data/GSEA_output/Reactome_Pathways_2025_diy/diseaseId=EFO_0000094\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f99cf090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------------------+-------------+-------------------+------------------+---------------------+-------------------+-------------------+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Term                                                                      |ID           |es                 |nes               |pval                 |sidak              |fdr                |geneset_size|leading_edge                                                                                                                                                                                   |propagated_edge                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |\n",
      "+--------------------------------------------------------------------------+-------------+-------------------+------------------+---------------------+-------------------+-------------------+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|Transcriptional regulation by RUNX1                                       |R-HSA-8878171|0.4249397679931725 |3.727270774578297 |1.93564470299723E-4  |0.11584148713126946|0.12310700311062384|40          |GATA3,SMARCB1,TAL1,MYB,ESR1,AXIN1,TCF3,CREBBP,PTPN11,SMARCA4,KMT2A,ARID1A,KMT2C,PAX5,RUNX1,ARID2,ARID1B,CCND3                                                                                  |ACTL6A,ACTL6B,ADRM1,AGO1,AGO2,AGO3,AGO4,ARID1A,ARID1B,ARID2,ASH2L,AUTS2,AXIN1,BLK,BMI1,CBFB,CBX2,CBX4,CBX6,CBX8,CCND1,CCND2,CCND3,CCNH,CDK6,CDK7,CLDN5,CR1,CREBBP,CSF2 gene,CSNK2A1,CSNK2A2,CSNK2B,CTLA4,CTSK,CTSL,CTSV,DPY30,ELF1,ELF2,EP300,ESR1,FOXP3,GATA1,GATA2,GATA3,GP1BA,GPAM,H19,H2AB1,H2AC14,H2AC18,H2AC20,H2AC4,H2AC6,H2AC7,H2AFX,H2AJ,H2AZ2,H2BC1,H2BC11,H2BC12,H2BC12L,H2BC13,H2BC14,H2BC15,H2BC17,H2BC21,H2BC26,H2BC3,H2BC4,H2BC5,H2BC9,H3-3A,H3C15,H4C1,HDAC1,HIPK2,HIST1H3G,IFNG,IL2,IL2RA,IL3,ITCH,ITGA2B,KAT2B,KCTD6,KMT2A,KMT2B,KMT2C,KMT2D,LDB1,LGALS3,LIFR,LMO1,LMO2,MIR106A,MIR17,MIR18A,MIR20A,MIR215,MIR27A,MIR302B,MIR378,MIR675,MNAT1,MOV10,MYB,MYL9,NFATC2,NFE2,NR4A3,OCLN,PAX5,PBRM1,PCGF5,PF4,PHC1,PHC2,PHC3,PML,PRKCB,PRKCQ,PRMT1,PRMT6,PSMA1,PSMA2,PSMA3,PSMA4,PSMA5,PSMA6,PSMA7,PSMB1,PSMB2,PSMB3,PSMB4,PSMB5,PSMB6,PSMB7,PSMC1,PSMC2,PSMC3,PSMC4,PSMC5,PSMC6,PSMD1,PSMD11,PSMD12,PSMD13,PSMD14,PSMD2,PSMD3,PSMD6,PSMD7,PSMD8,PTPN11,RBBP5,RING1,RNF2,RPS27A,RSPO3,RUNX1,RUNX2,RYBP,SCMH1,SEM1,SERPINB13,SETD1A,SETD1B,SIN3A,SIN3B,SMARCA2,SMARCA4,SMARCB1,SMARCC1,SMARCC2,SMARCD1,SMARCD2,SMARCD3,SMARCE1,SOCS3,SOCS4,SPI1,SRC,TAL1,TCF12,TCF3,THBS1,TJP1,TNFRSF18,TNRC6A,TNRC6B,TNRC6C,TP73,UBA52,UBB,UBC,WDR5,YAF2,YAP1,ZFPM1                                                                                                                                                                                                                                                                                                                                         |\n",
      "|Chromatin modifying enzymes                                               |R-HSA-3247509|0.4006448660816087 |3.4732001747913563|5.142916239031514E-4 |0.27904143442151047|0.16354473640120215|40          |TRRAP,NCOR2,NCOR1,SMARCB1,ARID5B,EZH2,CHD4,NCOA2,KDM6A,KMT2A,TBL1XR1,PBRM1,NFKB1,SETD2,PAX3,CREBBP,SUZ12,JAK2,ARID2,EP300,PRDM16,SMARCA4,KMT2D,KDM5C,ARID1B,NSD3,MECOM,ARID1A,KMT2C,NSD2       |ACTL6A,ACTL6B,AEBP2,ARID1A,ARID1B,ARID2,ARID4A,ARID4B,ARID5B,ASH1L,ASH2L,ATF2,ATF7IP,ATXN7,ATXN7L3,BRD1,BRD8,BRMS1,BRPF1,BRPF3,BRWD1,CARM1,CCND1,CDK4,CFL1,CHD3,CHD4,CLOCK,COPRS,CREBBP,DMAP1,DNMT3A,DOT1L,DPY30,DR1,EED,EHMT1,EHMT2,ELP2,ELP3,ELP4,ELP5,ELP6,ENY2,EP300,EP400,EPC1,EZH2,GATAD2A,GATAD2B,GPS2,H2AB1,H2AC1,H2AC11,H2AC12,H2AC14,H2AC18,H2AC20,H2AC21,H2AC25,H2AC4,H2AC6,H2AC7,H2AFX,H2AJ,H2AZ2,H2BC1,H2BC11,H2BC12,H2BC13,H2BC14,H2BC15,H2BC17,H2BC18,H2BC21,H2BC26,H2BC3,H2BC4,H2BC5,H2BC9,H3C15,H4C1,HAT1,HCFC1,HDAC1,HDAC10,HDAC2,HDAC3,HDAC8,HIST1H3G,HMG20B,IKBKAP,ING3,ING4,ING5,JADE1,JADE2,JADE3,JAK2,JMJD6,KANSL1,KANSL2,KANSL3,KAT14,KAT2A,KAT2B,KAT5,KAT6A,KAT6B,KAT7,KAT8,KDM1A,KDM1B,KDM2A,KDM2B,KDM3A,KDM3B,KDM4A,KDM4B,KDM4C,KDM4D,KDM5A,KDM5B,KDM5C,KDM5D,KDM6A,KDM6B,KDM7A,KMT2A,KMT2B,KMT2C,KMT2D,KMT5A,KMT5B,KMT5C,MBD3,MBIP,MCRS1,MEAF6,MECOM,MINA,MORF4L1,MORF4L2,MRGBP,MSL1,MSL2,MSL3,MTA1,MTA2,MTA3,NCOA1,NCOA2,NCOR1,NCOR2,NFKB1,NFKB2,NSD1,NSD2,NSD3,OGT,PADI1,PADI2,PADI3,PADI4,PADI6,PAX3,PBRM1,PHF2,PHF20,PHF21A,PHF8,PRDM16,PRDM9,PRMT1,PRMT3,PRMT5,PRMT6,PRMT7,RBBP4,RBBP5,RBBP7,RCOR1,RELA,REST,RPS2,RUVBL1,RUVBL2,SAP130,SAP18,SAP30,SAP30L,SETD1A,SETD1B,SETD2,SETD3,SETD6,SETD7,SETDB1,SETDB2,SGF29,SMARCA2,SMARCA4,SMARCB1,SMARCC1,SMARCC2,SMARCD1,SMARCD2,SMARCD3,SMARCE1,SMYD2,SMYD3,SUDS3,SUPT20H,SUPT3H,SUPT7L,SUV39H1,SUV39H2,SUZ12,TADA1,TADA2A,TADA2B,TADA3,TAF10,TAF12,TAF5L,TAF6L,TAF9,TBL1X,TBL1XR1,TRRAP,USP22,UTY,WDR5,WDR77,YEATS2,YEATS4,ZZZ3                                                                                            |\n",
      "|RUNX1 regulates transcription of genes involved in differentiation of HSCs|R-HSA-8939236|0.6823566584420674 |3.239377149348783 |0.0011979105852941707|0.533420951900572  |0.18810247075153202|12          |GATA3,MYB,TCF3,KMT2A,RUNX1                                                                                                                                                                     |ADRM1,CBFB,CCNH,CDK7,GATA1,GATA2,GATA3,H2AB1,H2AC14,H2AC18,H2AC20,H2AC4,H2AC6,H2AC7,H2AFX,H2AJ,H2AZ2,H2BC1,H2BC11,H2BC12,H2BC12L,H2BC13,H2BC14,H2BC15,H2BC17,H2BC21,H2BC26,H2BC3,H2BC4,H2BC5,H2BC9,H3-3A,H3C15,H4C1,HIST1H3G,ITCH,KMT2A,LDB1,LMO1,LMO2,MNAT1,MYB,PSMA1,PSMA2,PSMA3,PSMA4,PSMA5,PSMA6,PSMA7,PSMB1,PSMB2,PSMB3,PSMB4,PSMB5,PSMB6,PSMB7,PSMC1,PSMC2,PSMC3,PSMC4,PSMC5,PSMC6,PSMD1,PSMD11,PSMD12,PSMD13,PSMD14,PSMD2,PSMD3,PSMD6,PSMD7,PSMD8,RPS27A,RUNX1,SEM1,SPI1,TAL1,TCF12,TCF3,TP73,UBA52,UBB,UBC,YAP1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  |\n",
      "|Chromatin organization                                                    |R-HSA-4839726|0.3357171527886794 |3.1227113692857342|0.0017919338383376449|0.6804022960363598 |0.18810247075153202|44          |TRRAP,NCOR2,NCOR1,SMARCB1,ARID5B,EZH2,CHD4,NCOA2,KDM6A,KMT2A,TBL1XR1,PBRM1,NFKB1,SETD2,PAX3,CREBBP,SUZ12,JAK2,ARID2,EP300,DNMT3A,PRDM16,SMARCA4,KMT2D,KDM5C,ARID1B,NSD3,MECOM,ARID1A,KMT2C,NSD2|ACTL6A,ACTL6B,AEBP2,ARID1A,ARID1B,ARID2,ARID4A,ARID4B,ARID5B,ASH1L,ASH2L,ATF2,ATF7IP,ATXN7,ATXN7L3,BCL11A,BCL11B,BCL7A,BCL7B,BCL7C,BRD1,BRD7,BRD8,BRD9,BRMS1,BRPF1,BRPF3,BRWD1,CARM1,CCND1,CDK4,CFL1,CHD3,CHD4,CLOCK,COPRS,CREBBP,DMAP1,DNMT3A,DOT1L,DPF1,DPF2,DPF3,DPY30,DR1,EED,EHMT1,EHMT2,ELP2,ELP3,ELP4,ELP5,ELP6,ENY2,EP300,EP400,EPC1,EZH2,GATAD2A,GATAD2B,GLTSCR1,GLTSCR1L,GPS2,H2AB1,H2AC1,H2AC11,H2AC12,H2AC14,H2AC18,H2AC20,H2AC21,H2AC25,H2AC4,H2AC6,H2AC7,H2AFX,H2AJ,H2AZ2,H2BC1,H2BC11,H2BC12,H2BC13,H2BC14,H2BC15,H2BC17,H2BC18,H2BC21,H2BC26,H2BC3,H2BC4,H2BC5,H2BC9,H3C15,H4C1,HAT1,HCFC1,HDAC1,HDAC10,HDAC2,HDAC3,HDAC8,HIST1H3G,HMG20B,IKBKAP,ING3,ING4,ING5,JADE1,JADE2,JADE3,JAK2,JMJD6,KANSL1,KANSL2,KANSL3,KAT14,KAT2A,KAT2B,KAT5,KAT6A,KAT6B,KAT7,KAT8,KDM1A,KDM1B,KDM2A,KDM2B,KDM3A,KDM3B,KDM4A,KDM4B,KDM4C,KDM4D,KDM5A,KDM5B,KDM5C,KDM5D,KDM6A,KDM6B,KDM7A,KMT2A,KMT2B,KMT2C,KMT2D,KMT5A,KMT5B,KMT5C,MBD3,MBIP,MCRS1,MEAF6,MECOM,MINA,MORF4L1,MORF4L2,MRGBP,MSL1,MSL2,MSL3,MTA1,MTA2,MTA3,NCOA1,NCOA2,NCOR1,NCOR2,NFKB1,NFKB2,NSD1,NSD2,NSD3,OGT,PADI1,PADI2,PADI3,PADI4,PADI6,PAX3,PBRM1,PHF10,PHF2,PHF20,PHF21A,PHF8,PRDM16,PRDM9,PRMT1,PRMT3,PRMT5,PRMT6,PRMT7,RBBP4,RBBP5,RBBP7,RCOR1,RELA,REST,RPS2,RUVBL1,RUVBL2,SAP130,SAP18,SAP30,SAP30L,SETD1A,SETD1B,SETD2,SETD3,SETD6,SETD7,SETDB1,SETDB2,SGF29,SMARCA2,SMARCA4,SMARCB1,SMARCC1,SMARCC2,SMARCD1,SMARCD2,SMARCD3,SMARCE1,SMYD2,SMYD3,SS18,SS18L1,SUDS3,SUPT20H,SUPT3H,SUPT7L,SUV39H1,SUV39H2,SUZ12,TADA1,TADA2A,TADA2B,TADA3,TAF10,TAF12,TAF5L,TAF6L,TAF9,TBL1X,TBL1XR1,TRRAP,USP22,UTY,WDR5,WDR77,YEATS2,YEATS4,ZZZ3|\n",
      "|Respiratory electron transport                                            |R-HSA-611105 |-0.8250408640432152|-3.088833245949179|0.0020094419164649047|0.7217654511393337 |0.18810247075153202|5           |TIMMDC1,NDUFB1,COX6C                                                                                                                                                                           |ATP5SL,BCS1L,CMC1,COA1,COA3,COA5,COQ10A,COQ10B,COX11,COX14,COX15,COX16,COX17,COX18,COX19,COX20,COX4I1,COX4I2,COX5A,COX5B,COX6A1,COX6A2,COX6B1,COX6B2,COX6C,COX7A1,COX7A2,COX7A2L,COX7B,COX7C,COX8A,COX8C,CYC1,CYCS,ECSIT,ETFA,ETFB,ETFDH,FOXRED1,FXN,GOT1,GOT2,HCCS,HIGD1A,HIGD1C,HIGD2A,HSCB,HSPA9,ISCU,LETM1,LYRM2,LYRM4,LYRM7,MDH1,MDH2,MT-CO1,MT-CO2,MT-CO3,MT-CYB,MT-ND1,MT-ND2,MT-ND3,MT-ND4,MT-ND5,MT-ND6,NDUFA1,NDUFA10,NDUFA11,NDUFA12,NDUFA13,NDUFA2,NDUFA3,NDUFA4,NDUFA5,NDUFA6,NDUFA7,NDUFA8,NDUFA9,NDUFAB1,NDUFAF1,NDUFAF2,NDUFAF3,NDUFAF4,NDUFAF5,NDUFAF6,NDUFAF7,NDUFAF8,NDUFB1,NDUFB10,NDUFB11,NDUFB2,NDUFB3,NDUFB4,NDUFB5,NDUFB6,NDUFB7,NDUFB8,NDUFB9,NDUFC1,NDUFC2,NDUFS1,NDUFS2,NDUFS3,NDUFS4,NDUFS5,NDUFS6,NDUFS7,NDUFS8,NDUFV1,NDUFV2,NDUFV3,NFS1,NUBPL,OXA1L,PET100,PET117,PNKD,PYURF,RAB5IF,SCO1,SCO2,SDHA,SDHB,SDHC,SDHD,SFXN4,SLC25A11,SLC25A12,SLC25A13,SLC25A18,SLC25A22,SMIM20,SURF1,TACO1,TIMM21,TIMMDC1,TMEM126A,TMEM126B,TMEM177,TMEM186,TMEM223,TMEM261,TRAP1,TTC19,UQCC1,UQCC2,UQCC3,UQCC5,UQCC6,UQCR10,UQCR11,UQCRB,UQCRC1,UQCRC2,UQCRFS1,UQCRH,UQCRHL,UQCRQ                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           |\n",
      "+--------------------------------------------------------------------------+-------------+-------------------+------------------+---------------------+-------------------+-------------------+------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "df_test.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb57ad7",
   "metadata": {},
   "source": [
    "# Filter by FDR, add hierarchies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62241f07",
   "metadata": {},
   "source": [
    "1. Read all of files in Reactome_Pathways_2025_diy as one parquet file with new column diseaseId\n",
    "2. Filter by fdr cut off 0.1\n",
    "3. Add parent pathway into table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a178fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_reactome_pathways_with_hierarchy(base_dir, hierarchy_path, fdr_cutoff=0.1):\n",
    "\n",
    "    # Step 1: Read parquet files recursively\n",
    "    df = spark.read.option(\"recursiveFileLookup\", \"true\").parquet(base_dir)\n",
    "\n",
    "    # Step 2: Extract diseaseId from file path\n",
    "    df = df.withColumn(\n",
    "        \"diseaseId\",\n",
    "        regexp_extract(input_file_name(), r\"diseaseId=([^/]+)\", 1)\n",
    "    )\n",
    "\n",
    "    # Step 3: Filter by FDR cutoff\n",
    "    df_filtered = df.filter(col(\"fdr\") <= fdr_cutoff)\n",
    "\n",
    "    # Step 4: Load pathway hierarchy file\n",
    "    pathways_hierarchy_df = (\n",
    "        spark.read.option(\"delimiter\", \"\\t\")\n",
    "        .csv(hierarchy_path, header=False)\n",
    "        .withColumnRenamed(\"_c0\", \"parentId\")\n",
    "        .withColumnRenamed(\"_c1\", \"childId\")\n",
    "    )\n",
    "\n",
    "    # Step 5: Compute hierarchy level in-memory\n",
    "    hierarchy_pairs = pathways_hierarchy_df.collect()\n",
    "    parent_map = {row[\"childId\"]: row[\"parentId\"] for row in hierarchy_pairs}\n",
    "\n",
    "    def get_level(child_id):\n",
    "        level = 0\n",
    "        current = child_id\n",
    "        while current in parent_map and parent_map[current] is not None:\n",
    "            current = parent_map[current]\n",
    "            level += 1\n",
    "            if level > 50:  # safety break for cycles\n",
    "                break\n",
    "        return level\n",
    "\n",
    "    levels_data = [\n",
    "        Row(parentId=row[\"parentId\"], childId=row[\"childId\"], hierLevel=get_level(row[\"childId\"]))\n",
    "        for row in hierarchy_pairs\n",
    "    ]\n",
    "\n",
    "    pathways_hierarchy_df = spark.createDataFrame(levels_data)\n",
    "\n",
    "    # Step 6: Merge filtered pathways with hierarchy (now including hierLevel)\n",
    "    joined_df = df_filtered.join(\n",
    "        pathways_hierarchy_df,\n",
    "        df_filtered[\"ID\"] == pathways_hierarchy_df[\"childId\"],\n",
    "        \"left\"\n",
    "    )\n",
    "\n",
    "    return joined_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eff1cc4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "base_dir = \"/Users/polina/Pathwaganda/data/GSEA_output/Reactome_Pathways_2025_diy\"\n",
    "hierarchy_path = \"/Users/polina/Pathwaganda/data/gmt_pathway_files_prep/Reactome/Pathways_hierarchy_relationship.txt\"\n",
    "\n",
    "df_filtered = load_reactome_pathways_with_hierarchy(base_dir, hierarchy_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "47080671",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:=======================================>                (34 + 8) / 48]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-------------------+-------------------+--------------------+------------------+--------------------+------------+--------------------+--------------------+-------------+------------+------------+---------+\n",
      "|                Term|          ID|                 es|                nes|                pval|             sidak|                 fdr|geneset_size|        leading_edge|     propagated_edge|    diseaseId|    parentId|     childId|hierLevel|\n",
      "+--------------------+------------+-------------------+-------------------+--------------------+------------------+--------------------+------------+--------------------+--------------------+-------------+------------+------------+---------+\n",
      "|Formation of HIV-...|R-HSA-167200|-0.6109253369441215|-2.7815364391841655|0.005410225667489277|0.9999213261776687|0.052358961737590665|          17|POLR2K,POLR2C,POL...|CCNT1,CDK7,CDK9,C...|  EFO_0009676|R-HSA-167246|R-HSA-167200|        9|\n",
      "|Formation of HIV-...|R-HSA-167200|-0.5330343747744494|-2.7898945812422107|0.005272520585370...|0.9999003967853771| 0.06297485089783995|          18|GTF2H3,POLR2L,TCE...|CCNT1,CDK7,CDK9,C...|  EFO_0004302|R-HSA-167246|R-HSA-167200|        9|\n",
      "|Formation of HIV-...|R-HSA-167200|-0.6576724205375721| -2.836641619403294|0.004559075436435078|0.9959013931623664| 0.06855709687539249|           9|ELOA2,POLR2E,POLR...|CCNT1,CDK7,CDK9,C...|  EFO_1002003|R-HSA-167246|R-HSA-167200|        9|\n",
      "|Formation of HIV-...|R-HSA-167200|   0.79586725825519|  2.072555694413316| 0.03821364796365723|               1.0| 0.09880551160826508|           6|           ERCC3,ELL|CCNT1,CDK7,CDK9,C...|  EFO_1001331|R-HSA-167246|R-HSA-167200|        9|\n",
      "|Formation of HIV-...|R-HSA-167200|  0.774742548344697|  2.416793393466239|0.015657901818980635|0.9999998746622907| 0.04934180772462316|           6|           ELL,ERCC3|CCNT1,CDK7,CDK9,C...|MONDO_0002715|R-HSA-167246|R-HSA-167200|        9|\n",
      "|Formation of HIV-...|R-HSA-167200| 0.7500488925307891| 2.4703753948758456|0.013497133138684392|0.9999964772894618|0.056475451464003255|           5|     ELL,TCEA1,ERCC3|CCNT1,CDK7,CDK9,C...|MONDO_0002516|R-HSA-167246|R-HSA-167200|        9|\n",
      "|Formation of HIV-...|R-HSA-167200|-0.7563894624543063| -2.356002554825459|0.018472790647180215|0.9999934309924788| 0.08042575519860774|           5|  ERCC3,POLR2A,TCEA1|CCNT1,CDK7,CDK9,C...|  EFO_1000613|R-HSA-167246|R-HSA-167200|        9|\n",
      "+--------------------+------------+-------------------+-------------------+--------------------+------------------+--------------------+------------+--------------------+--------------------+-------------+------------+------------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_filtered.filter(col(\"hierLevel\") > 8).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53aab3ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/14 14:21:39 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/14 18:08:15 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 987319 ms exceeds timeout 120000 ms\n",
      "25/08/14 18:08:16 WARN SparkContext: Killing executors is not supported by current scheduler.\n",
      "25/08/14 18:08:18 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 18:08:18 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 18:08:27 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 18:08:27 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 18:08:38 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 18:08:38 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 18:23:58 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 18:23:58 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 18:58:18 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 18:58:18 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 19:07:25 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:153)\n",
      "\t... 17 more\n",
      "25/08/14 19:07:25 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:153)\n",
      "\t... 17 more\n",
      "25/08/14 19:39:29 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 19:39:29 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 20:08:19 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 20:08:19 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 20:25:45 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 20:25:45 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 20:57:10 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 20:57:10 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 20:57:21 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 20:57:21 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 20:57:31 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 20:57:31 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 21:01:50 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 21:01:50 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 21:02:00 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 21:02:00 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 21:05:29 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 21:05:29 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 21:09:21 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 21:09:21 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 21:26:16 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 21:26:16 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 21:40:29 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 21:40:29 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 21:40:46 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 21:40:46 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 21:41:55 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 21:41:55 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 21:59:22 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 21:59:22 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:10:27 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:10:27 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:41:21 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:41:21 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:42:07 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:153)\n",
      "\t... 17 more\n",
      "25/08/14 22:42:07 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:153)\n",
      "\t... 17 more\n",
      "25/08/14 22:42:16 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:42:16 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:42:26 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:42:26 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:42:36 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:153)\n",
      "\t... 17 more\n",
      "25/08/14 22:42:36 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:153)\n",
      "\t... 17 more\n",
      "25/08/14 22:42:46 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:42:46 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:42:56 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:42:56 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:43:06 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:43:06 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:43:16 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:153)\n",
      "\t... 17 more\n",
      "25/08/14 22:43:16 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:153)\n",
      "\t... 17 more\n",
      "25/08/14 22:43:26 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:153)\n",
      "\t... 17 more\n",
      "25/08/14 22:43:26 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:153)\n",
      "\t... 17 more\n",
      "25/08/14 22:43:36 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:43:36 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:43:46 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:43:46 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:43:56 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:43:56 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:44:06 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:44:06 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:44:16 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:153)\n",
      "\t... 17 more\n",
      "25/08/14 22:44:16 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:153)\n",
      "\t... 17 more\n",
      "25/08/14 22:44:26 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:44:26 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:44:36 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:153)\n",
      "\t... 17 more\n",
      "25/08/14 22:44:36 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:153)\n",
      "\t... 17 more\n",
      "25/08/14 22:44:46 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:153)\n",
      "\t... 17 more\n",
      "25/08/14 22:44:46 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:153)\n",
      "\t... 17 more\n",
      "25/08/14 22:44:56 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:44:56 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:45:06 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:45:06 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:45:16 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:45:16 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:45:26 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:45:26 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:45:36 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:45:36 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:45:46 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:153)\n",
      "\t... 17 more\n",
      "25/08/14 22:45:46 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.dispatchOrAddCallbacks(Promise.scala:312)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.flatMap(Promise.scala:176)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.asyncSetupEndpointRefByURI(NettyRpcEnv.scala:153)\n",
      "\t... 17 more\n",
      "25/08/14 22:45:56 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:45:56 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:46:06 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:46:06 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:46:16 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:46:16 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:46:26 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:46:26 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:46:36 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:46:36 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:46:46 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:46:46 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:46:56 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:46:56 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:47:06 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:47:06 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:47:16 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:47:16 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:47:26 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:47:26 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:47:36 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:47:36 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:47:46 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:47:46 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:47:56 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:47:56 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:48:06 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:669)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1296)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:48:06 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:700)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:699)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:739)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@172.23.50.10:55624\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:470)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:504)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:335)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:484)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:429)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:338)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:285)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:278)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "25/08/14 22:48:06 ERROR Executor: Exit as unable to send heartbeats to driver more than 60 times\n"
     ]
    }
   ],
   "source": [
    "df_filtered.write.mode(\"overwrite\").parquet(\"/Users/polina/Pathwaganda/data/GSEA-output_filt_hier\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451a395f",
   "metadata": {},
   "source": [
    "# Target-pathway matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0109c9",
   "metadata": {},
   "source": [
    "Explode targets from propagated_edge and create boolean matrix TxP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe7da9a",
   "metadata": {},
   "source": [
    "Hierarchical levels explanation:\n",
    "- Level 1 - broad propagation: all pathways\n",
    "- Level 2 - medium propagation: all except higher parent pathways\n",
    "- Level 3 - specific propagation: except 1st and 2nd higher parent pathways"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cc649a",
   "metadata": {},
   "source": [
    "## Add targetIds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d1c7bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------+-------------------+------------------+--------------------+--------------------+--------------------+------------+--------------------+--------------------+-----------+-------------+-------------+---------+\n",
      "|                Term|           ID|                 es|               nes|                pval|               sidak|                 fdr|geneset_size|        leading_edge|     propagated_edge|  diseaseId|     parentId|      childId|hierLevel|\n",
      "+--------------------+-------------+-------------------+------------------+--------------------+--------------------+--------------------+------------+--------------------+--------------------+-----------+-------------+-------------+---------+\n",
      "|Extracellular mat...|R-HSA-1474244|0.27986151888660815|4.4332464205552755|9.282466883142604E-6| 0.01783777610591378|2.399827104855134...|         273|SGCG,PCOLCE,ADAMT...|ACAN,ACTA1,ACTA2,...|EFO_0004747|         NULL|         NULL|     NULL|\n",
      "|Transcriptional r...|R-HSA-9616222|0.39964355593428214|2.6121439530923665|0.008997635905660495|  0.9999999755192386| 0.06583475605253358|          35|MYC,TAL1,E2F1,CDK...|CDK2,CDK4,CDKN1A,...|EFO_0004747|R-HSA-1266738|R-HSA-9616222|        1|\n",
      "|Glycerophospholip...|R-HSA-1483206| 0.2103581990787265|2.4611400923039772|0.013849627537045661|   0.999999999998198| 0.09201616365191037|         101|CHPT1,CPNE1,LPIN2...|ABHD4,ACHE,ACP6,A...|EFO_0004747|R-HSA-1483257|R-HSA-1483206|        3|\n",
      "|Extracellular mat...|R-HSA-1474244| 0.3747569290253538| 4.964506892287638|6.887584045323081E-7|0.001287837443371...|2.863704388622107...|         257|FBN1,COL2A1,PLG,P...|ACAN,ACTA1,ACTA2,...|EFO_0000508|         NULL|         NULL|     NULL|\n",
      "|Transcriptional r...|R-HSA-9616222|  0.520217291010168|3.9135535374480663|9.094772438311693E-5| 0.16068843033343233| 0.00139160731571808|          34|GATA2,FLI1,CSF3R,...|CDK2,CDK4,CDKN1A,...|EFO_0004503|R-HSA-1266738|R-HSA-9616222|        1|\n",
      "+--------------------+-------------+-------------------+------------------+--------------------+--------------------+--------------------+------------+--------------------+--------------------+-----------+-------------+-------------+---------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"/Users/polina/Pathwaganda/data/GSEA-output_filt_hier\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff466dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = spark.read.parquet(\"/Users/polina/Pathwaganda/data/target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f3b77c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_pathway_interm(input_path: str, target_path: str, fdr_threshold: float = 0.1, delimiter: str = \",\"):\n",
    "    \n",
    "    # --- Read input parquet ---\n",
    "    df = spark.read.parquet(input_path)\n",
    "    df = df.select(\"ID\", \"fdr\", \"propagated_edge\", \"diseaseId\", \"hierLevel\")\n",
    "    df = df.filter(F.col(\"fdr\") <= fdr_threshold)\n",
    "    df = df.withColumn(\"propagated_edge_array\", F.split(F.col(\"propagated_edge\"), delimiter))\n",
    "    df = df.withColumn(\"approvedSymbol\", F.explode(\"propagated_edge_array\"))\n",
    "    df = df.select(\"ID\", \"diseaseId\", \"hierLevel\", \"approvedSymbol\")\n",
    "    \n",
    "    # --- Read target parquet ---\n",
    "    target_df = spark.read.parquet(target_path).select(\n",
    "        F.col(\"id\").alias(\"targetId\"),\n",
    "        \"approvedSymbol\",\n",
    "        \"symbolSynonyms\"\n",
    "    )\n",
    "    \n",
    "    # --- Explode synonyms into mapping ---\n",
    "    synonyms_df = (\n",
    "        target_df\n",
    "        .withColumn(\"syn_struct\", F.explode_outer(\"symbolSynonyms\"))\n",
    "        .withColumn(\"approvedSymbol\", F.col(\"syn_struct\").getField(\"label\"))  # use label\n",
    "        .select(\"targetId\", \"approvedSymbol\")\n",
    "        .filter(F.col(\"approvedSymbol\").isNotNull())\n",
    "    )\n",
    "    \n",
    "    # --- Union approvedSymbol + synonyms mapping ---\n",
    "    mapping_df = (\n",
    "        target_df.select(\"targetId\", \"approvedSymbol\")\n",
    "        .unionByName(synonyms_df)\n",
    "        .dropDuplicates([\"targetId\", \"approvedSymbol\"])\n",
    "    )\n",
    "    \n",
    "    # --- Left join on mapping ---\n",
    "    joined = df.join(mapping_df, on=\"approvedSymbol\", how=\"left\")\n",
    "    \n",
    "    # --- Aggregate multiple targetIds for same approvedSymbol ---\n",
    "    result = (\n",
    "        joined.groupBy(\"ID\", \"diseaseId\", \"hierLevel\", \"approvedSymbol\")\n",
    "              .agg(F.concat_ws(\",\", F.collect_set(\"targetId\")).alias(\"targetId\"))\n",
    "    )\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "861c7076",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_pathway_interm_df = target_pathway_interm(\n",
    "    \"/Users/polina/Pathwaganda/data/GSEA-output_filt_hier\",\n",
    "    \"/Users/polina/Pathwaganda/data/target\",\n",
    "    fdr_threshold=0.1,\n",
    "    delimiter=\",\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77733a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of distinct rows with empty targetId: 425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 80:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|approvedSymbol|\n",
      "+--------------+\n",
      "|          opaE|\n",
      "|          opaJ|\n",
      "|       5S rRNA|\n",
      "| POU5F1 (OCT4)|\n",
      "|          UL83|\n",
      "+--------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Count distinct rows where targetId is null or empty\n",
    "empty_targetid = (\n",
    "    target_pathway_interm_df.filter((F.col(\"targetId\").isNull()) | (F.col(\"targetId\") == \"\"))\n",
    "          .select(\"approvedSymbol\")   # or \"ID\",\"diseaseId\",\"hierLevel\",\"approvedSymbol\" if you want row uniqueness\n",
    "          .distinct()\n",
    ")\n",
    "\n",
    "print(f\"Number of distinct rows with empty targetId: {empty_targetid.count()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3adc74af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/18 13:44:37 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "target_pathway_interm_df.write.mode(\"overwrite\").parquet(\"/Users/polina/Pathwaganda/data/GSEA-output_by_target_interm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d101642b",
   "metadata": {},
   "source": [
    "## Save target-pathway matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f31e2957",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mtx_prepare_hier(input_path: str, level: int) -> None:\n",
    "    \"\"\"\n",
    "    Reads a parquet file, filters rows by hierLevel and targetId, groups by diseaseId,\n",
    "    concatenates ID values into pathwayIds, and writes result to parquet.\n",
    "\n",
    "    Args:\n",
    "        input_path (str): Path to input parquet file\n",
    "        output_path (str): Path to save output parquet file\n",
    "        level (int): Hierarchy filter. If 0, keep all rows. If n, keep rows where hierLevel >= n.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read parquet\n",
    "    df = spark.read.parquet(input_path)\n",
    "\n",
    "    # Apply hierLevel filter\n",
    "    if level > 0:\n",
    "        df = df.filter(F.col(\"hierLevel\") >= level)\n",
    "\n",
    "    # Filter out null or empty targetId\n",
    "    df = df.filter(~(F.col(\"targetId\").isNull() | (F.col(\"targetId\") == \"\")))\n",
    "\n",
    "    # Group by diseaseId, targetId, approvedSymbol and aggregate IDs\n",
    "    result = (df.groupBy(\"diseaseId\", \"targetId\", \"approvedSymbol\")\n",
    "                .agg(F.collect_list(\"ID\").alias(\"pathwayIds\"))\n",
    "                .withColumn(\"pathwayIds\", F.concat_ws(\",\", \"pathwayIds\")))\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "62d585d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3101968"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtx_level_0 = mtx_prepare_hier(\"/Users/polina/Pathwaganda/data/GSEA-output_by_target_interm\", 0)\n",
    "mtx_level_0.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb67348",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/18 14:06:51 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mtx_level_0.write.mode(\"overwrite\").parquet(\"/Users/polina/Pathwaganda/data/tpm_levels/0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b556dc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1786033"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtx_level_1 = mtx_prepare_hier(\"/Users/polina/Pathwaganda/data/GSEA-output_by_target_interm\", 1)\n",
    "mtx_level_1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0eab88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/18 14:07:01 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mtx_level_1.write.mode(\"overwrite\").parquet(\"/Users/polina/Pathwaganda/data/tpm_levels/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fbe29aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1149592"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mtx_level_2 = mtx_prepare_hier(\"/Users/polina/Pathwaganda/data/GSEA-output_by_target_interm\", 2)\n",
    "mtx_level_2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be323dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/18 14:07:12 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "mtx_level_2.write.mode(\"overwrite\").parquet(\"/Users/polina/Pathwaganda/data/tpm_levels/2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db27609a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+--------------+------------+\n",
      "|  diseaseId|       targetId|approvedSymbol|  pathwayIds|\n",
      "+-----------+---------------+--------------+------------+\n",
      "|EFO_0000095|ENSG00000004897|         CDC27|R-HSA-212436|\n",
      "|EFO_0000095|ENSG00000013275|         PSMC4|R-HSA-212436|\n",
      "|EFO_0000095|ENSG00000047315|        POLR2B|R-HSA-212436|\n",
      "|EFO_0000095|ENSG00000051180|         RAD51|R-HSA-212436|\n",
      "|EFO_0000095|ENSG00000060069|         CTDP1|R-HSA-212436|\n",
      "+-----------+---------------+--------------+------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"/Users/polina/Pathwaganda/data/tpm_levels/2\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12af426e",
   "metadata": {},
   "source": [
    "# Pathway embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5398e261",
   "metadata": {},
   "source": [
    "## Hierarchical (Poincare ball model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39aa195c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when\n",
    "from gensim.models.poincare import PoincareModel\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "import types\n",
    "# Ensure gensim can import numpy.strings\n",
    "sys.modules['numpy.strings'] = types.ModuleType('numpy.strings')\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48733b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_hierarchy_diseases(input_file, negative=10, epochs=100, max_hierLevel=None):\n",
    "    \"\"\"\n",
    "    Process disease hierarchies with optional level filtering\n",
    "    \n",
    "    Args:\n",
    "        input_file: Path to input parquet file\n",
    "        negative: Maximum number of negative samples to use\n",
    "        epochs: Number of training epochs\n",
    "        max_hierLevel: Maximum hierarchy level to include (None = all levels, 0 = root only, \n",
    "                      1 = root and level 1, etc.)\n",
    "    \"\"\"\n",
    "    # 1) Read required columns\n",
    "    base_cols = [\"diseaseId\", \"ID\", \"parentId\"]\n",
    "    if max_hierLevel is not None:\n",
    "        base_cols.append(\"hierLevel\")\n",
    "    \n",
    "    df_all = spark.read.parquet(input_file).select(*base_cols).cache()\n",
    "\n",
    "    # 2) Apply hierarchy level filtering if specified\n",
    "    if max_hierLevel is not None:\n",
    "        df_all = df_all.filter(col(\"hierLevel\") <= max_hierLevel)\n",
    "        print(f\"Filtering to hierarchy levels <= {max_hierLevel}\")\n",
    "\n",
    "    # 3) Collect distinct diseases\n",
    "    disease_ids = [r[\"diseaseId\"] for r in df_all.select(\"diseaseId\").distinct().collect()]\n",
    "\n",
    "    all_embeddings = []\n",
    "\n",
    "    for disease in disease_ids:\n",
    "        print(f\"\\nProcessing disease: {disease}\")\n",
    "\n",
    "        # Filter to one disease\n",
    "        df = df_all.filter(col(\"diseaseId\") == disease)\n",
    "\n",
    "        # Fill null parents with self\n",
    "        df = df.withColumn(\n",
    "            \"parentId\",\n",
    "            when(col(\"parentId\").isNull(), col(\"ID\")).otherwise(col(\"parentId\"))\n",
    "        )\n",
    "\n",
    "        # Count unique nodes\n",
    "        N = df.selectExpr(\"ID as node\").union(\n",
    "            df.selectExpr(\"parentId as node\")\n",
    "        ).distinct().count()\n",
    "\n",
    "        dims = max(2, math.ceil(math.log2(N)))\n",
    "        print(f\"{disease} - Total distinct nodes N: {N}; Chosen d: {dims}\")\n",
    "\n",
    "        # Skip too small graphs\n",
    "        if N < 3:\n",
    "            print(f\"Skipping {disease}: too few nodes ({N})\")\n",
    "            continue\n",
    "\n",
    "        # Extract edges (parentId  ID)\n",
    "        edges = (\n",
    "            df.select(\"parentId\", \"ID\")\n",
    "            .dropDuplicates()\n",
    "            .localCheckpoint()\n",
    "            .toPandas()\n",
    "            .values.tolist()\n",
    "        )\n",
    "\n",
    "        # BULLETPROOF NEGATIVE SAMPLING\n",
    "        def calculate_safe_negatives(requested, total_nodes):\n",
    "            max_possible = total_nodes - 2  # Conservative estimate\n",
    "            safe_neg = min(requested, max(1, max_possible))\n",
    "            print(f\"Negative sampling: Requested {requested}, Safe maximum {max_possible}, Using {safe_neg}\")\n",
    "            return safe_neg\n",
    "\n",
    "        neg = calculate_safe_negatives(negative, N)\n",
    "\n",
    "        # Train with automatic retry logic\n",
    "        max_attempts = 2\n",
    "        for attempt in range(max_attempts):\n",
    "            try:\n",
    "                model = PoincareModel(edges, negative=neg, size=dims)\n",
    "                model.train(epochs=epochs)\n",
    "                break\n",
    "            except ValueError as e:\n",
    "                if attempt == max_attempts - 1:\n",
    "                    print(f\"Failed after {max_attempts} attempts for {disease}: {str(e)}\")\n",
    "                    continue  # Skip this disease\n",
    "                print(f\"Attempt {attempt + 1} failed: {str(e)}\")\n",
    "                neg = max(1, neg - 2)\n",
    "                print(f\"Retrying with {neg} negatives\")\n",
    "\n",
    "        if 'model' not in locals():\n",
    "            continue  # Skip to next disease if all attempts failed\n",
    "\n",
    "        # Dump embeddings\n",
    "        emb = [(key, *model.kv[key]) for key in model.kv.index_to_key]\n",
    "        pdf = pd.DataFrame(emb, columns=[\"ID\"] + [f\"dim_{i}\" for i in range(dims)])\n",
    "        pdf[\"diseaseId\"] = disease\n",
    "\n",
    "        all_embeddings.append(pdf)\n",
    "\n",
    "    # Combine all into one Spark DataFrame\n",
    "    if all_embeddings:\n",
    "        final_pdf = pd.concat(all_embeddings, ignore_index=True)\n",
    "        final_sdf = spark.createDataFrame(final_pdf)\n",
    "    else:\n",
    "        print(\"No embeddings generated.\")\n",
    "        final_sdf = spark.createDataFrame(pd.DataFrame(columns=[\"ID\", \"diseaseId\"]))\n",
    "\n",
    "    df_all.unpersist()\n",
    "    return final_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7b68983",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing disease: EFO_0000701\n",
      "EFO_0000701 - Total distinct nodes N: 59; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 57, Using 10\n",
      "\n",
      "Processing disease: EFO_0004872\n",
      "EFO_0004872 - Total distinct nodes N: 61; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 59, Using 10\n",
      "\n",
      "Processing disease: EFO_0005803\n",
      "EFO_0005803 - Total distinct nodes N: 367; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 365, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002715\n",
      "MONDO_0002715 - Total distinct nodes N: 478; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 476, Using 10\n",
      "\n",
      "Processing disease: GO_0008150\n",
      "GO_0008150 - Total distinct nodes N: 24; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 22, Using 10\n",
      "\n",
      "Processing disease: HP_0012638\n",
      "HP_0012638 - Total distinct nodes N: 48; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 46, Using 10\n",
      "\n",
      "Processing disease: MONDO_0023370\n",
      "MONDO_0023370 - Total distinct nodes N: 493; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 491, Using 10\n",
      "\n",
      "Processing disease: EFO_0004516\n",
      "EFO_0004516 - Total distinct nodes N: 38; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 36, Using 10\n",
      "\n",
      "Processing disease: EFO_0005127\n",
      "EFO_0005127 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: EFO_0803540\n",
      "EFO_0803540 - Total distinct nodes N: 34; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 32, Using 10\n",
      "\n",
      "Processing disease: EFO_0010285\n",
      "EFO_0010285 - Total distinct nodes N: 198; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 196, Using 10\n",
      "\n",
      "Processing disease: HP_0001626\n",
      "HP_0001626 - Total distinct nodes N: 128; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 126, Using 10\n",
      "\n",
      "Processing disease: MONDO_0007254\n",
      "MONDO_0007254 - Total distinct nodes N: 405; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 403, Using 10\n",
      "\n",
      "Processing disease: EFO_0006943\n",
      "EFO_0006943 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0006943: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0004833\n",
      "EFO_0004833 - Total distinct nodes N: 13; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 11, Using 10\n",
      "\n",
      "Processing disease: EFO_0006843\n",
      "EFO_0006843 - Total distinct nodes N: 50; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 48, Using 10\n",
      "\n",
      "Processing disease: EFO_0010284\n",
      "EFO_0010284 - Total distinct nodes N: 24; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 22, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002039\n",
      "MONDO_0002039 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "\n",
      "Processing disease: EFO_0004574\n",
      "EFO_0004574 - Total distinct nodes N: 41; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 39, Using 10\n",
      "\n",
      "Processing disease: EFO_0002461\n",
      "EFO_0002461 - Total distinct nodes N: 281; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 279, Using 10\n",
      "\n",
      "Processing disease: EFO_1001986\n",
      "EFO_1001986 - Total distinct nodes N: 141; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 139, Using 10\n",
      "\n",
      "Processing disease: EFO_0004517\n",
      "EFO_0004517 - Total distinct nodes N: 11; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 9, Using 9\n",
      "\n",
      "Processing disease: EFO_0004747\n",
      "EFO_0004747 - Total distinct nodes N: 341; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 339, Using 10\n",
      "\n",
      "Processing disease: EFO_0004512\n",
      "EFO_0004512 - Total distinct nodes N: 42; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 40, Using 10\n",
      "\n",
      "Processing disease: EFO_0004269\n",
      "EFO_0004269 - Total distinct nodes N: 21; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 19, Using 10\n",
      "\n",
      "Processing disease: MONDO_0015757\n",
      "MONDO_0015757 - Total distinct nodes N: 117; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 115, Using 10\n",
      "\n",
      "Processing disease: EFO_0005771\n",
      "EFO_0005771 - Total distinct nodes N: 81; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 79, Using 10\n",
      "\n",
      "Processing disease: GO_0032501\n",
      "GO_0032501 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: EFO_0004586\n",
      "EFO_0004586 - Total distinct nodes N: 326; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 324, Using 10\n",
      "\n",
      "Processing disease: EFO_0010700\n",
      "EFO_0010700 - Total distinct nodes N: 1; Chosen d: 2\n",
      "Skipping EFO_0010700: too few nodes (1)\n",
      "\n",
      "Processing disease: EFO_0009433\n",
      "EFO_0009433 - Total distinct nodes N: 44; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 42, Using 10\n",
      "\n",
      "Processing disease: EFO_0004527\n",
      "EFO_0004527 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_0009676\n",
      "EFO_0009676 - Total distinct nodes N: 290; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 288, Using 10\n",
      "\n",
      "Processing disease: EFO_0004260\n",
      "EFO_0004260 - Total distinct nodes N: 169; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 167, Using 10\n",
      "\n",
      "Processing disease: MONDO_0008315\n",
      "MONDO_0008315 - Total distinct nodes N: 63; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 61, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002025\n",
      "MONDO_0002025 - Total distinct nodes N: 47; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 45, Using 10\n",
      "\n",
      "Processing disease: EFO_0004346\n",
      "EFO_0004346 - Total distinct nodes N: 30; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 28, Using 10\n",
      "\n",
      "Processing disease: EFO_0004530\n",
      "EFO_0004530 - Total distinct nodes N: 44; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 42, Using 10\n",
      "\n",
      "Processing disease: MONDO_0003274\n",
      "MONDO_0003274 - Total distinct nodes N: 171; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 169, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021230\n",
      "MONDO_0021230 - Total distinct nodes N: 254; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 252, Using 10\n",
      "\n",
      "Processing disease: EFO_1001331\n",
      "EFO_1001331 - Total distinct nodes N: 423; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 421, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021066\n",
      "MONDO_0021066 - Total distinct nodes N: 415; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 413, Using 10\n",
      "\n",
      "Processing disease: EFO_0005741\n",
      "EFO_0005741 - Total distinct nodes N: 11; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 9, Using 9\n",
      "\n",
      "Processing disease: EFO_0003892\n",
      "EFO_0003892 - Total distinct nodes N: 71; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 69, Using 10\n",
      "\n",
      "Processing disease: EFO_0000508\n",
      "EFO_0000508 - Total distinct nodes N: 446; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 444, Using 10\n",
      "\n",
      "Processing disease: EFO_0004302\n",
      "EFO_0004302 - Total distinct nodes N: 209; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 207, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021148\n",
      "MONDO_0021148 - Total distinct nodes N: 557; Chosen d: 10\n",
      "Negative sampling: Requested 10, Safe maximum 555, Using 10\n",
      "\n",
      "Processing disease: EFO_0004611\n",
      "EFO_0004611 - Total distinct nodes N: 104; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 102, Using 10\n",
      "\n",
      "Processing disease: EFO_0004509\n",
      "EFO_0004509 - Total distinct nodes N: 85; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 83, Using 10\n",
      "\n",
      "Processing disease: EFO_0009386\n",
      "EFO_0009386 - Total distinct nodes N: 155; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 153, Using 10\n",
      "\n",
      "Processing disease: EFO_0001379\n",
      "EFO_0001379 - Total distinct nodes N: 198; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 196, Using 10\n",
      "\n",
      "Processing disease: EFO_0011008\n",
      "EFO_0011008 - Total distinct nodes N: 13; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 11, Using 10\n",
      "\n",
      "Processing disease: EFO_0000651\n",
      "EFO_0000651 - Total distinct nodes N: 190; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 188, Using 10\n",
      "\n",
      "Processing disease: EFO_0004348\n",
      "EFO_0004348 - Total distinct nodes N: 13; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 11, Using 10\n",
      "\n",
      "Processing disease: EFO_0004309\n",
      "EFO_0004309 - Total distinct nodes N: 81; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 79, Using 10\n",
      "\n",
      "Processing disease: EFO_0004614\n",
      "EFO_0004614 - Total distinct nodes N: 28; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 26, Using 10\n",
      "\n",
      "Processing disease: EFO_0005755\n",
      "EFO_0005755 - Total distinct nodes N: 37; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 35, Using 10\n",
      "\n",
      "Processing disease: EFO_0803547\n",
      "EFO_0803547 - Total distinct nodes N: 38; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 36, Using 10\n",
      "\n",
      "Processing disease: EFO_0000719\n",
      "EFO_0000719 - Total distinct nodes N: 21; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 19, Using 10\n",
      "\n",
      "Processing disease: EFO_0004995\n",
      "EFO_0004995 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_0004264\n",
      "EFO_0004264 - Total distinct nodes N: 278; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 276, Using 10\n",
      "\n",
      "Processing disease: EFO_0000574\n",
      "EFO_0000574 - Total distinct nodes N: 118; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 116, Using 10\n",
      "\n",
      "Processing disease: EFO_0006842\n",
      "EFO_0006842 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_0009431\n",
      "EFO_0009431 - Total distinct nodes N: 73; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 71, Using 10\n",
      "\n",
      "Processing disease: EFO_0005809\n",
      "EFO_0005809 - Total distinct nodes N: 96; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 94, Using 10\n",
      "\n",
      "Processing disease: EFO_0004303\n",
      "EFO_0004303 - Total distinct nodes N: 69; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 67, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021259\n",
      "MONDO_0021259 - Total distinct nodes N: 119; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 117, Using 10\n",
      "\n",
      "Processing disease: EFO_0004298\n",
      "EFO_0004298 - Total distinct nodes N: 215; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 213, Using 10\n",
      "\n",
      "Processing disease: MONDO_0001933\n",
      "MONDO_0001933 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: HP_0011842\n",
      "HP_0011842 - Total distinct nodes N: 39; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 37, Using 10\n",
      "\n",
      "Processing disease: EFO_0004198\n",
      "EFO_0004198 - Total distinct nodes N: 321; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 319, Using 10\n",
      "\n",
      "Processing disease: EFO_0005105\n",
      "EFO_0005105 - Total distinct nodes N: 189; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 187, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002259\n",
      "MONDO_0002259 - Total distinct nodes N: 126; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 124, Using 10\n",
      "\n",
      "Processing disease: EFO_0006945\n",
      "EFO_0006945 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0006945: too few nodes (2)\n",
      "\n",
      "Processing disease: MONDO_0003939\n",
      "MONDO_0003939 - Total distinct nodes N: 180; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 178, Using 10\n",
      "\n",
      "Processing disease: EFO_0007987\n",
      "EFO_0007987 - Total distinct nodes N: 33; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 31, Using 10\n",
      "\n",
      "Processing disease: EFO_0004587\n",
      "EFO_0004587 - Total distinct nodes N: 49; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 47, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002334\n",
      "MONDO_0002334 - Total distinct nodes N: 160; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 158, Using 10\n",
      "\n",
      "Processing disease: EFO_1000051\n",
      "EFO_1000051 - Total distinct nodes N: 560; Chosen d: 10\n",
      "Negative sampling: Requested 10, Safe maximum 558, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021350\n",
      "MONDO_0021350 - Total distinct nodes N: 228; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 226, Using 10\n",
      "\n",
      "Processing disease: EFO_0009682\n",
      "EFO_0009682 - Total distinct nodes N: 15; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 13, Using 10\n",
      "\n",
      "Processing disease: HP_0000924\n",
      "HP_0000924 - Total distinct nodes N: 11; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 9, Using 9\n",
      "Attempt 1 failed: Cannot sample 9 negative nodes from a set of 8 negative nodes for R-HSA-1474244\n",
      "Retrying with 7 negatives\n",
      "\n",
      "Processing disease: EFO_0803548\n",
      "EFO_0803548 - Total distinct nodes N: 15; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 13, Using 10\n",
      "\n",
      "Processing disease: EFO_0004340\n",
      "EFO_0004340 - Total distinct nodes N: 30; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 28, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000473\n",
      "MONDO_0000473 - Total distinct nodes N: 73; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 71, Using 10\n",
      "\n",
      "Processing disease: EFO_1000999\n",
      "EFO_1000999 - Total distinct nodes N: 16; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 14, Using 10\n",
      "\n",
      "Processing disease: Orphanet_322126\n",
      "Orphanet_322126 - Total distinct nodes N: 84; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 82, Using 10\n",
      "\n",
      "Processing disease: EFO_0000684\n",
      "EFO_0000684 - Total distinct nodes N: 32; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 30, Using 10\n",
      "\n",
      "Processing disease: EFO_0001421\n",
      "EFO_0001421 - Total distinct nodes N: 22; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 20, Using 10\n",
      "\n",
      "Processing disease: EFO_0000589\n",
      "EFO_0000589 - Total distinct nodes N: 155; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 153, Using 10\n",
      "\n",
      "Processing disease: EFO_0004503\n",
      "EFO_0004503 - Total distinct nodes N: 390; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 388, Using 10\n",
      "\n",
      "Processing disease: MONDO_0044881\n",
      "MONDO_0044881 - Total distinct nodes N: 204; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 202, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002614\n",
      "MONDO_0002614 - Total distinct nodes N: 13; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 11, Using 10\n",
      "\n",
      "Processing disease: EFO_0004612\n",
      "EFO_0004612 - Total distinct nodes N: 81; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 79, Using 10\n",
      "\n",
      "Processing disease: EFO_0009690\n",
      "EFO_0009690 - Total distinct nodes N: 130; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 128, Using 10\n",
      "\n",
      "Processing disease: EFO_0007937\n",
      "EFO_0007937 - Total distinct nodes N: 101; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 99, Using 10\n",
      "\n",
      "Processing disease: EFO_0006841\n",
      "EFO_0006841 - Total distinct nodes N: 39; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 37, Using 10\n",
      "\n",
      "Processing disease: HP_0000118\n",
      "HP_0000118 - Total distinct nodes N: 441; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 439, Using 10\n",
      "\n",
      "Processing disease: EFO_0002919\n",
      "EFO_0002919 - Total distinct nodes N: 271; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 269, Using 10\n",
      "\n",
      "Processing disease: EFO_0005278\n",
      "EFO_0005278 - Total distinct nodes N: 198; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 196, Using 10\n",
      "\n",
      "Processing disease: EFO_0010282\n",
      "EFO_0010282 - Total distinct nodes N: 119; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 117, Using 10\n",
      "\n",
      "Processing disease: EFO_0006335\n",
      "EFO_0006335 - Total distinct nodes N: 34; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 32, Using 10\n",
      "\n",
      "Processing disease: MONDO_0045024\n",
      "MONDO_0045024 - Total distinct nodes N: 529; Chosen d: 10\n",
      "Negative sampling: Requested 10, Safe maximum 527, Using 10\n",
      "\n",
      "Processing disease: EFO_0000400\n",
      "EFO_0000400 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: EFO_0005774\n",
      "EFO_0005774 - Total distinct nodes N: 70; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 68, Using 10\n",
      "\n",
      "Processing disease: GO_0050896\n",
      "GO_0050896 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_0803546\n",
      "EFO_0803546 - Total distinct nodes N: 37; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 35, Using 10\n",
      "\n",
      "Processing disease: EFO_1002003\n",
      "EFO_1002003 - Total distinct nodes N: 141; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 139, Using 10\n",
      "\n",
      "Processing disease: MONDO_0024582\n",
      "MONDO_0024582 - Total distinct nodes N: 164; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 162, Using 10\n",
      "\n",
      "Processing disease: EFO_0007010\n",
      "EFO_0007010 - Total distinct nodes N: 54; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 52, Using 10\n",
      "\n",
      "Processing disease: EFO_0004468\n",
      "EFO_0004468 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_0004541\n",
      "EFO_0004541 - Total distinct nodes N: 1; Chosen d: 2\n",
      "Skipping EFO_0004541: too few nodes (1)\n",
      "\n",
      "Processing disease: EFO_0001444\n",
      "EFO_0001444 - Total distinct nodes N: 452; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 450, Using 10\n",
      "\n",
      "Processing disease: HP_0000707\n",
      "HP_0000707 - Total distinct nodes N: 61; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 59, Using 10\n",
      "\n",
      "Processing disease: EFO_0007355\n",
      "EFO_0007355 - Total distinct nodes N: 100; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 98, Using 10\n",
      "\n",
      "Processing disease: EFO_0004338\n",
      "EFO_0004338 - Total distinct nodes N: 31; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 29, Using 10\n",
      "\n",
      "Processing disease: EFO_0003869\n",
      "EFO_0003869 - Total distinct nodes N: 272; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 270, Using 10\n",
      "\n",
      "Processing disease: EFO_1000363\n",
      "EFO_1000363 - Total distinct nodes N: 353; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 351, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021581\n",
      "MONDO_0021581 - Total distinct nodes N: 266; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 264, Using 10\n",
      "\n",
      "Processing disease: EFO_0004732\n",
      "EFO_0004732 - Total distinct nodes N: 129; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 127, Using 10\n",
      "\n",
      "Processing disease: EFO_0000618\n",
      "EFO_0000618 - Total distinct nodes N: 210; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 208, Using 10\n",
      "\n",
      "Processing disease: EFO_0004555\n",
      "EFO_0004555 - Total distinct nodes N: 20; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 18, Using 10\n",
      "\n",
      "Processing disease: EFO_0004980\n",
      "EFO_0004980 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "\n",
      "Processing disease: MONDO_0002974\n",
      "MONDO_0002974 - Total distinct nodes N: 132; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 130, Using 10\n",
      "\n",
      "Processing disease: GO_0009410\n",
      "GO_0009410 - Total distinct nodes N: 1; Chosen d: 2\n",
      "Skipping GO_0009410: too few nodes (1)\n",
      "\n",
      "Processing disease: EFO_0006858\n",
      "EFO_0006858 - Total distinct nodes N: 533; Chosen d: 10\n",
      "Negative sampling: Requested 10, Safe maximum 531, Using 10\n",
      "\n",
      "Processing disease: EFO_0011015\n",
      "EFO_0011015 - Total distinct nodes N: 12; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 10, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002149\n",
      "MONDO_0002149 - Total distinct nodes N: 466; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 464, Using 10\n",
      "\n",
      "Processing disease: EFO_0003777\n",
      "EFO_0003777 - Total distinct nodes N: 202; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 200, Using 10\n",
      "\n",
      "Processing disease: GO_0007610\n",
      "GO_0007610 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: EFO_0001061\n",
      "EFO_0001061 - Total distinct nodes N: 220; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 218, Using 10\n",
      "\n",
      "Processing disease: EFO_0007441\n",
      "EFO_0007441 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_0000228\n",
      "EFO_0000228 - Total distinct nodes N: 137; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 135, Using 10\n",
      "\n",
      "Processing disease: EFO_0003893\n",
      "EFO_0003893 - Total distinct nodes N: 410; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 408, Using 10\n",
      "\n",
      "Processing disease: EFO_0004528\n",
      "EFO_0004528 - Total distinct nodes N: 49; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 47, Using 10\n",
      "\n",
      "Processing disease: EFO_0003966\n",
      "EFO_0003966 - Total distinct nodes N: 65; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 63, Using 10\n",
      "\n",
      "Processing disease: EFO_0005091\n",
      "EFO_0005091 - Total distinct nodes N: 41; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 39, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002654\n",
      "MONDO_0002654 - Total distinct nodes N: 23; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 21, Using 10\n",
      "\n",
      "Processing disease: Orphanet_68336\n",
      "Orphanet_68336 - Total distinct nodes N: 430; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 428, Using 10\n",
      "\n",
      "Processing disease: EFO_0000616\n",
      "EFO_0000616 - Total distinct nodes N: 479; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 477, Using 10\n",
      "\n",
      "Processing disease: EFO_0000305\n",
      "EFO_0000305 - Total distinct nodes N: 128; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 126, Using 10\n",
      "\n",
      "Processing disease: EFO_0009406\n",
      "EFO_0009406 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: EFO_0000537\n",
      "EFO_0000537 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "\n",
      "Processing disease: EFO_0004306\n",
      "EFO_0004306 - Total distinct nodes N: 108; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 106, Using 10\n",
      "\n",
      "Processing disease: EFO_0004735\n",
      "EFO_0004735 - Total distinct nodes N: 1; Chosen d: 2\n",
      "Skipping EFO_0004735: too few nodes (1)\n",
      "\n",
      "Processing disease: EFO_0001663\n",
      "EFO_0001663 - Total distinct nodes N: 130; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 128, Using 10\n",
      "\n",
      "Processing disease: EFO_0004529\n",
      "EFO_0004529 - Total distinct nodes N: 216; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 214, Using 10\n",
      "\n",
      "Processing disease: EFO_0004736\n",
      "EFO_0004736 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_0000677\n",
      "EFO_0000677 - Total distinct nodes N: 22; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 20, Using 10\n",
      "\n",
      "Processing disease: EFO_0000319\n",
      "EFO_0000319 - Total distinct nodes N: 301; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 299, Using 10\n",
      "\n",
      "Processing disease: EFO_0004842\n",
      "EFO_0004842 - Total distinct nodes N: 28; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 26, Using 10\n",
      "\n",
      "Processing disease: EFO_0009605\n",
      "EFO_0009605 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: EFO_0005772\n",
      "EFO_0005772 - Total distinct nodes N: 38; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 36, Using 10\n",
      "\n",
      "Processing disease: EFO_0000313\n",
      "EFO_0000313 - Total distinct nodes N: 391; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 389, Using 10\n",
      "\n",
      "Processing disease: EFO_0000540\n",
      "EFO_0000540 - Total distinct nodes N: 354; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 352, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000653\n",
      "MONDO_0000653 - Total distinct nodes N: 229; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 227, Using 10\n",
      "\n",
      "Processing disease: EFO_0005140\n",
      "EFO_0005140 - Total distinct nodes N: 118; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 116, Using 10\n",
      "\n",
      "Processing disease: EFO_0002422\n",
      "EFO_0002422 - Total distinct nodes N: 356; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 354, Using 10\n",
      "\n",
      "Processing disease: EFO_0008549\n",
      "EFO_0008549 - Total distinct nodes N: 288; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 286, Using 10\n",
      "\n",
      "Processing disease: EFO_0003086\n",
      "EFO_0003086 - Total distinct nodes N: 143; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 141, Using 10\n",
      "\n",
      "Processing disease: EFO_0004324\n",
      "EFO_0004324 - Total distinct nodes N: 84; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 82, Using 10\n",
      "\n",
      "Processing disease: EFO_0003859\n",
      "EFO_0003859 - Total distinct nodes N: 440; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 438, Using 10\n",
      "\n",
      "Processing disease: EFO_0006336\n",
      "EFO_0006336 - Total distinct nodes N: 16; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 14, Using 10\n",
      "\n",
      "Processing disease: EFO_0004627\n",
      "EFO_0004627 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_0009549\n",
      "EFO_0009549 - Total distinct nodes N: 110; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 108, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000426\n",
      "MONDO_0000426 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_0004725\n",
      "EFO_0004725 - Total distinct nodes N: 151; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 149, Using 10\n",
      "\n",
      "Processing disease: EFO_0004305\n",
      "EFO_0004305 - Total distinct nodes N: 44; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 42, Using 10\n",
      "\n",
      "Processing disease: EFO_0004531\n",
      "EFO_0004531 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_0004730\n",
      "EFO_0004730 - Total distinct nodes N: 24; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 22, Using 10\n",
      "\n",
      "Processing disease: EFO_0005856\n",
      "EFO_0005856 - Total distinct nodes N: 11; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 9, Using 9\n",
      "\n",
      "Processing disease: EFO_0007985\n",
      "EFO_0007985 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "Attempt 1 failed: Cannot sample 2 negative nodes from a set of 1 negative nodes for R-HSA-109582\n",
      "Retrying with 1 negatives\n",
      "\n",
      "Processing disease: EFO_0000512\n",
      "EFO_0000512 - Total distinct nodes N: 95; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 93, Using 10\n",
      "\n",
      "Processing disease: MONDO_0004992\n",
      "MONDO_0004992 - Total distinct nodes N: 511; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 509, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021193\n",
      "MONDO_0021193 - Total distinct nodes N: 62; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 60, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021248\n",
      "MONDO_0021248 - Total distinct nodes N: 165; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 163, Using 10\n",
      "\n",
      "Processing disease: MONDO_0017343\n",
      "MONDO_0017343 - Total distinct nodes N: 27; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 25, Using 10\n",
      "\n",
      "Processing disease: MONDO_0017595\n",
      "MONDO_0017595 - Total distinct nodes N: 9; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 7, Using 7\n",
      "\n",
      "Processing disease: EFO_1000627\n",
      "EFO_1000627 - Total distinct nodes N: 29; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 27, Using 10\n",
      "\n",
      "Processing disease: EFO_0000565\n",
      "EFO_0000565 - Total distinct nodes N: 46; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 44, Using 10\n",
      "\n",
      "Processing disease: EFO_1000068\n",
      "EFO_1000068 - Total distinct nodes N: 62; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 60, Using 10\n",
      "\n",
      "Processing disease: EFO_0004838\n",
      "EFO_0004838 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: HP_0000478\n",
      "HP_0000478 - Total distinct nodes N: 14; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 12, Using 10\n",
      "\n",
      "Processing disease: EFO_0009555\n",
      "EFO_0009555 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: MONDO_0004095\n",
      "MONDO_0004095 - Total distinct nodes N: 192; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 190, Using 10\n",
      "\n",
      "Processing disease: EFO_0010642\n",
      "EFO_0010642 - Total distinct nodes N: 9; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 7, Using 7\n",
      "\n",
      "Processing disease: HP_0002011\n",
      "HP_0002011 - Total distinct nodes N: 38; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 36, Using 10\n",
      "\n",
      "Processing disease: EFO_0005689\n",
      "EFO_0005689 - Total distinct nodes N: 25; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 23, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021583\n",
      "MONDO_0021583 - Total distinct nodes N: 135; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 133, Using 10\n",
      "\n",
      "Processing disease: EFO_0007392\n",
      "EFO_0007392 - Total distinct nodes N: 89; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 87, Using 10\n",
      "\n",
      "Processing disease: MONDO_0024276\n",
      "MONDO_0024276 - Total distinct nodes N: 81; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 79, Using 10\n",
      "\n",
      "Processing disease: EFO_1000172\n",
      "EFO_1000172 - Total distinct nodes N: 208; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 206, Using 10\n",
      "\n",
      "Processing disease: EFO_0001075\n",
      "EFO_0001075 - Total distinct nodes N: 156; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 154, Using 10\n",
      "\n",
      "Processing disease: EFO_0007991\n",
      "EFO_0007991 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: MONDO_0002406\n",
      "MONDO_0002406 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "\n",
      "Processing disease: EFO_0010934\n",
      "EFO_0010934 - Total distinct nodes N: 1; Chosen d: 2\n",
      "Skipping EFO_0010934: too few nodes (1)\n",
      "\n",
      "Processing disease: MONDO_0008170\n",
      "MONDO_0008170 - Total distinct nodes N: 48; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 46, Using 10\n",
      "\n",
      "Processing disease: EFO_0000503\n",
      "EFO_0000503 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_0004149\n",
      "EFO_0004149 - Total distinct nodes N: 37; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 35, Using 10\n",
      "\n",
      "Processing disease: EFO_0007789\n",
      "EFO_0007789 - Total distinct nodes N: 1; Chosen d: 2\n",
      "Skipping EFO_0007789: too few nodes (1)\n",
      "\n",
      "Processing disease: MONDO_0021143\n",
      "MONDO_0021143 - Total distinct nodes N: 234; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 232, Using 10\n",
      "\n",
      "Processing disease: EFO_0004617\n",
      "EFO_0004617 - Total distinct nodes N: 1; Chosen d: 2\n",
      "Skipping EFO_0004617: too few nodes (1)\n",
      "\n",
      "Processing disease: EFO_0009259\n",
      "EFO_0009259 - Total distinct nodes N: 91; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 89, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021662\n",
      "MONDO_0021662 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: MONDO_0003060\n",
      "MONDO_0003060 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: EFO_0009119\n",
      "EFO_0009119 - Total distinct nodes N: 275; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 273, Using 10\n",
      "\n",
      "Processing disease: MONDO_0008903\n",
      "MONDO_0008903 - Total distinct nodes N: 31; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 29, Using 10\n",
      "\n",
      "Processing disease: EFO_0000096\n",
      "EFO_0000096 - Total distinct nodes N: 178; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 176, Using 10\n",
      "\n",
      "Processing disease: EFO_0000708\n",
      "EFO_0000708 - Total distinct nodes N: 23; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 21, Using 10\n",
      "\n",
      "Processing disease: EFO_0000466\n",
      "EFO_0000466 - Total distinct nodes N: 100; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 98, Using 10\n",
      "\n",
      "Processing disease: EFO_0004784\n",
      "EFO_0004784 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_0004458\n",
      "EFO_0004458 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "\n",
      "Processing disease: EFO_0003818\n",
      "EFO_0003818 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0003818: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0002427\n",
      "EFO_0002427 - Total distinct nodes N: 36; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 34, Using 10\n",
      "\n",
      "Processing disease: EFO_0004142\n",
      "EFO_0004142 - Total distinct nodes N: 38; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 36, Using 10\n",
      "\n",
      "Processing disease: MONDO_0018364\n",
      "MONDO_0018364 - Total distinct nodes N: 108; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 106, Using 10\n",
      "\n",
      "Processing disease: EFO_0006848\n",
      "EFO_0006848 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_1002018\n",
      "EFO_1002018 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: MONDO_0007263\n",
      "MONDO_0007263 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: EFO_0000220\n",
      "EFO_0000220 - Total distinct nodes N: 138; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 136, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002229\n",
      "MONDO_0002229 - Total distinct nodes N: 111; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 109, Using 10\n",
      "\n",
      "Processing disease: EFO_0002425\n",
      "EFO_0002425 - Total distinct nodes N: 113; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 111, Using 10\n",
      "\n",
      "Processing disease: MONDO_0004867\n",
      "MONDO_0004867 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: MONDO_0002529\n",
      "MONDO_0002529 - Total distinct nodes N: 62; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 60, Using 10\n",
      "\n",
      "Processing disease: EFO_0005548\n",
      "EFO_0005548 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "\n",
      "Processing disease: EFO_0004741\n",
      "EFO_0004741 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: MONDO_0024477\n",
      "MONDO_0024477 - Total distinct nodes N: 66; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 64, Using 10\n",
      "\n",
      "Processing disease: EFO_0803539\n",
      "EFO_0803539 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: EFO_0007330\n",
      "EFO_0007330 - Total distinct nodes N: 49; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 47, Using 10\n",
      "\n",
      "Processing disease: EFO_0000681\n",
      "EFO_0000681 - Total distinct nodes N: 28; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 26, Using 10\n",
      "\n",
      "Processing disease: EFO_0004314\n",
      "EFO_0004314 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: MONDO_0002367\n",
      "MONDO_0002367 - Total distinct nodes N: 33; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 31, Using 10\n",
      "\n",
      "Processing disease: EFO_0005952\n",
      "EFO_0005952 - Total distinct nodes N: 178; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 176, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021118\n",
      "MONDO_0021118 - Total distinct nodes N: 67; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 65, Using 10\n",
      "\n",
      "Processing disease: HP_0012443\n",
      "HP_0012443 - Total distinct nodes N: 22; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 20, Using 10\n",
      "\n",
      "Processing disease: EFO_0010967\n",
      "EFO_0010967 - Total distinct nodes N: 1; Chosen d: 2\n",
      "Skipping EFO_0010967: too few nodes (1)\n",
      "\n",
      "Processing disease: MONDO_0005575\n",
      "MONDO_0005575 - Total distinct nodes N: 33; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 31, Using 10\n",
      "\n",
      "Processing disease: EFO_0003865\n",
      "EFO_0003865 - Total distinct nodes N: 59; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 57, Using 10\n",
      "\n",
      "Processing disease: EFO_0007993\n",
      "EFO_0007993 - Total distinct nodes N: 12; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 10, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000429\n",
      "MONDO_0000429 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "Attempt 1 failed: Cannot sample 4 negative nodes from a set of 3 negative nodes for R-HSA-9609507\n",
      "Retrying with 2 negatives\n",
      "\n",
      "Processing disease: MONDO_0002516\n",
      "MONDO_0002516 - Total distinct nodes N: 395; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 393, Using 10\n",
      "\n",
      "Processing disease: HP_0001574\n",
      "HP_0001574 - Total distinct nodes N: 9; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 7, Using 7\n",
      "Attempt 1 failed: Cannot sample 7 negative nodes from a set of 6 negative nodes for R-HSA-1266738\n",
      "Retrying with 5 negatives\n",
      "\n",
      "Processing disease: EFO_0003853\n",
      "EFO_0003853 - Total distinct nodes N: 61; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 59, Using 10\n",
      "\n",
      "Processing disease: EFO_0000403\n",
      "EFO_0000403 - Total distinct nodes N: 14; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 12, Using 10\n",
      "\n",
      "Processing disease: EFO_0001645\n",
      "EFO_0001645 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: EFO_0003769\n",
      "EFO_0003769 - Total distinct nodes N: 184; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 182, Using 10\n",
      "\n",
      "Processing disease: EFO_0005708\n",
      "EFO_0005708 - Total distinct nodes N: 40; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 38, Using 10\n",
      "\n",
      "Processing disease: EFO_0004323\n",
      "EFO_0004323 - Total distinct nodes N: 1; Chosen d: 2\n",
      "Skipping EFO_0004323: too few nodes (1)\n",
      "\n",
      "Processing disease: EFO_0009270\n",
      "EFO_0009270 - Total distinct nodes N: 1; Chosen d: 2\n",
      "Skipping EFO_0009270: too few nodes (1)\n",
      "\n",
      "Processing disease: MONDO_0005148\n",
      "MONDO_0005148 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "\n",
      "Processing disease: HP_0001871\n",
      "HP_0001871 - Total distinct nodes N: 84; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 82, Using 10\n",
      "\n",
      "Processing disease: EFO_0010968\n",
      "EFO_0010968 - Total distinct nodes N: 34; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 32, Using 10\n",
      "\n",
      "Processing disease: EFO_0000756\n",
      "EFO_0000756 - Total distinct nodes N: 176; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 174, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021634\n",
      "MONDO_0021634 - Total distinct nodes N: 106; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 104, Using 10\n",
      "\n",
      "Processing disease: EFO_0004731\n",
      "EFO_0004731 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "\n",
      "Processing disease: EFO_0007984\n",
      "EFO_0007984 - Total distinct nodes N: 31; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 29, Using 10\n",
      "\n",
      "Processing disease: EFO_0000326\n",
      "EFO_0000326 - Total distinct nodes N: 98; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 96, Using 10\n",
      "\n",
      "Processing disease: EFO_0005950\n",
      "EFO_0005950 - Total distinct nodes N: 208; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 206, Using 10\n",
      "\n",
      "Processing disease: EFO_0002892\n",
      "EFO_0002892 - Total distinct nodes N: 20; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 18, Using 10\n",
      "\n",
      "Processing disease: EFO_0001642\n",
      "EFO_0001642 - Total distinct nodes N: 122; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 120, Using 10\n",
      "\n",
      "Processing disease: EFO_0003891\n",
      "EFO_0003891 - Total distinct nodes N: 30; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 28, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002532\n",
      "MONDO_0002532 - Total distinct nodes N: 117; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 115, Using 10\n",
      "\n",
      "Processing disease: EFO_0000318\n",
      "EFO_0000318 - Total distinct nodes N: 103; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 101, Using 10\n",
      "\n",
      "Processing disease: MONDO_0004979\n",
      "MONDO_0004979 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_0011011\n",
      "EFO_0011011 - Total distinct nodes N: 26; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 24, Using 10\n",
      "\n",
      "Processing disease: MONDO_0017341\n",
      "MONDO_0017341 - Total distinct nodes N: 49; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 47, Using 10\n",
      "\n",
      "Processing disease: EFO_0008550\n",
      "EFO_0008550 - Total distinct nodes N: 31; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 29, Using 10\n",
      "\n",
      "Processing disease: EFO_0000389\n",
      "EFO_0000389 - Total distinct nodes N: 58; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 56, Using 10\n",
      "\n",
      "Processing disease: EFO_1000218\n",
      "EFO_1000218 - Total distinct nodes N: 274; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 272, Using 10\n",
      "\n",
      "Processing disease: EFO_0003925\n",
      "EFO_0003925 - Total distinct nodes N: 1; Chosen d: 2\n",
      "Skipping EFO_0003925: too few nodes (1)\n",
      "\n",
      "Processing disease: MONDO_0005178\n",
      "MONDO_0005178 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_0005763\n",
      "EFO_0005763 - Total distinct nodes N: 22; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 20, Using 10\n",
      "\n",
      "Processing disease: HP_0011025\n",
      "HP_0011025 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: MONDO_0017342\n",
      "MONDO_0017342 - Total distinct nodes N: 45; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 43, Using 10\n",
      "\n",
      "Processing disease: EFO_0005670\n",
      "EFO_0005670 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: MONDO_0003409\n",
      "MONDO_0003409 - Total distinct nodes N: 15; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 13, Using 10\n",
      "\n",
      "Processing disease: EFO_0004312\n",
      "EFO_0004312 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: EFO_1000158\n",
      "EFO_1000158 - Total distinct nodes N: 214; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 212, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000836\n",
      "MONDO_0000836 - Total distinct nodes N: 1; Chosen d: 2\n",
      "Skipping MONDO_0000836: too few nodes (1)\n",
      "\n",
      "Processing disease: EFO_0004532\n",
      "EFO_0004532 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: EFO_0000707\n",
      "EFO_0000707 - Total distinct nodes N: 55; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 53, Using 10\n",
      "\n",
      "Processing disease: MONDO_0001627\n",
      "MONDO_0001627 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: MONDO_0002898\n",
      "MONDO_0002898 - Total distinct nodes N: 227; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 225, Using 10\n",
      "\n",
      "Processing disease: EFO_0000304\n",
      "EFO_0000304 - Total distinct nodes N: 38; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 36, Using 10\n",
      "\n",
      "Processing disease: EFO_0003767\n",
      "EFO_0003767 - Total distinct nodes N: 16; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 14, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021117\n",
      "MONDO_0021117 - Total distinct nodes N: 53; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 51, Using 10\n",
      "\n",
      "Processing disease: EFO_1001938\n",
      "EFO_1001938 - Total distinct nodes N: 67; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 65, Using 10\n",
      "\n",
      "Processing disease: EFO_0002890\n",
      "EFO_0002890 - Total distinct nodes N: 17; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 15, Using 10\n",
      "\n",
      "Processing disease: EFO_0004289\n",
      "EFO_0004289 - Total distinct nodes N: 74; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 72, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002108\n",
      "MONDO_0002108 - Total distinct nodes N: 44; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 42, Using 10\n",
      "\n",
      "Processing disease: EFO_0002917\n",
      "EFO_0002917 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: HP_0000951\n",
      "HP_0000951 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_0002970\n",
      "EFO_0002970 - Total distinct nodes N: 39; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 37, Using 10\n",
      "\n",
      "Processing disease: EFO_0000348\n",
      "EFO_0000348 - Total distinct nodes N: 195; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 193, Using 10\n",
      "\n",
      "Processing disease: MONDO_0020665\n",
      "MONDO_0020665 - Total distinct nodes N: 95; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 93, Using 10\n",
      "\n",
      "Processing disease: MONDO_0005341\n",
      "MONDO_0005341 - Total distinct nodes N: 70; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 68, Using 10\n",
      "\n",
      "Processing disease: MONDO_0004805\n",
      "MONDO_0004805 - Total distinct nodes N: 27; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 25, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000376\n",
      "MONDO_0000376 - Total distinct nodes N: 51; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 49, Using 10\n",
      "\n",
      "Processing disease: EFO_0004536\n",
      "EFO_0004536 - Total distinct nodes N: 1; Chosen d: 2\n",
      "Skipping EFO_0004536: too few nodes (1)\n",
      "\n",
      "Processing disease: MONDO_0021251\n",
      "MONDO_0021251 - Total distinct nodes N: 154; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 152, Using 10\n",
      "\n",
      "Processing disease: EFO_0005543\n",
      "EFO_0005543 - Total distinct nodes N: 71; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 69, Using 10\n",
      "\n",
      "Processing disease: EFO_0000272\n",
      "EFO_0000272 - Total distinct nodes N: 48; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 46, Using 10\n",
      "\n",
      "Processing disease: EFO_1000417\n",
      "EFO_1000417 - Total distinct nodes N: 35; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 33, Using 10\n",
      "\n",
      "Processing disease: EFO_0008528\n",
      "EFO_0008528 - Total distinct nodes N: 254; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 252, Using 10\n",
      "\n",
      "Processing disease: EFO_0004193\n",
      "EFO_0004193 - Total distinct nodes N: 21; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 19, Using 10\n",
      "\n",
      "Processing disease: MONDO_0011962\n",
      "MONDO_0011962 - Total distinct nodes N: 147; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 145, Using 10\n",
      "\n",
      "Processing disease: MONDO_0044937\n",
      "MONDO_0044937 - Total distinct nodes N: 211; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 209, Using 10\n",
      "\n",
      "Processing disease: EFO_0000673\n",
      "EFO_0000673 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0000673: too few nodes (2)\n",
      "\n",
      "Processing disease: MONDO_0004986\n",
      "MONDO_0004986 - Total distinct nodes N: 324; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 322, Using 10\n",
      "\n",
      "Processing disease: EFO_0000182\n",
      "EFO_0000182 - Total distinct nodes N: 101; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 99, Using 10\n",
      "\n",
      "Processing disease: MONDO_0003812\n",
      "MONDO_0003812 - Total distinct nodes N: 82; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 80, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002480\n",
      "MONDO_0002480 - Total distinct nodes N: 39; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 37, Using 10\n",
      "\n",
      "Processing disease: EFO_0000181\n",
      "EFO_0000181 - Total distinct nodes N: 138; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 136, Using 10\n",
      "\n",
      "Processing disease: HP_0002597\n",
      "HP_0002597 - Total distinct nodes N: 11; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 9, Using 9\n",
      "\n",
      "Processing disease: MONDO_0002120\n",
      "MONDO_0002120 - Total distinct nodes N: 20; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 18, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000621\n",
      "MONDO_0000621 - Total distinct nodes N: 162; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 160, Using 10\n",
      "\n",
      "Processing disease: EFO_1000646\n",
      "EFO_1000646 - Total distinct nodes N: 52; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 50, Using 10\n",
      "\n",
      "Processing disease: EFO_1001950\n",
      "EFO_1001950 - Total distinct nodes N: 18; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 16, Using 10\n",
      "\n",
      "Processing disease: EFO_0005232\n",
      "EFO_0005232 - Total distinct nodes N: 177; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 175, Using 10\n",
      "\n",
      "Processing disease: EFO_0002618\n",
      "EFO_0002618 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_0005134\n",
      "EFO_0005134 - Total distinct nodes N: 16; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 14, Using 10\n",
      "\n",
      "Processing disease: EFO_0009260\n",
      "EFO_0009260 - Total distinct nodes N: 31; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 29, Using 10\n",
      "\n",
      "Processing disease: MONDO_0018531\n",
      "MONDO_0018531 - Total distinct nodes N: 25; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 23, Using 10\n",
      "\n",
      "Processing disease: HP_0100543\n",
      "HP_0100543 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: EFO_1001463\n",
      "EFO_1001463 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: EFO_1001471\n",
      "EFO_1001471 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_1001471: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_1000021\n",
      "EFO_1000021 - Total distinct nodes N: 191; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 189, Using 10\n",
      "\n",
      "Processing disease: EFO_1000416\n",
      "EFO_1000416 - Total distinct nodes N: 64; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 62, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002033\n",
      "MONDO_0002033 - Total distinct nodes N: 198; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 196, Using 10\n",
      "\n",
      "Processing disease: EFO_0006859\n",
      "EFO_0006859 - Total distinct nodes N: 217; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 215, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021096\n",
      "MONDO_0021096 - Total distinct nodes N: 58; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 56, Using 10\n",
      "\n",
      "Processing disease: EFO_0000178\n",
      "EFO_0000178 - Total distinct nodes N: 22; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 20, Using 10\n",
      "\n",
      "Processing disease: EFO_0003060\n",
      "EFO_0003060 - Total distinct nodes N: 20; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 18, Using 10\n",
      "\n",
      "Processing disease: EFO_0007861\n",
      "EFO_0007861 - Total distinct nodes N: 9; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 7, Using 7\n",
      "\n",
      "Processing disease: EFO_1001763\n",
      "EFO_1001763 - Total distinct nodes N: 18; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 16, Using 10\n",
      "\n",
      "Processing disease: HP_0011446\n",
      "HP_0011446 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: EFO_0005570\n",
      "EFO_0005570 - Total distinct nodes N: 258; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 256, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002038\n",
      "MONDO_0002038 - Total distinct nodes N: 206; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 204, Using 10\n",
      "\n",
      "Processing disease: EFO_0000341\n",
      "EFO_0000341 - Total distinct nodes N: 1; Chosen d: 2\n",
      "Skipping EFO_0000341: too few nodes (1)\n",
      "\n",
      "Processing disease: MONDO_0037254\n",
      "MONDO_0037254 - Total distinct nodes N: 250; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 248, Using 10\n",
      "\n",
      "Processing disease: EFO_0000232\n",
      "EFO_0000232 - Total distinct nodes N: 81; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 79, Using 10\n",
      "\n",
      "Processing disease: MONDO_0015760\n",
      "MONDO_0015760 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_0009255\n",
      "EFO_0009255 - Total distinct nodes N: 71; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 69, Using 10\n",
      "\n",
      "Processing disease: EFO_0000275\n",
      "EFO_0000275 - Total distinct nodes N: 9; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 7, Using 7\n",
      "\n",
      "Processing disease: MONDO_0004251\n",
      "MONDO_0004251 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: MONDO_0021636\n",
      "MONDO_0021636 - Total distinct nodes N: 36; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 34, Using 10\n",
      "\n",
      "Processing disease: EFO_1000143\n",
      "EFO_1000143 - Total distinct nodes N: 171; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 169, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000629\n",
      "MONDO_0000629 - Total distinct nodes N: 32; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 30, Using 10\n",
      "\n",
      "Processing disease: EFO_0008591\n",
      "EFO_0008591 - Total distinct nodes N: 36; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 34, Using 10\n",
      "\n",
      "Processing disease: EFO_1000350\n",
      "EFO_1000350 - Total distinct nodes N: 127; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 125, Using 10\n",
      "\n",
      "Processing disease: EFO_0009387\n",
      "EFO_0009387 - Total distinct nodes N: 24; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 22, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021063\n",
      "MONDO_0021063 - Total distinct nodes N: 24; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 22, Using 10\n",
      "\n",
      "Processing disease: EFO_0004639\n",
      "EFO_0004639 - Total distinct nodes N: 33; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 31, Using 10\n",
      "\n",
      "Processing disease: EFO_1001902\n",
      "EFO_1001902 - Total distinct nodes N: 40; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 38, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000637\n",
      "MONDO_0000637 - Total distinct nodes N: 106; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 104, Using 10\n",
      "\n",
      "Processing disease: EFO_0006318\n",
      "EFO_0006318 - Total distinct nodes N: 120; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 118, Using 10\n",
      "\n",
      "Processing disease: HP_0012759\n",
      "HP_0012759 - Total distinct nodes N: 9; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 7, Using 7\n",
      "\n",
      "Processing disease: MONDO_0024479\n",
      "MONDO_0024479 - Total distinct nodes N: 12; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 10, Using 10\n",
      "Attempt 1 failed: Cannot sample 10 negative nodes from a set of 8 negative nodes for R-HSA-162582\n",
      "Retrying with 8 negatives\n",
      "\n",
      "Processing disease: EFO_0006545\n",
      "EFO_0006545 - Total distinct nodes N: 21; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 19, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021069\n",
      "MONDO_0021069 - Total distinct nodes N: 48; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 46, Using 10\n",
      "\n",
      "Processing disease: MONDO_0001056\n",
      "MONDO_0001056 - Total distinct nodes N: 55; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 53, Using 10\n",
      "\n",
      "Processing disease: EFO_0005540\n",
      "EFO_0005540 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_0000763\n",
      "EFO_0000763 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_0001071\n",
      "EFO_0001071 - Total distinct nodes N: 68; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 66, Using 10\n",
      "\n",
      "Processing disease: EFO_0004570\n",
      "EFO_0004570 - Total distinct nodes N: 15; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 13, Using 10\n",
      "\n",
      "Processing disease: MONDO_0015756\n",
      "MONDO_0015756 - Total distinct nodes N: 22; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 20, Using 10\n",
      "\n",
      "Processing disease: EFO_0000246\n",
      "EFO_0000246 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: EFO_1000601\n",
      "EFO_1000601 - Total distinct nodes N: 256; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 254, Using 10\n",
      "\n",
      "Processing disease: EFO_0004343\n",
      "EFO_0004343 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: MONDO_0024476\n",
      "MONDO_0024476 - Total distinct nodes N: 218; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 216, Using 10\n",
      "\n",
      "Processing disease: EFO_1001949\n",
      "EFO_1001949 - Total distinct nodes N: 201; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 199, Using 10\n",
      "\n",
      "Processing disease: EFO_0010351\n",
      "EFO_0010351 - Total distinct nodes N: 37; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 35, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021076\n",
      "MONDO_0021076 - Total distinct nodes N: 21; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 19, Using 10\n",
      "\n",
      "Processing disease: EFO_0003897\n",
      "EFO_0003897 - Total distinct nodes N: 46; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 44, Using 10\n",
      "\n",
      "Processing disease: EFO_0005220\n",
      "EFO_0005220 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_0002571\n",
      "EFO_0002571 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: EFO_0004288\n",
      "EFO_0004288 - Total distinct nodes N: 36; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 34, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002691\n",
      "MONDO_0002691 - Total distinct nodes N: 26; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 24, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021355\n",
      "MONDO_0021355 - Total distinct nodes N: 14; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 12, Using 10\n",
      "\n",
      "Processing disease: EFO_1001901\n",
      "EFO_1001901 - Total distinct nodes N: 62; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 60, Using 10\n",
      "\n",
      "Processing disease: MONDO_0007576\n",
      "MONDO_0007576 - Total distinct nodes N: 18; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 16, Using 10\n",
      "\n",
      "Processing disease: EFO_0005592\n",
      "EFO_0005592 - Total distinct nodes N: 15; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 13, Using 10\n",
      "\n",
      "Processing disease: EFO_0000294\n",
      "EFO_0000294 - Total distinct nodes N: 319; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 317, Using 10\n",
      "\n",
      "Processing disease: EFO_0005631\n",
      "EFO_0005631 - Total distinct nodes N: 68; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 66, Using 10\n",
      "\n",
      "Processing disease: EFO_0003820\n",
      "EFO_0003820 - Total distinct nodes N: 31; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 29, Using 10\n",
      "\n",
      "Processing disease: EFO_0007990\n",
      "EFO_0007990 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "\n",
      "Processing disease: EFO_0009483\n",
      "EFO_0009483 - Total distinct nodes N: 83; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 81, Using 10\n",
      "\n",
      "Processing disease: EFO_0010176\n",
      "EFO_0010176 - Total distinct nodes N: 30; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 28, Using 10\n",
      "\n",
      "Processing disease: EFO_0008524\n",
      "EFO_0008524 - Total distinct nodes N: 21; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 19, Using 10\n",
      "\n",
      "Processing disease: MONDO_0037255\n",
      "MONDO_0037255 - Total distinct nodes N: 153; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 151, Using 10\n",
      "\n",
      "Processing disease: EFO_0007989\n",
      "EFO_0007989 - Total distinct nodes N: 26; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 24, Using 10\n",
      "\n",
      "Processing disease: EFO_0002428\n",
      "EFO_0002428 - Total distinct nodes N: 18; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 16, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000956\n",
      "MONDO_0000956 - Total distinct nodes N: 18; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 16, Using 10\n",
      "\n",
      "Processing disease: EFO_1001512\n",
      "EFO_1001512 - Total distinct nodes N: 77; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 75, Using 10\n",
      "\n",
      "Processing disease: MONDO_0017594\n",
      "MONDO_0017594 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: EFO_0009534\n",
      "EFO_0009534 - Total distinct nodes N: 12; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 10, Using 10\n",
      "\n",
      "Processing disease: MONDO_0005374\n",
      "MONDO_0005374 - Total distinct nodes N: 19; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 17, Using 10\n",
      "\n",
      "Processing disease: EFO_1000613\n",
      "EFO_1000613 - Total distinct nodes N: 220; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 218, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002616\n",
      "MONDO_0002616 - Total distinct nodes N: 27; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 25, Using 10\n",
      "\n",
      "Processing disease: EFO_0000222\n",
      "EFO_0000222 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: MONDO_0003059\n",
      "MONDO_0003059 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: EFO_0006460\n",
      "EFO_0006460 - Total distinct nodes N: 112; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 110, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021138\n",
      "MONDO_0021138 - Total distinct nodes N: 17; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 15, Using 10\n",
      "\n",
      "Processing disease: EFO_1000657\n",
      "EFO_1000657 - Total distinct nodes N: 237; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 235, Using 10\n",
      "\n",
      "Processing disease: MONDO_0044334\n",
      "MONDO_0044334 - Total distinct nodes N: 48; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 46, Using 10\n",
      "\n",
      "Processing disease: MONDO_0001014\n",
      "MONDO_0001014 - Total distinct nodes N: 21; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 19, Using 10\n",
      "\n",
      "Processing disease: EFO_0020946\n",
      "EFO_0020946 - Total distinct nodes N: 32; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 30, Using 10\n",
      "\n",
      "Processing disease: MONDO_0016680\n",
      "MONDO_0016680 - Total distinct nodes N: 23; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 21, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021043\n",
      "MONDO_0021043 - Total distinct nodes N: 49; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 47, Using 10\n",
      "\n",
      "Processing disease: EFO_0005922\n",
      "EFO_0005922 - Total distinct nodes N: 65; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 63, Using 10\n",
      "\n",
      "Processing disease: EFO_0004327\n",
      "EFO_0004327 - Total distinct nodes N: 9; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 7, Using 7\n",
      "\n",
      "Processing disease: MONDO_0002165\n",
      "MONDO_0002165 - Total distinct nodes N: 113; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 111, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000589\n",
      "MONDO_0000589 - Total distinct nodes N: 64; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 62, Using 10\n",
      "\n",
      "Processing disease: EFO_1000233\n",
      "EFO_1000233 - Total distinct nodes N: 225; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 223, Using 10\n",
      "\n",
      "Processing disease: EFO_0000519\n",
      "EFO_0000519 - Total distinct nodes N: 23; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 21, Using 10\n",
      "\n",
      "Processing disease: MONDO_0001187\n",
      "MONDO_0001187 - Total distinct nodes N: 306; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 304, Using 10\n",
      "\n",
      "Processing disease: MONDO_0100342\n",
      "MONDO_0100342 - Total distinct nodes N: 49; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 47, Using 10\n",
      "\n",
      "Processing disease: EFO_0003825\n",
      "EFO_0003825 - Total distinct nodes N: 264; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 262, Using 10\n",
      "\n",
      "Processing disease: EFO_0000095\n",
      "EFO_0000095 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0000095: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0003868\n",
      "EFO_0003868 - Total distinct nodes N: 291; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 289, Using 10\n",
      "\n",
      "Processing disease: MONDO_0037256\n",
      "MONDO_0037256 - Total distinct nodes N: 203; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 201, Using 10\n",
      "\n",
      "Processing disease: EFO_0009804\n",
      "EFO_0009804 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: MONDO_0002512\n",
      "MONDO_0002512 - Total distinct nodes N: 27; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 25, Using 10\n",
      "\n",
      "Processing disease: EFO_0000571\n",
      "EFO_0000571 - Total distinct nodes N: 32; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 30, Using 10\n",
      "\n",
      "Processing disease: EFO_0005588\n",
      "EFO_0005588 - Total distinct nodes N: 19; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 17, Using 10\n",
      "\n",
      "Processing disease: EFO_0000199\n",
      "EFO_0000199 - Total distinct nodes N: 131; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 129, Using 10\n",
      "\n",
      "Processing disease: EFO_1001951\n",
      "EFO_1001951 - Total distinct nodes N: 31; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 29, Using 10\n",
      "\n",
      "Processing disease: EFO_1000356\n",
      "EFO_1000356 - Total distinct nodes N: 117; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 115, Using 10\n",
      "\n",
      "Processing disease: MONDO_0005499\n",
      "MONDO_0005499 - Total distinct nodes N: 24; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 22, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000812\n",
      "MONDO_0000812 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: Orphanet_271847\n",
      "Orphanet_271847 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_1000017\n",
      "EFO_1000017 - Total distinct nodes N: 9; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 7, Using 7\n",
      "Attempt 1 failed: Cannot sample 7 negative nodes from a set of 6 negative nodes for R-HSA-9609507\n",
      "Retrying with 5 negatives\n",
      "\n",
      "Processing disease: EFO_0000209\n",
      "EFO_0000209 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_0020943\n",
      "EFO_0020943 - Total distinct nodes N: 32; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 30, Using 10\n",
      "\n",
      "Processing disease: MONDO_0020638\n",
      "MONDO_0020638 - Total distinct nodes N: 12; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 10, Using 10\n",
      "\n",
      "Processing disease: EFO_0003860\n",
      "EFO_0003860 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "\n",
      "Processing disease: EFO_0000195\n",
      "EFO_0000195 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: MONDO_0003193\n",
      "MONDO_0003193 - Total distinct nodes N: 72; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 70, Using 10\n",
      "\n",
      "Processing disease: EFO_0001073\n",
      "EFO_0001073 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: EFO_0000640\n",
      "EFO_0000640 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: EFO_0006544\n",
      "EFO_0006544 - Total distinct nodes N: 157; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 155, Using 10\n",
      "\n",
      "Processing disease: EFO_0000546\n",
      "EFO_0000546 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: MONDO_0000636\n",
      "MONDO_0000636 - Total distinct nodes N: 21; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 19, Using 10\n",
      "\n",
      "Processing disease: EFO_0000545\n",
      "EFO_0000545 - Total distinct nodes N: 1; Chosen d: 2\n",
      "Skipping EFO_0000545: too few nodes (1)\n",
      "\n",
      "Processing disease: MONDO_0021058\n",
      "MONDO_0021058 - Total distinct nodes N: 65; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 63, Using 10\n",
      "\n",
      "Processing disease: EFO_1000541\n",
      "EFO_1000541 - Total distinct nodes N: 32; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 30, Using 10\n",
      "\n",
      "Processing disease: MONDO_0007915\n",
      "MONDO_0007915 - Total distinct nodes N: 12; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 10, Using 10\n",
      "Attempt 1 failed: Cannot sample 10 negative nodes from a set of 9 negative nodes for R-HSA-168256\n",
      "Retrying with 8 negatives\n",
      "\n",
      "Processing disease: MONDO_0024757\n",
      "MONDO_0024757 - Total distinct nodes N: 28; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 26, Using 10\n",
      "\n",
      "Processing disease: HP_0002060\n",
      "HP_0002060 - Total distinct nodes N: 67; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 65, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002116\n",
      "MONDO_0002116 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_0007352\n",
      "EFO_0007352 - Total distinct nodes N: 230; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 228, Using 10\n",
      "\n",
      "Processing disease: HP_0030680\n",
      "HP_0030680 - Total distinct nodes N: 42; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 40, Using 10\n",
      "\n",
      "Processing disease: EFO_0004615\n",
      "EFO_0004615 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: MONDO_0000648\n",
      "MONDO_0000648 - Total distinct nodes N: 147; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 145, Using 10\n",
      "\n",
      "Processing disease: MONDO_0024337\n",
      "MONDO_0024337 - Total distinct nodes N: 253; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 251, Using 10\n",
      "\n",
      "Processing disease: MONDO_0004907\n",
      "MONDO_0004907 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: MONDO_0021631\n",
      "MONDO_0021631 - Total distinct nodes N: 21; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 19, Using 10\n",
      "\n",
      "Processing disease: EFO_0000200\n",
      "EFO_0000200 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0000200: too few nodes (2)\n",
      "\n",
      "Processing disease: MONDO_0000386\n",
      "MONDO_0000386 - Total distinct nodes N: 45; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 43, Using 10\n",
      "\n",
      "Processing disease: EFO_0000705\n",
      "EFO_0000705 - Total distinct nodes N: 71; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 69, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002928\n",
      "MONDO_0002928 - Total distinct nodes N: 202; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 200, Using 10\n",
      "\n",
      "Processing disease: EFO_0008515\n",
      "EFO_0008515 - Total distinct nodes N: 19; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 17, Using 10\n",
      "\n",
      "Processing disease: HP_0001939\n",
      "HP_0001939 - Total distinct nodes N: 75; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 73, Using 10\n",
      "\n",
      "Processing disease: MONDO_0005184\n",
      "MONDO_0005184 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping MONDO_0005184: too few nodes (2)\n",
      "\n",
      "Processing disease: HP_0000271\n",
      "HP_0000271 - Total distinct nodes N: 9; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 7, Using 7\n",
      "\n",
      "Processing disease: EFO_1000532\n",
      "EFO_1000532 - Total distinct nodes N: 125; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 123, Using 10\n",
      "\n",
      "Processing disease: EFO_1001956\n",
      "EFO_1001956 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "Attempt 1 failed: Cannot sample 8 negative nodes from a set of 7 negative nodes for R-HSA-74160\n",
      "Retrying with 6 negatives\n",
      "\n",
      "Processing disease: EFO_0009283\n",
      "EFO_0009283 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0009283: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0021796\n",
      "EFO_0021796 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_0004705\n",
      "EFO_0004705 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: HP_0012647\n",
      "HP_0012647 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_1000786\n",
      "EFO_1000786 - Total distinct nodes N: 1; Chosen d: 2\n",
      "Skipping EFO_1000786: too few nodes (1)\n",
      "\n",
      "Processing disease: EFO_0020944\n",
      "EFO_0020944 - Total distinct nodes N: 35; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 33, Using 10\n",
      "\n",
      "Processing disease: EFO_0000478\n",
      "EFO_0000478 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: MONDO_0044710\n",
      "MONDO_0044710 - Total distinct nodes N: 158; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 156, Using 10\n",
      "\n",
      "Processing disease: EFO_0003833\n",
      "EFO_0003833 - Total distinct nodes N: 141; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 139, Using 10\n",
      "\n",
      "Processing disease: MONDO_0024503\n",
      "MONDO_0024503 - Total distinct nodes N: 16; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 14, Using 10\n",
      "\n",
      "Processing disease: EFO_0000514\n",
      "EFO_0000514 - Total distinct nodes N: 30; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 28, Using 10\n",
      "\n",
      "Processing disease: MONDO_0024637\n",
      "MONDO_0024637 - Total distinct nodes N: 54; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 52, Using 10\n",
      "\n",
      "Processing disease: HP_0012649\n",
      "HP_0012649 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_0005423\n",
      "EFO_0005423 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0005423: too few nodes (2)\n",
      "\n",
      "Processing disease: MONDO_0021254\n",
      "MONDO_0021254 - Total distinct nodes N: 21; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 19, Using 10\n",
      "\n",
      "Processing disease: EFO_0003839\n",
      "EFO_0003839 - Total distinct nodes N: 16; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 14, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000383\n",
      "MONDO_0000383 - Total distinct nodes N: 29; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 27, Using 10\n",
      "\n",
      "Processing disease: EFO_1001047\n",
      "EFO_1001047 - Total distinct nodes N: 1; Chosen d: 2\n",
      "Skipping EFO_1001047: too few nodes (1)\n",
      "\n",
      "Processing disease: MONDO_0005271\n",
      "MONDO_0005271 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: MONDO_0019472\n",
      "MONDO_0019472 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: EFO_0008589\n",
      "EFO_0008589 - Total distinct nodes N: 38; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 36, Using 10\n",
      "\n",
      "Processing disease: EFO_0005090\n",
      "EFO_0005090 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_0003763\n",
      "EFO_0003763 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_1000255\n",
      "EFO_1000255 - Total distinct nodes N: 20; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 18, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000591\n",
      "MONDO_0000591 - Total distinct nodes N: 138; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 136, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000833\n",
      "MONDO_0000833 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: MONDO_0000594\n",
      "MONDO_0000594 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping MONDO_0000594: too few nodes (2)\n",
      "\n",
      "Processing disease: HP_0002715\n",
      "HP_0002715 - Total distinct nodes N: 51; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 49, Using 10\n",
      "\n",
      "Processing disease: EFO_1000304\n",
      "EFO_1000304 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "\n",
      "Processing disease: MONDO_0044925\n",
      "MONDO_0044925 - Total distinct nodes N: 273; Chosen d: 9\n",
      "Negative sampling: Requested 10, Safe maximum 271, Using 10\n",
      "\n",
      "Processing disease: MONDO_0001657\n",
      "MONDO_0001657 - Total distinct nodes N: 25; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 23, Using 10\n",
      "\n",
      "Processing disease: EFO_0003914\n",
      "EFO_0003914 - Total distinct nodes N: 1; Chosen d: 2\n",
      "Skipping EFO_0003914: too few nodes (1)\n",
      "\n",
      "Processing disease: MONDO_0004643\n",
      "MONDO_0004643 - Total distinct nodes N: 35; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 33, Using 10\n",
      "\n",
      "Processing disease: EFO_0005815\n",
      "EFO_0005815 - Total distinct nodes N: 67; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 65, Using 10\n",
      "\n",
      "Processing disease: EFO_0003841\n",
      "EFO_0003841 - Total distinct nodes N: 144; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 142, Using 10\n",
      "\n",
      "Processing disease: EFO_0000685\n",
      "EFO_0000685 - Total distinct nodes N: 28; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 26, Using 10\n",
      "\n",
      "Processing disease: EFO_0004280\n",
      "EFO_0004280 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: EFO_0001069\n",
      "EFO_0001069 - Total distinct nodes N: 15; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 13, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000640\n",
      "MONDO_0000640 - Total distinct nodes N: 15; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 13, Using 10\n",
      "\n",
      "Processing disease: EFO_1001968\n",
      "EFO_1001968 - Total distinct nodes N: 62; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 60, Using 10\n",
      "\n",
      "Processing disease: HP_0001977\n",
      "HP_0001977 - Total distinct nodes N: 11; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 9, Using 9\n",
      "\n",
      "Processing disease: EFO_0009282\n",
      "EFO_0009282 - Total distinct nodes N: 1; Chosen d: 2\n",
      "Skipping EFO_0009282: too few nodes (1)\n",
      "\n",
      "Processing disease: EFO_0002916\n",
      "EFO_0002916 - Total distinct nodes N: 114; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 112, Using 10\n",
      "\n",
      "Processing disease: EFO_0022196\n",
      "EFO_0022196 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: MONDO_0021375\n",
      "MONDO_0021375 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: MONDO_0024296\n",
      "MONDO_0024296 - Total distinct nodes N: 20; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 18, Using 10\n",
      "\n",
      "Processing disease: MONDO_0020663\n",
      "MONDO_0020663 - Total distinct nodes N: 47; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 45, Using 10\n",
      "\n",
      "Processing disease: EFO_1000044\n",
      "EFO_1000044 - Total distinct nodes N: 23; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 21, Using 10\n",
      "\n",
      "Processing disease: EFO_0004908\n",
      "EFO_0004908 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0004908: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0000706\n",
      "EFO_0000706 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: MONDO_0021545\n",
      "MONDO_0021545 - Total distinct nodes N: 1; Chosen d: 2\n",
      "Skipping MONDO_0021545: too few nodes (1)\n",
      "\n",
      "Processing disease: MONDO_0000588\n",
      "MONDO_0000588 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: HP_0001877\n",
      "HP_0001877 - Total distinct nodes N: 21; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 19, Using 10\n",
      "\n",
      "Processing disease: HP_0000598\n",
      "HP_0000598 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_1000359\n",
      "EFO_1000359 - Total distinct nodes N: 13; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 11, Using 10\n",
      "\n",
      "Processing disease: MONDO_0004670\n",
      "MONDO_0004670 - Total distinct nodes N: 11; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 9, Using 9\n",
      "Attempt 1 failed: Cannot sample 9 negative nodes from a set of 8 negative nodes for R-HSA-168256\n",
      "Retrying with 7 negatives\n",
      "\n",
      "Processing disease: MONDO_0005411\n",
      "MONDO_0005411 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: EFO_0004695\n",
      "EFO_0004695 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: MONDO_0040677\n",
      "MONDO_0040677 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: MONDO_0002977\n",
      "MONDO_0002977 - Total distinct nodes N: 1; Chosen d: 2\n",
      "Skipping MONDO_0002977: too few nodes (1)\n",
      "\n",
      "Processing disease: EFO_1000217\n",
      "EFO_1000217 - Total distinct nodes N: 39; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 37, Using 10\n",
      "\n",
      "Processing disease: EFO_0005775\n",
      "EFO_0005775 - Total distinct nodes N: 11; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 9, Using 9\n",
      "\n",
      "Processing disease: EFO_0004713\n",
      "EFO_0004713 - Total distinct nodes N: 9; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 7, Using 7\n",
      "Attempt 1 failed: Cannot sample 7 negative nodes from a set of 6 negative nodes for R-HSA-1474244\n",
      "Retrying with 5 negatives\n",
      "\n",
      "Processing disease: EFO_0004631\n",
      "EFO_0004631 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0004631: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_1001455\n",
      "EFO_1001455 - Total distinct nodes N: 17; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 15, Using 10\n",
      "\n",
      "Processing disease: EFO_1001185\n",
      "EFO_1001185 - Total distinct nodes N: 1; Chosen d: 2\n",
      "Skipping EFO_1001185: too few nodes (1)\n",
      "\n",
      "Processing disease: EFO_0004606\n",
      "EFO_0004606 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: EFO_0000538\n",
      "EFO_0000538 - Total distinct nodes N: 166; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 164, Using 10\n",
      "\n",
      "Processing disease: MONDO_0003061\n",
      "MONDO_0003061 - Total distinct nodes N: 11; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 9, Using 9\n",
      "Attempt 1 failed: Cannot sample 9 negative nodes from a set of 8 negative nodes for R-HSA-162582\n",
      "Retrying with 7 negatives\n",
      "\n",
      "Processing disease: EFO_0003851\n",
      "EFO_0003851 - Total distinct nodes N: 14; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 12, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021632\n",
      "MONDO_0021632 - Total distinct nodes N: 18; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 16, Using 10\n",
      "\n",
      "Processing disease: EFO_0020945\n",
      "EFO_0020945 - Total distinct nodes N: 26; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 24, Using 10\n",
      "\n",
      "Processing disease: EFO_0001378\n",
      "EFO_0001378 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0001378: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0004729\n",
      "EFO_0004729 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: MONDO_0023644\n",
      "MONDO_0023644 - Total distinct nodes N: 226; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 224, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000726\n",
      "MONDO_0000726 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping MONDO_0000726: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0000474\n",
      "EFO_0000474 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_1000307\n",
      "EFO_1000307 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "\n",
      "Processing disease: EFO_0000384\n",
      "EFO_0000384 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_1002050\n",
      "EFO_1002050 - Total distinct nodes N: 27; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 25, Using 10\n",
      "\n",
      "Processing disease: EFO_0008002\n",
      "EFO_0008002 - Total distinct nodes N: 1; Chosen d: 2\n",
      "Skipping EFO_0008002: too few nodes (1)\n",
      "\n",
      "Processing disease: MONDO_0002256\n",
      "MONDO_0002256 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping MONDO_0002256: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0003872\n",
      "EFO_0003872 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: HP_0001249\n",
      "HP_0001249 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_0000676\n",
      "EFO_0000676 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: EFO_1000223\n",
      "EFO_1000223 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: EFO_0006812\n",
      "EFO_0006812 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "\n",
      "Processing disease: MONDO_0021080\n",
      "MONDO_0021080 - Total distinct nodes N: 38; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 36, Using 10\n",
      "\n",
      "Processing disease: EFO_0005110\n",
      "EFO_0005110 - Total distinct nodes N: 41; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 39, Using 10\n",
      "\n",
      "Processing disease: EFO_0004273\n",
      "EFO_0004273 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_1000636\n",
      "EFO_1000636 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: MONDO_0002917\n",
      "MONDO_0002917 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_0003888\n",
      "EFO_0003888 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: MONDO_0001572\n",
      "MONDO_0001572 - Total distinct nodes N: 14; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 12, Using 10\n",
      "\n",
      "Processing disease: EFO_0003100\n",
      "EFO_0003100 - Total distinct nodes N: 13; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 11, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002232\n",
      "MONDO_0002232 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_1000121\n",
      "EFO_1000121 - Total distinct nodes N: 14; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 12, Using 10\n",
      "\n",
      "Processing disease: EFO_0000662\n",
      "EFO_0000662 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: HP_0025142\n",
      "HP_0025142 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "\n",
      "Processing disease: EFO_0000729\n",
      "EFO_0000729 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_0003756\n",
      "EFO_0003756 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0003756: too few nodes (2)\n",
      "\n",
      "Processing disease: HP_0040064\n",
      "HP_0040064 - Total distinct nodes N: 59; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 57, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000569\n",
      "MONDO_0000569 - Total distinct nodes N: 14; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 12, Using 10\n",
      "\n",
      "Processing disease: MONDO_0004126\n",
      "MONDO_0004126 - Total distinct nodes N: 11; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 9, Using 9\n",
      "\n",
      "Processing disease: EFO_0009477\n",
      "EFO_0009477 - Total distinct nodes N: 1; Chosen d: 2\n",
      "Skipping EFO_0009477: too few nodes (1)\n",
      "\n",
      "Processing disease: EFO_0005784\n",
      "EFO_0005784 - Total distinct nodes N: 9; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 7, Using 7\n",
      "\n",
      "Processing disease: MONDO_0004975\n",
      "MONDO_0004975 - Total distinct nodes N: 29; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 27, Using 10\n",
      "\n",
      "Processing disease: EFO_0009602\n",
      "EFO_0009602 - Total distinct nodes N: 1; Chosen d: 2\n",
      "Skipping EFO_0009602: too few nodes (1)\n",
      "\n",
      "Processing disease: EFO_0006340\n",
      "EFO_0006340 - Total distinct nodes N: 1; Chosen d: 2\n",
      "Skipping EFO_0006340: too few nodes (1)\n",
      "\n",
      "Processing disease: GO_0036273\n",
      "GO_0036273 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping GO_0036273: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0000691\n",
      "EFO_0000691 - Total distinct nodes N: 14; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 12, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021335\n",
      "MONDO_0021335 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: HP_0004936\n",
      "HP_0004936 - Total distinct nodes N: 20; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 18, Using 10\n",
      "\n",
      "Processing disease: EFO_0006500\n",
      "EFO_0006500 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: MONDO_0002280\n",
      "MONDO_0002280 - Total distinct nodes N: 131; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 129, Using 10\n",
      "\n",
      "Processing disease: EFO_0000365\n",
      "EFO_0000365 - Total distinct nodes N: 18; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 16, Using 10\n",
      "\n",
      "Processing disease: EFO_1000262\n",
      "EFO_1000262 - Total distinct nodes N: 61; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 59, Using 10\n",
      "\n",
      "Processing disease: EFO_0010118\n",
      "EFO_0010118 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_0004329\n",
      "EFO_0004329 - Total distinct nodes N: 1; Chosen d: 2\n",
      "Skipping EFO_0004329: too few nodes (1)\n",
      "\n",
      "Processing disease: EFO_0007005\n",
      "EFO_0007005 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_0005541\n",
      "EFO_0005541 - Total distinct nodes N: 11; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 9, Using 9\n",
      "\n",
      "Processing disease: EFO_0003931\n",
      "EFO_0003931 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0003931: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0020092\n",
      "EFO_0020092 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0020092: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0009607\n",
      "EFO_0009607 - Total distinct nodes N: 1; Chosen d: 2\n",
      "Skipping EFO_0009607: too few nodes (1)\n",
      "\n",
      "Processing disease: MONDO_0005301\n",
      "MONDO_0005301 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping MONDO_0005301: too few nodes (2)\n",
      "\n",
      "Processing disease: MONDO_0002525\n",
      "MONDO_0002525 - Total distinct nodes N: 50; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 48, Using 10\n",
      "\n",
      "Processing disease: EFO_0008595\n",
      "EFO_0008595 - Total distinct nodes N: 19; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 17, Using 10\n",
      "\n",
      "Processing disease: MONDO_0004580\n",
      "MONDO_0004580 - Total distinct nodes N: 17; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 15, Using 10\n",
      "\n",
      "Processing disease: EFO_0005653\n",
      "EFO_0005653 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_0005680\n",
      "EFO_0005680 - Total distinct nodes N: 21; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 19, Using 10\n",
      "\n",
      "Processing disease: EFO_0009608\n",
      "EFO_0009608 - Total distinct nodes N: 95; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 93, Using 10\n",
      "\n",
      "Processing disease: EFO_0004622\n",
      "EFO_0004622 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_1000870\n",
      "EFO_1000870 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_1000870: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0004145\n",
      "EFO_0004145 - Total distinct nodes N: 53; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 51, Using 10\n",
      "\n",
      "Processing disease: HP_0003124\n",
      "HP_0003124 - Total distinct nodes N: 30; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 28, Using 10\n",
      "\n",
      "Processing disease: EFO_0010226\n",
      "EFO_0010226 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: EFO_0020947\n",
      "EFO_0020947 - Total distinct nodes N: 58; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 56, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002562\n",
      "MONDO_0002562 - Total distinct nodes N: 13; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 11, Using 10\n",
      "\n",
      "Processing disease: HP_0100790\n",
      "HP_0100790 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: HP_0003119\n",
      "HP_0003119 - Total distinct nodes N: 23; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 21, Using 10\n",
      "\n",
      "Processing disease: EFO_1000020\n",
      "EFO_1000020 - Total distinct nodes N: 178; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 176, Using 10\n"
     ]
    }
   ],
   "source": [
    "pem_level0 = process_hierarchy_diseases(\"/Users/polina/Pathwaganda/data/GSEA-output_filt_hier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ede0a466",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "pem_level0.write.mode(\"overwrite\").parquet(\"/Users/polina/Pathwaganda/data/pem_levels/0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0152eab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering to hierarchy levels <= 1\n",
      "\n",
      "Processing disease: EFO_0000701\n",
      "EFO_0000701 - Total distinct nodes N: 25; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 23, Using 10\n",
      "\n",
      "Processing disease: EFO_0004872\n",
      "EFO_0004872 - Total distinct nodes N: 21; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 19, Using 10\n",
      "\n",
      "Processing disease: EFO_0005803\n",
      "EFO_0005803 - Total distinct nodes N: 56; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 54, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002715\n",
      "MONDO_0002715 - Total distinct nodes N: 66; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 64, Using 10\n",
      "\n",
      "Processing disease: GO_0008150\n",
      "GO_0008150 - Total distinct nodes N: 14; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 12, Using 10\n",
      "\n",
      "Processing disease: HP_0012638\n",
      "HP_0012638 - Total distinct nodes N: 15; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 13, Using 10\n",
      "\n",
      "Processing disease: MONDO_0023370\n",
      "MONDO_0023370 - Total distinct nodes N: 76; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 74, Using 10\n",
      "\n",
      "Processing disease: EFO_0004516\n",
      "EFO_0004516 - Total distinct nodes N: 22; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 20, Using 10\n",
      "\n",
      "Processing disease: EFO_0005127\n",
      "EFO_0005127 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_0803540\n",
      "EFO_0803540 - Total distinct nodes N: 13; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 11, Using 10\n",
      "\n",
      "Processing disease: EFO_0010285\n",
      "EFO_0010285 - Total distinct nodes N: 41; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 39, Using 10\n",
      "\n",
      "Processing disease: HP_0001626\n",
      "HP_0001626 - Total distinct nodes N: 32; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 30, Using 10\n",
      "\n",
      "Processing disease: MONDO_0007254\n",
      "MONDO_0007254 - Total distinct nodes N: 58; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 56, Using 10\n",
      "\n",
      "Processing disease: EFO_0004833\n",
      "EFO_0004833 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_0006843\n",
      "EFO_0006843 - Total distinct nodes N: 24; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 22, Using 10\n",
      "\n",
      "Processing disease: EFO_0010284\n",
      "EFO_0010284 - Total distinct nodes N: 11; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 9, Using 9\n",
      "Attempt 1 failed: Cannot sample 9 negative nodes from a set of 8 negative nodes for R-HSA-1430728\n",
      "Retrying with 7 negatives\n",
      "\n",
      "Processing disease: MONDO_0002039\n",
      "MONDO_0002039 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_0004574\n",
      "EFO_0004574 - Total distinct nodes N: 16; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 14, Using 10\n",
      "\n",
      "Processing disease: EFO_0002461\n",
      "EFO_0002461 - Total distinct nodes N: 60; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 58, Using 10\n",
      "\n",
      "Processing disease: EFO_1001986\n",
      "EFO_1001986 - Total distinct nodes N: 37; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 35, Using 10\n",
      "\n",
      "Processing disease: EFO_0004517\n",
      "EFO_0004517 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_0004747\n",
      "EFO_0004747 - Total distinct nodes N: 71; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 69, Using 10\n",
      "\n",
      "Processing disease: EFO_0004512\n",
      "EFO_0004512 - Total distinct nodes N: 17; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 15, Using 10\n",
      "\n",
      "Processing disease: EFO_0004269\n",
      "EFO_0004269 - Total distinct nodes N: 12; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 10, Using 10\n",
      "\n",
      "Processing disease: MONDO_0015757\n",
      "MONDO_0015757 - Total distinct nodes N: 34; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 32, Using 10\n",
      "\n",
      "Processing disease: GO_0032501\n",
      "GO_0032501 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_0005771\n",
      "EFO_0005771 - Total distinct nodes N: 19; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 17, Using 10\n",
      "\n",
      "Processing disease: EFO_0004586\n",
      "EFO_0004586 - Total distinct nodes N: 72; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 70, Using 10\n",
      "\n",
      "Processing disease: EFO_0009433\n",
      "EFO_0009433 - Total distinct nodes N: 18; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 16, Using 10\n",
      "\n",
      "Processing disease: EFO_0004527\n",
      "EFO_0004527 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0004527: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0009676\n",
      "EFO_0009676 - Total distinct nodes N: 65; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 63, Using 10\n",
      "\n",
      "Processing disease: EFO_0004260\n",
      "EFO_0004260 - Total distinct nodes N: 51; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 49, Using 10\n",
      "\n",
      "Processing disease: MONDO_0008315\n",
      "MONDO_0008315 - Total distinct nodes N: 28; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 26, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002025\n",
      "MONDO_0002025 - Total distinct nodes N: 17; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 15, Using 10\n",
      "\n",
      "Processing disease: EFO_0004530\n",
      "EFO_0004530 - Total distinct nodes N: 22; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 20, Using 10\n",
      "\n",
      "Processing disease: EFO_0004346\n",
      "EFO_0004346 - Total distinct nodes N: 12; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 10, Using 10\n",
      "Attempt 1 failed: Cannot sample 10 negative nodes from a set of 8 negative nodes for R-HSA-162582\n",
      "Retrying with 8 negatives\n",
      "\n",
      "Processing disease: MONDO_0003274\n",
      "MONDO_0003274 - Total distinct nodes N: 45; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 43, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021230\n",
      "MONDO_0021230 - Total distinct nodes N: 37; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 35, Using 10\n",
      "\n",
      "Processing disease: EFO_1001331\n",
      "EFO_1001331 - Total distinct nodes N: 65; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 63, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021066\n",
      "MONDO_0021066 - Total distinct nodes N: 66; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 64, Using 10\n",
      "\n",
      "Processing disease: EFO_0005741\n",
      "EFO_0005741 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0005741: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0003892\n",
      "EFO_0003892 - Total distinct nodes N: 20; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 18, Using 10\n",
      "\n",
      "Processing disease: EFO_0000508\n",
      "EFO_0000508 - Total distinct nodes N: 94; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 92, Using 10\n",
      "\n",
      "Processing disease: EFO_0004302\n",
      "EFO_0004302 - Total distinct nodes N: 61; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 59, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021148\n",
      "MONDO_0021148 - Total distinct nodes N: 79; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 77, Using 10\n",
      "\n",
      "Processing disease: EFO_0004611\n",
      "EFO_0004611 - Total distinct nodes N: 31; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 29, Using 10\n",
      "\n",
      "Processing disease: EFO_0004509\n",
      "EFO_0004509 - Total distinct nodes N: 26; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 24, Using 10\n",
      "\n",
      "Processing disease: EFO_0009386\n",
      "EFO_0009386 - Total distinct nodes N: 34; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 32, Using 10\n",
      "\n",
      "Processing disease: EFO_0001379\n",
      "EFO_0001379 - Total distinct nodes N: 43; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 41, Using 10\n",
      "\n",
      "Processing disease: EFO_0011008\n",
      "EFO_0011008 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: EFO_0000651\n",
      "EFO_0000651 - Total distinct nodes N: 66; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 64, Using 10\n",
      "\n",
      "Processing disease: EFO_0004348\n",
      "EFO_0004348 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: EFO_0004309\n",
      "EFO_0004309 - Total distinct nodes N: 32; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 30, Using 10\n",
      "\n",
      "Processing disease: EFO_0004614\n",
      "EFO_0004614 - Total distinct nodes N: 16; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 14, Using 10\n",
      "\n",
      "Processing disease: EFO_0005755\n",
      "EFO_0005755 - Total distinct nodes N: 15; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 13, Using 10\n",
      "\n",
      "Processing disease: EFO_0803547\n",
      "EFO_0803547 - Total distinct nodes N: 21; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 19, Using 10\n",
      "\n",
      "Processing disease: EFO_0000719\n",
      "EFO_0000719 - Total distinct nodes N: 12; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 10, Using 10\n",
      "\n",
      "Processing disease: EFO_0004995\n",
      "EFO_0004995 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0004995: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0004264\n",
      "EFO_0004264 - Total distinct nodes N: 63; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 61, Using 10\n",
      "\n",
      "Processing disease: EFO_0000574\n",
      "EFO_0000574 - Total distinct nodes N: 34; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 32, Using 10\n",
      "\n",
      "Processing disease: EFO_0006842\n",
      "EFO_0006842 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_0009431\n",
      "EFO_0009431 - Total distinct nodes N: 21; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 19, Using 10\n",
      "\n",
      "Processing disease: EFO_0005809\n",
      "EFO_0005809 - Total distinct nodes N: 17; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 15, Using 10\n",
      "\n",
      "Processing disease: EFO_0004303\n",
      "EFO_0004303 - Total distinct nodes N: 32; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 30, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021259\n",
      "MONDO_0021259 - Total distinct nodes N: 38; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 36, Using 10\n",
      "\n",
      "Processing disease: EFO_0004298\n",
      "EFO_0004298 - Total distinct nodes N: 58; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 56, Using 10\n",
      "\n",
      "Processing disease: MONDO_0001933\n",
      "MONDO_0001933 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: HP_0011842\n",
      "HP_0011842 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: EFO_0004198\n",
      "EFO_0004198 - Total distinct nodes N: 50; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 48, Using 10\n",
      "\n",
      "Processing disease: EFO_0005105\n",
      "EFO_0005105 - Total distinct nodes N: 55; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 53, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002259\n",
      "MONDO_0002259 - Total distinct nodes N: 23; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 21, Using 10\n",
      "\n",
      "Processing disease: MONDO_0003939\n",
      "MONDO_0003939 - Total distinct nodes N: 42; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 40, Using 10\n",
      "\n",
      "Processing disease: EFO_0007987\n",
      "EFO_0007987 - Total distinct nodes N: 14; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 12, Using 10\n",
      "\n",
      "Processing disease: EFO_0004587\n",
      "EFO_0004587 - Total distinct nodes N: 25; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 23, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002334\n",
      "MONDO_0002334 - Total distinct nodes N: 45; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 43, Using 10\n",
      "\n",
      "Processing disease: EFO_1000051\n",
      "EFO_1000051 - Total distinct nodes N: 76; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 74, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021350\n",
      "MONDO_0021350 - Total distinct nodes N: 49; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 47, Using 10\n",
      "\n",
      "Processing disease: EFO_0009682\n",
      "EFO_0009682 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "\n",
      "Processing disease: HP_0000924\n",
      "HP_0000924 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: EFO_0803548\n",
      "EFO_0803548 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: EFO_0004340\n",
      "EFO_0004340 - Total distinct nodes N: 14; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 12, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000473\n",
      "MONDO_0000473 - Total distinct nodes N: 31; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 29, Using 10\n",
      "\n",
      "Processing disease: EFO_1000999\n",
      "EFO_1000999 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: Orphanet_322126\n",
      "Orphanet_322126 - Total distinct nodes N: 30; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 28, Using 10\n",
      "\n",
      "Processing disease: EFO_0000684\n",
      "EFO_0000684 - Total distinct nodes N: 14; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 12, Using 10\n",
      "\n",
      "Processing disease: EFO_0001421\n",
      "EFO_0001421 - Total distinct nodes N: 11; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 9, Using 9\n",
      "\n",
      "Processing disease: EFO_0000589\n",
      "EFO_0000589 - Total distinct nodes N: 39; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 37, Using 10\n",
      "\n",
      "Processing disease: EFO_0004503\n",
      "EFO_0004503 - Total distinct nodes N: 82; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 80, Using 10\n",
      "\n",
      "Processing disease: MONDO_0044881\n",
      "MONDO_0044881 - Total distinct nodes N: 42; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 40, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002614\n",
      "MONDO_0002614 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: EFO_0004612\n",
      "EFO_0004612 - Total distinct nodes N: 39; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 37, Using 10\n",
      "\n",
      "Processing disease: EFO_0009690\n",
      "EFO_0009690 - Total distinct nodes N: 40; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 38, Using 10\n",
      "\n",
      "Processing disease: EFO_0007937\n",
      "EFO_0007937 - Total distinct nodes N: 26; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 24, Using 10\n",
      "\n",
      "Processing disease: EFO_0006841\n",
      "EFO_0006841 - Total distinct nodes N: 13; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 11, Using 10\n",
      "Attempt 1 failed: Cannot sample 10 negative nodes from a set of 8 negative nodes for R-HSA-162582\n",
      "Retrying with 8 negatives\n",
      "\n",
      "Processing disease: HP_0000118\n",
      "HP_0000118 - Total distinct nodes N: 90; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 88, Using 10\n",
      "\n",
      "Processing disease: EFO_0002919\n",
      "EFO_0002919 - Total distinct nodes N: 56; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 54, Using 10\n",
      "\n",
      "Processing disease: EFO_0005278\n",
      "EFO_0005278 - Total distinct nodes N: 58; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 56, Using 10\n",
      "\n",
      "Processing disease: EFO_0010282\n",
      "EFO_0010282 - Total distinct nodes N: 39; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 37, Using 10\n",
      "\n",
      "Processing disease: EFO_0006335\n",
      "EFO_0006335 - Total distinct nodes N: 21; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 19, Using 10\n",
      "\n",
      "Processing disease: MONDO_0045024\n",
      "MONDO_0045024 - Total distinct nodes N: 81; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 79, Using 10\n",
      "\n",
      "Processing disease: EFO_0000400\n",
      "EFO_0000400 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_0005774\n",
      "EFO_0005774 - Total distinct nodes N: 27; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 25, Using 10\n",
      "\n",
      "Processing disease: GO_0050896\n",
      "GO_0050896 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping GO_0050896: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0803546\n",
      "EFO_0803546 - Total distinct nodes N: 22; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 20, Using 10\n",
      "\n",
      "Processing disease: EFO_1002003\n",
      "EFO_1002003 - Total distinct nodes N: 30; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 28, Using 10\n",
      "\n",
      "Processing disease: MONDO_0024582\n",
      "MONDO_0024582 - Total distinct nodes N: 36; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 34, Using 10\n",
      "\n",
      "Processing disease: EFO_0007010\n",
      "EFO_0007010 - Total distinct nodes N: 17; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 15, Using 10\n",
      "\n",
      "Processing disease: EFO_0004468\n",
      "EFO_0004468 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0004468: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0001444\n",
      "EFO_0001444 - Total distinct nodes N: 58; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 56, Using 10\n",
      "\n",
      "Processing disease: HP_0000707\n",
      "HP_0000707 - Total distinct nodes N: 21; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 19, Using 10\n",
      "\n",
      "Processing disease: EFO_0007355\n",
      "EFO_0007355 - Total distinct nodes N: 34; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 32, Using 10\n",
      "\n",
      "Processing disease: EFO_0004338\n",
      "EFO_0004338 - Total distinct nodes N: 15; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 13, Using 10\n",
      "\n",
      "Processing disease: EFO_0003869\n",
      "EFO_0003869 - Total distinct nodes N: 59; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 57, Using 10\n",
      "\n",
      "Processing disease: EFO_1000363\n",
      "EFO_1000363 - Total distinct nodes N: 64; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 62, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021581\n",
      "MONDO_0021581 - Total distinct nodes N: 47; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 45, Using 10\n",
      "\n",
      "Processing disease: EFO_0004732\n",
      "EFO_0004732 - Total distinct nodes N: 32; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 30, Using 10\n",
      "\n",
      "Processing disease: EFO_0000618\n",
      "EFO_0000618 - Total distinct nodes N: 57; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 55, Using 10\n",
      "\n",
      "Processing disease: EFO_0004555\n",
      "EFO_0004555 - Total distinct nodes N: 12; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 10, Using 10\n",
      "Attempt 1 failed: Cannot sample 10 negative nodes from a set of 9 negative nodes for R-HSA-162582\n",
      "Retrying with 8 negatives\n",
      "\n",
      "Processing disease: EFO_0004980\n",
      "EFO_0004980 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: MONDO_0002974\n",
      "MONDO_0002974 - Total distinct nodes N: 29; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 27, Using 10\n",
      "\n",
      "Processing disease: EFO_0006858\n",
      "EFO_0006858 - Total distinct nodes N: 76; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 74, Using 10\n",
      "\n",
      "Processing disease: EFO_0011015\n",
      "EFO_0011015 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: MONDO_0002149\n",
      "MONDO_0002149 - Total distinct nodes N: 70; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 68, Using 10\n",
      "\n",
      "Processing disease: EFO_0003777\n",
      "EFO_0003777 - Total distinct nodes N: 59; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 57, Using 10\n",
      "\n",
      "Processing disease: GO_0007610\n",
      "GO_0007610 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_0001061\n",
      "EFO_0001061 - Total distinct nodes N: 37; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 35, Using 10\n",
      "\n",
      "Processing disease: EFO_0007441\n",
      "EFO_0007441 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_0000228\n",
      "EFO_0000228 - Total distinct nodes N: 35; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 33, Using 10\n",
      "\n",
      "Processing disease: EFO_0003893\n",
      "EFO_0003893 - Total distinct nodes N: 63; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 61, Using 10\n",
      "\n",
      "Processing disease: EFO_0004528\n",
      "EFO_0004528 - Total distinct nodes N: 17; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 15, Using 10\n",
      "\n",
      "Processing disease: EFO_0003966\n",
      "EFO_0003966 - Total distinct nodes N: 24; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 22, Using 10\n",
      "\n",
      "Processing disease: EFO_0005091\n",
      "EFO_0005091 - Total distinct nodes N: 22; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 20, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002654\n",
      "MONDO_0002654 - Total distinct nodes N: 16; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 14, Using 10\n",
      "\n",
      "Processing disease: Orphanet_68336\n",
      "Orphanet_68336 - Total distinct nodes N: 61; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 59, Using 10\n",
      "\n",
      "Processing disease: EFO_0000616\n",
      "EFO_0000616 - Total distinct nodes N: 76; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 74, Using 10\n",
      "\n",
      "Processing disease: EFO_0000305\n",
      "EFO_0000305 - Total distinct nodes N: 36; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 34, Using 10\n",
      "\n",
      "Processing disease: EFO_0009406\n",
      "EFO_0009406 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_0000537\n",
      "EFO_0000537 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0000537: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0004306\n",
      "EFO_0004306 - Total distinct nodes N: 29; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 27, Using 10\n",
      "\n",
      "Processing disease: EFO_0001663\n",
      "EFO_0001663 - Total distinct nodes N: 37; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 35, Using 10\n",
      "\n",
      "Processing disease: EFO_0004529\n",
      "EFO_0004529 - Total distinct nodes N: 61; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 59, Using 10\n",
      "\n",
      "Processing disease: EFO_0000677\n",
      "EFO_0000677 - Total distinct nodes N: 13; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 11, Using 10\n",
      "\n",
      "Processing disease: EFO_0000319\n",
      "EFO_0000319 - Total distinct nodes N: 74; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 72, Using 10\n",
      "\n",
      "Processing disease: EFO_0004842\n",
      "EFO_0004842 - Total distinct nodes N: 12; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 10, Using 10\n",
      "Attempt 1 failed: Cannot sample 10 negative nodes from a set of 9 negative nodes for R-HSA-168256\n",
      "Retrying with 8 negatives\n",
      "\n",
      "Processing disease: EFO_0009605\n",
      "EFO_0009605 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_0005772\n",
      "EFO_0005772 - Total distinct nodes N: 19; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 17, Using 10\n",
      "\n",
      "Processing disease: EFO_0000313\n",
      "EFO_0000313 - Total distinct nodes N: 72; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 70, Using 10\n",
      "\n",
      "Processing disease: EFO_0000540\n",
      "EFO_0000540 - Total distinct nodes N: 45; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 43, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000653\n",
      "MONDO_0000653 - Total distinct nodes N: 45; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 43, Using 10\n",
      "\n",
      "Processing disease: EFO_0005140\n",
      "EFO_0005140 - Total distinct nodes N: 26; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 24, Using 10\n",
      "\n",
      "Processing disease: EFO_0002422\n",
      "EFO_0002422 - Total distinct nodes N: 63; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 61, Using 10\n",
      "\n",
      "Processing disease: EFO_0008549\n",
      "EFO_0008549 - Total distinct nodes N: 55; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 53, Using 10\n",
      "\n",
      "Processing disease: EFO_0003086\n",
      "EFO_0003086 - Total distinct nodes N: 35; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 33, Using 10\n",
      "\n",
      "Processing disease: EFO_0004324\n",
      "EFO_0004324 - Total distinct nodes N: 33; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 31, Using 10\n",
      "\n",
      "Processing disease: EFO_0003859\n",
      "EFO_0003859 - Total distinct nodes N: 71; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 69, Using 10\n",
      "\n",
      "Processing disease: EFO_0006336\n",
      "EFO_0006336 - Total distinct nodes N: 9; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 7, Using 7\n",
      "\n",
      "Processing disease: EFO_0004627\n",
      "EFO_0004627 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0004627: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0009549\n",
      "EFO_0009549 - Total distinct nodes N: 33; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 31, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000426\n",
      "MONDO_0000426 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping MONDO_0000426: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0004725\n",
      "EFO_0004725 - Total distinct nodes N: 37; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 35, Using 10\n",
      "\n",
      "Processing disease: EFO_0004305\n",
      "EFO_0004305 - Total distinct nodes N: 22; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 20, Using 10\n",
      "\n",
      "Processing disease: EFO_0004531\n",
      "EFO_0004531 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_0004730\n",
      "EFO_0004730 - Total distinct nodes N: 16; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 14, Using 10\n",
      "\n",
      "Processing disease: EFO_0005856\n",
      "EFO_0005856 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_0007985\n",
      "EFO_0007985 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_0000512\n",
      "EFO_0000512 - Total distinct nodes N: 30; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 28, Using 10\n",
      "\n",
      "Processing disease: MONDO_0004992\n",
      "MONDO_0004992 - Total distinct nodes N: 82; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 80, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021193\n",
      "MONDO_0021193 - Total distinct nodes N: 25; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 23, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021248\n",
      "MONDO_0021248 - Total distinct nodes N: 42; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 40, Using 10\n",
      "\n",
      "Processing disease: MONDO_0017343\n",
      "MONDO_0017343 - Total distinct nodes N: 15; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 13, Using 10\n",
      "\n",
      "Processing disease: MONDO_0017595\n",
      "MONDO_0017595 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_1000627\n",
      "EFO_1000627 - Total distinct nodes N: 12; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 10, Using 10\n",
      "\n",
      "Processing disease: EFO_0000565\n",
      "EFO_0000565 - Total distinct nodes N: 22; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 20, Using 10\n",
      "\n",
      "Processing disease: EFO_1000068\n",
      "EFO_1000068 - Total distinct nodes N: 22; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 20, Using 10\n",
      "\n",
      "Processing disease: EFO_0004838\n",
      "EFO_0004838 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0004838: too few nodes (2)\n",
      "\n",
      "Processing disease: HP_0000478\n",
      "HP_0000478 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_0009555\n",
      "EFO_0009555 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: MONDO_0004095\n",
      "MONDO_0004095 - Total distinct nodes N: 28; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 26, Using 10\n",
      "\n",
      "Processing disease: EFO_0010642\n",
      "EFO_0010642 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: HP_0002011\n",
      "HP_0002011 - Total distinct nodes N: 16; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 14, Using 10\n",
      "\n",
      "Processing disease: EFO_0005689\n",
      "EFO_0005689 - Total distinct nodes N: 13; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 11, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021583\n",
      "MONDO_0021583 - Total distinct nodes N: 24; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 22, Using 10\n",
      "\n",
      "Processing disease: EFO_0007392\n",
      "EFO_0007392 - Total distinct nodes N: 26; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 24, Using 10\n",
      "\n",
      "Processing disease: MONDO_0024276\n",
      "MONDO_0024276 - Total distinct nodes N: 28; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 26, Using 10\n",
      "\n",
      "Processing disease: EFO_1000172\n",
      "EFO_1000172 - Total distinct nodes N: 36; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 34, Using 10\n",
      "\n",
      "Processing disease: EFO_0001075\n",
      "EFO_0001075 - Total distinct nodes N: 32; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 30, Using 10\n",
      "\n",
      "Processing disease: EFO_0007991\n",
      "EFO_0007991 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0007991: too few nodes (2)\n",
      "\n",
      "Processing disease: MONDO_0002406\n",
      "MONDO_0002406 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: MONDO_0008170\n",
      "MONDO_0008170 - Total distinct nodes N: 22; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 20, Using 10\n",
      "\n",
      "Processing disease: EFO_0004149\n",
      "EFO_0004149 - Total distinct nodes N: 13; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 11, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021143\n",
      "MONDO_0021143 - Total distinct nodes N: 30; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 28, Using 10\n",
      "\n",
      "Processing disease: EFO_0009259\n",
      "EFO_0009259 - Total distinct nodes N: 29; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 27, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021662\n",
      "MONDO_0021662 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: MONDO_0003060\n",
      "MONDO_0003060 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_0009119\n",
      "EFO_0009119 - Total distinct nodes N: 40; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 38, Using 10\n",
      "\n",
      "Processing disease: MONDO_0008903\n",
      "MONDO_0008903 - Total distinct nodes N: 15; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 13, Using 10\n",
      "\n",
      "Processing disease: EFO_0000096\n",
      "EFO_0000096 - Total distinct nodes N: 28; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 26, Using 10\n",
      "\n",
      "Processing disease: EFO_0000708\n",
      "EFO_0000708 - Total distinct nodes N: 12; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 10, Using 10\n",
      "Attempt 1 failed: Cannot sample 10 negative nodes from a set of 9 negative nodes for R-HSA-162582\n",
      "Retrying with 8 negatives\n",
      "\n",
      "Processing disease: EFO_0000466\n",
      "EFO_0000466 - Total distinct nodes N: 26; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 24, Using 10\n",
      "\n",
      "Processing disease: EFO_0004784\n",
      "EFO_0004784 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0004784: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0004458\n",
      "EFO_0004458 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: EFO_0002427\n",
      "EFO_0002427 - Total distinct nodes N: 17; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 15, Using 10\n",
      "\n",
      "Processing disease: EFO_0004142\n",
      "EFO_0004142 - Total distinct nodes N: 15; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 13, Using 10\n",
      "\n",
      "Processing disease: MONDO_0018364\n",
      "MONDO_0018364 - Total distinct nodes N: 23; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 21, Using 10\n",
      "\n",
      "Processing disease: EFO_1002018\n",
      "EFO_1002018 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_1002018: too few nodes (2)\n",
      "\n",
      "Processing disease: MONDO_0007263\n",
      "MONDO_0007263 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_0000220\n",
      "EFO_0000220 - Total distinct nodes N: 29; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 27, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002229\n",
      "MONDO_0002229 - Total distinct nodes N: 31; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 29, Using 10\n",
      "\n",
      "Processing disease: EFO_0002425\n",
      "EFO_0002425 - Total distinct nodes N: 29; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 27, Using 10\n",
      "\n",
      "Processing disease: MONDO_0004867\n",
      "MONDO_0004867 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping MONDO_0004867: too few nodes (2)\n",
      "\n",
      "Processing disease: MONDO_0002529\n",
      "MONDO_0002529 - Total distinct nodes N: 20; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 18, Using 10\n",
      "\n",
      "Processing disease: EFO_0005548\n",
      "EFO_0005548 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_0004741\n",
      "EFO_0004741 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: MONDO_0024477\n",
      "MONDO_0024477 - Total distinct nodes N: 27; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 25, Using 10\n",
      "\n",
      "Processing disease: EFO_0803539\n",
      "EFO_0803539 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_0007330\n",
      "EFO_0007330 - Total distinct nodes N: 19; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 17, Using 10\n",
      "\n",
      "Processing disease: EFO_0000681\n",
      "EFO_0000681 - Total distinct nodes N: 15; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 13, Using 10\n",
      "\n",
      "Processing disease: EFO_0004314\n",
      "EFO_0004314 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0004314: too few nodes (2)\n",
      "\n",
      "Processing disease: MONDO_0002367\n",
      "MONDO_0002367 - Total distinct nodes N: 22; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 20, Using 10\n",
      "\n",
      "Processing disease: EFO_0005952\n",
      "EFO_0005952 - Total distinct nodes N: 29; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 27, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021118\n",
      "MONDO_0021118 - Total distinct nodes N: 23; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 21, Using 10\n",
      "\n",
      "Processing disease: HP_0012443\n",
      "HP_0012443 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "\n",
      "Processing disease: MONDO_0005575\n",
      "MONDO_0005575 - Total distinct nodes N: 14; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 12, Using 10\n",
      "\n",
      "Processing disease: EFO_0003865\n",
      "EFO_0003865 - Total distinct nodes N: 28; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 26, Using 10\n",
      "\n",
      "Processing disease: EFO_0007993\n",
      "EFO_0007993 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: MONDO_0000429\n",
      "MONDO_0000429 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: MONDO_0002516\n",
      "MONDO_0002516 - Total distinct nodes N: 60; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 58, Using 10\n",
      "\n",
      "Processing disease: HP_0001574\n",
      "HP_0001574 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: EFO_0003853\n",
      "EFO_0003853 - Total distinct nodes N: 27; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 25, Using 10\n",
      "\n",
      "Processing disease: EFO_0000403\n",
      "EFO_0000403 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: EFO_0001645\n",
      "EFO_0001645 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_0003769\n",
      "EFO_0003769 - Total distinct nodes N: 39; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 37, Using 10\n",
      "\n",
      "Processing disease: EFO_0005708\n",
      "EFO_0005708 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "\n",
      "Processing disease: MONDO_0005148\n",
      "MONDO_0005148 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: HP_0001871\n",
      "HP_0001871 - Total distinct nodes N: 27; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 25, Using 10\n",
      "\n",
      "Processing disease: EFO_0010968\n",
      "EFO_0010968 - Total distinct nodes N: 15; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 13, Using 10\n",
      "\n",
      "Processing disease: EFO_0000756\n",
      "EFO_0000756 - Total distinct nodes N: 29; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 27, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021634\n",
      "MONDO_0021634 - Total distinct nodes N: 28; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 26, Using 10\n",
      "\n",
      "Processing disease: EFO_0004731\n",
      "EFO_0004731 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_0007984\n",
      "EFO_0007984 - Total distinct nodes N: 11; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 9, Using 9\n",
      "\n",
      "Processing disease: EFO_0000326\n",
      "EFO_0000326 - Total distinct nodes N: 27; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 25, Using 10\n",
      "\n",
      "Processing disease: EFO_0005950\n",
      "EFO_0005950 - Total distinct nodes N: 41; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 39, Using 10\n",
      "\n",
      "Processing disease: EFO_0002892\n",
      "EFO_0002892 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "Attempt 1 failed: Cannot sample 8 negative nodes from a set of 7 negative nodes for R-HSA-162582\n",
      "Retrying with 6 negatives\n",
      "\n",
      "Processing disease: EFO_0001642\n",
      "EFO_0001642 - Total distinct nodes N: 36; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 34, Using 10\n",
      "\n",
      "Processing disease: EFO_0003891\n",
      "EFO_0003891 - Total distinct nodes N: 15; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 13, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002532\n",
      "MONDO_0002532 - Total distinct nodes N: 27; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 25, Using 10\n",
      "\n",
      "Processing disease: EFO_0000318\n",
      "EFO_0000318 - Total distinct nodes N: 27; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 25, Using 10\n",
      "\n",
      "Processing disease: MONDO_0004979\n",
      "MONDO_0004979 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping MONDO_0004979: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0011011\n",
      "EFO_0011011 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "Attempt 1 failed: Cannot sample 8 negative nodes from a set of 6 negative nodes for R-HSA-162582\n",
      "Retrying with 6 negatives\n",
      "\n",
      "Processing disease: MONDO_0017341\n",
      "MONDO_0017341 - Total distinct nodes N: 23; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 21, Using 10\n",
      "\n",
      "Processing disease: EFO_0008550\n",
      "EFO_0008550 - Total distinct nodes N: 19; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 17, Using 10\n",
      "\n",
      "Processing disease: EFO_0000389\n",
      "EFO_0000389 - Total distinct nodes N: 21; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 19, Using 10\n",
      "\n",
      "Processing disease: EFO_1000218\n",
      "EFO_1000218 - Total distinct nodes N: 53; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 51, Using 10\n",
      "\n",
      "Processing disease: EFO_0005763\n",
      "EFO_0005763 - Total distinct nodes N: 12; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 10, Using 10\n",
      "\n",
      "Processing disease: HP_0011025\n",
      "HP_0011025 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping HP_0011025: too few nodes (2)\n",
      "\n",
      "Processing disease: MONDO_0017342\n",
      "MONDO_0017342 - Total distinct nodes N: 20; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 18, Using 10\n",
      "\n",
      "Processing disease: EFO_0005670\n",
      "EFO_0005670 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: MONDO_0003409\n",
      "MONDO_0003409 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "\n",
      "Processing disease: EFO_0004312\n",
      "EFO_0004312 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_1000158\n",
      "EFO_1000158 - Total distinct nodes N: 35; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 33, Using 10\n",
      "\n",
      "Processing disease: EFO_0004532\n",
      "EFO_0004532 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0004532: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0000707\n",
      "EFO_0000707 - Total distinct nodes N: 19; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 17, Using 10\n",
      "\n",
      "Processing disease: MONDO_0001627\n",
      "MONDO_0001627 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: MONDO_0002898\n",
      "MONDO_0002898 - Total distinct nodes N: 38; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 36, Using 10\n",
      "\n",
      "Processing disease: EFO_0000304\n",
      "EFO_0000304 - Total distinct nodes N: 13; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 11, Using 10\n",
      "Attempt 1 failed: Cannot sample 10 negative nodes from a set of 9 negative nodes for R-HSA-162582\n",
      "Retrying with 8 negatives\n",
      "\n",
      "Processing disease: EFO_0003767\n",
      "EFO_0003767 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "Attempt 1 failed: Cannot sample 6 negative nodes from a set of 5 negative nodes for R-HSA-168256\n",
      "Retrying with 4 negatives\n",
      "\n",
      "Processing disease: MONDO_0021117\n",
      "MONDO_0021117 - Total distinct nodes N: 22; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 20, Using 10\n",
      "\n",
      "Processing disease: EFO_1001938\n",
      "EFO_1001938 - Total distinct nodes N: 21; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 19, Using 10\n",
      "\n",
      "Processing disease: EFO_0002890\n",
      "EFO_0002890 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: EFO_0004289\n",
      "EFO_0004289 - Total distinct nodes N: 25; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 23, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002108\n",
      "MONDO_0002108 - Total distinct nodes N: 21; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 19, Using 10\n",
      "\n",
      "Processing disease: EFO_0002917\n",
      "EFO_0002917 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0002917: too few nodes (2)\n",
      "\n",
      "Processing disease: HP_0000951\n",
      "HP_0000951 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping HP_0000951: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0002970\n",
      "EFO_0002970 - Total distinct nodes N: 16; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 14, Using 10\n",
      "\n",
      "Processing disease: EFO_0000348\n",
      "EFO_0000348 - Total distinct nodes N: 34; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 32, Using 10\n",
      "\n",
      "Processing disease: MONDO_0020665\n",
      "MONDO_0020665 - Total distinct nodes N: 19; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 17, Using 10\n",
      "\n",
      "Processing disease: MONDO_0005341\n",
      "MONDO_0005341 - Total distinct nodes N: 20; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 18, Using 10\n",
      "\n",
      "Processing disease: MONDO_0004805\n",
      "MONDO_0004805 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "Attempt 1 failed: Cannot sample 4 negative nodes from a set of 3 negative nodes for R-HSA-168256\n",
      "Retrying with 2 negatives\n",
      "\n",
      "Processing disease: MONDO_0000376\n",
      "MONDO_0000376 - Total distinct nodes N: 20; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 18, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021251\n",
      "MONDO_0021251 - Total distinct nodes N: 28; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 26, Using 10\n",
      "\n",
      "Processing disease: EFO_0005543\n",
      "EFO_0005543 - Total distinct nodes N: 20; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 18, Using 10\n",
      "\n",
      "Processing disease: EFO_0000272\n",
      "EFO_0000272 - Total distinct nodes N: 15; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 13, Using 10\n",
      "Attempt 1 failed: Cannot sample 10 negative nodes from a set of 9 negative nodes for R-HSA-162582\n",
      "Retrying with 8 negatives\n",
      "\n",
      "Processing disease: EFO_1000417\n",
      "EFO_1000417 - Total distinct nodes N: 14; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 12, Using 10\n",
      "\n",
      "Processing disease: EFO_0008528\n",
      "EFO_0008528 - Total distinct nodes N: 43; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 41, Using 10\n",
      "\n",
      "Processing disease: EFO_0004193\n",
      "EFO_0004193 - Total distinct nodes N: 13; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 11, Using 10\n",
      "\n",
      "Processing disease: MONDO_0011962\n",
      "MONDO_0011962 - Total distinct nodes N: 28; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 26, Using 10\n",
      "\n",
      "Processing disease: MONDO_0044937\n",
      "MONDO_0044937 - Total distinct nodes N: 39; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 37, Using 10\n",
      "\n",
      "Processing disease: MONDO_0004986\n",
      "MONDO_0004986 - Total distinct nodes N: 52; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 50, Using 10\n",
      "\n",
      "Processing disease: EFO_0000182\n",
      "EFO_0000182 - Total distinct nodes N: 24; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 22, Using 10\n",
      "\n",
      "Processing disease: MONDO_0003812\n",
      "MONDO_0003812 - Total distinct nodes N: 16; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 14, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002480\n",
      "MONDO_0002480 - Total distinct nodes N: 12; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 10, Using 10\n",
      "\n",
      "Processing disease: EFO_0000181\n",
      "EFO_0000181 - Total distinct nodes N: 27; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 25, Using 10\n",
      "\n",
      "Processing disease: HP_0002597\n",
      "HP_0002597 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping HP_0002597: too few nodes (2)\n",
      "\n",
      "Processing disease: MONDO_0002120\n",
      "MONDO_0002120 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: MONDO_0000621\n",
      "MONDO_0000621 - Total distinct nodes N: 31; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 29, Using 10\n",
      "\n",
      "Processing disease: EFO_1000646\n",
      "EFO_1000646 - Total distinct nodes N: 18; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 16, Using 10\n",
      "\n",
      "Processing disease: EFO_1001950\n",
      "EFO_1001950 - Total distinct nodes N: 9; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 7, Using 7\n",
      "Attempt 1 failed: Cannot sample 7 negative nodes from a set of 5 negative nodes for R-HSA-162582\n",
      "Retrying with 5 negatives\n",
      "\n",
      "Processing disease: EFO_0005232\n",
      "EFO_0005232 - Total distinct nodes N: 28; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 26, Using 10\n",
      "\n",
      "Processing disease: EFO_0002618\n",
      "EFO_0002618 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0002618: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0005134\n",
      "EFO_0005134 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: EFO_0009260\n",
      "EFO_0009260 - Total distinct nodes N: 17; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 15, Using 10\n",
      "\n",
      "Processing disease: MONDO_0018531\n",
      "MONDO_0018531 - Total distinct nodes N: 11; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 9, Using 9\n",
      "Attempt 1 failed: Cannot sample 9 negative nodes from a set of 7 negative nodes for R-HSA-162582\n",
      "Retrying with 7 negatives\n",
      "\n",
      "Processing disease: HP_0100543\n",
      "HP_0100543 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping HP_0100543: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_1001463\n",
      "EFO_1001463 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_1001463: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_1000021\n",
      "EFO_1000021 - Total distinct nodes N: 30; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 28, Using 10\n",
      "\n",
      "Processing disease: EFO_1000416\n",
      "EFO_1000416 - Total distinct nodes N: 18; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 16, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002033\n",
      "MONDO_0002033 - Total distinct nodes N: 29; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 27, Using 10\n",
      "\n",
      "Processing disease: EFO_0006859\n",
      "EFO_0006859 - Total distinct nodes N: 36; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 34, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021096\n",
      "MONDO_0021096 - Total distinct nodes N: 19; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 17, Using 10\n",
      "\n",
      "Processing disease: EFO_0000178\n",
      "EFO_0000178 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "Attempt 1 failed: Cannot sample 6 negative nodes from a set of 5 negative nodes for R-HSA-162582\n",
      "Retrying with 4 negatives\n",
      "\n",
      "Processing disease: EFO_0003060\n",
      "EFO_0003060 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "Attempt 1 failed: Cannot sample 6 negative nodes from a set of 5 negative nodes for R-HSA-162582\n",
      "Retrying with 4 negatives\n",
      "\n",
      "Processing disease: EFO_0007861\n",
      "EFO_0007861 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0007861: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_1001763\n",
      "EFO_1001763 - Total distinct nodes N: 13; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 11, Using 10\n",
      "\n",
      "Processing disease: HP_0011446\n",
      "HP_0011446 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_0005570\n",
      "EFO_0005570 - Total distinct nodes N: 37; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 35, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002038\n",
      "MONDO_0002038 - Total distinct nodes N: 33; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 31, Using 10\n",
      "\n",
      "Processing disease: MONDO_0037254\n",
      "MONDO_0037254 - Total distinct nodes N: 46; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 44, Using 10\n",
      "\n",
      "Processing disease: EFO_0000232\n",
      "EFO_0000232 - Total distinct nodes N: 20; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 18, Using 10\n",
      "\n",
      "Processing disease: MONDO_0015760\n",
      "MONDO_0015760 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_0009255\n",
      "EFO_0009255 - Total distinct nodes N: 27; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 25, Using 10\n",
      "\n",
      "Processing disease: EFO_0000275\n",
      "EFO_0000275 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: MONDO_0004251\n",
      "MONDO_0004251 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: MONDO_0021636\n",
      "MONDO_0021636 - Total distinct nodes N: 14; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 12, Using 10\n",
      "Attempt 1 failed: Cannot sample 10 negative nodes from a set of 9 negative nodes for R-HSA-162582\n",
      "Retrying with 8 negatives\n",
      "\n",
      "Processing disease: EFO_1000143\n",
      "EFO_1000143 - Total distinct nodes N: 39; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 37, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000629\n",
      "MONDO_0000629 - Total distinct nodes N: 12; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 10, Using 10\n",
      "Attempt 1 failed: Cannot sample 10 negative nodes from a set of 7 negative nodes for R-HSA-162582\n",
      "Retrying with 8 negatives\n",
      "Failed after 2 attempts for MONDO_0000629: Cannot sample 8 negative nodes from a set of 7 negative nodes for R-HSA-162582\n",
      "\n",
      "Processing disease: EFO_0008591\n",
      "EFO_0008591 - Total distinct nodes N: 19; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 17, Using 10\n",
      "\n",
      "Processing disease: EFO_1000350\n",
      "EFO_1000350 - Total distinct nodes N: 24; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 22, Using 10\n",
      "\n",
      "Processing disease: EFO_0009387\n",
      "EFO_0009387 - Total distinct nodes N: 11; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 9, Using 9\n",
      "\n",
      "Processing disease: MONDO_0021063\n",
      "MONDO_0021063 - Total distinct nodes N: 12; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 10, Using 10\n",
      "Attempt 1 failed: Cannot sample 10 negative nodes from a set of 7 negative nodes for R-HSA-162582\n",
      "Retrying with 8 negatives\n",
      "Failed after 2 attempts for MONDO_0021063: Cannot sample 8 negative nodes from a set of 7 negative nodes for R-HSA-162582\n",
      "\n",
      "Processing disease: EFO_0004639\n",
      "EFO_0004639 - Total distinct nodes N: 13; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 11, Using 10\n",
      "\n",
      "Processing disease: EFO_1001902\n",
      "EFO_1001902 - Total distinct nodes N: 15; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 13, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000637\n",
      "MONDO_0000637 - Total distinct nodes N: 25; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 23, Using 10\n",
      "\n",
      "Processing disease: EFO_0006318\n",
      "EFO_0006318 - Total distinct nodes N: 23; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 21, Using 10\n",
      "\n",
      "Processing disease: HP_0012759\n",
      "HP_0012759 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: MONDO_0024479\n",
      "MONDO_0024479 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "Attempt 1 failed: Cannot sample 6 negative nodes from a set of 5 negative nodes for R-HSA-162582\n",
      "Retrying with 4 negatives\n",
      "\n",
      "Processing disease: EFO_0006545\n",
      "EFO_0006545 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "Attempt 1 failed: Cannot sample 8 negative nodes from a set of 5 negative nodes for R-HSA-162582\n",
      "Retrying with 6 negatives\n",
      "Failed after 2 attempts for EFO_0006545: Cannot sample 6 negative nodes from a set of 5 negative nodes for R-HSA-162582\n",
      "\n",
      "Processing disease: MONDO_0021069\n",
      "MONDO_0021069 - Total distinct nodes N: 19; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 17, Using 10\n",
      "\n",
      "Processing disease: EFO_0005540\n",
      "EFO_0005540 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: MONDO_0001056\n",
      "MONDO_0001056 - Total distinct nodes N: 15; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 13, Using 10\n",
      "\n",
      "Processing disease: EFO_0001071\n",
      "EFO_0001071 - Total distinct nodes N: 16; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 14, Using 10\n",
      "\n",
      "Processing disease: EFO_0004570\n",
      "EFO_0004570 - Total distinct nodes N: 11; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 9, Using 9\n",
      "\n",
      "Processing disease: MONDO_0015756\n",
      "MONDO_0015756 - Total distinct nodes N: 11; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 9, Using 9\n",
      "\n",
      "Processing disease: EFO_0000246\n",
      "EFO_0000246 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_1000601\n",
      "EFO_1000601 - Total distinct nodes N: 45; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 43, Using 10\n",
      "\n",
      "Processing disease: EFO_0004343\n",
      "EFO_0004343 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0004343: too few nodes (2)\n",
      "\n",
      "Processing disease: MONDO_0024476\n",
      "MONDO_0024476 - Total distinct nodes N: 33; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 31, Using 10\n",
      "\n",
      "Processing disease: EFO_1001949\n",
      "EFO_1001949 - Total distinct nodes N: 25; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 23, Using 10\n",
      "\n",
      "Processing disease: EFO_0010351\n",
      "EFO_0010351 - Total distinct nodes N: 15; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 13, Using 10\n",
      "\n",
      "Processing disease: EFO_0003897\n",
      "EFO_0003897 - Total distinct nodes N: 14; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 12, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021076\n",
      "MONDO_0021076 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_0005220\n",
      "EFO_0005220 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0005220: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0002571\n",
      "EFO_0002571 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0002571: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0004288\n",
      "EFO_0004288 - Total distinct nodes N: 15; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 13, Using 10\n",
      "Attempt 1 failed: Cannot sample 10 negative nodes from a set of 9 negative nodes for R-HSA-162582\n",
      "Retrying with 8 negatives\n",
      "\n",
      "Processing disease: MONDO_0002691\n",
      "MONDO_0002691 - Total distinct nodes N: 14; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 12, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021355\n",
      "MONDO_0021355 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: EFO_1001901\n",
      "EFO_1001901 - Total distinct nodes N: 17; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 15, Using 10\n",
      "\n",
      "Processing disease: MONDO_0007576\n",
      "MONDO_0007576 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: EFO_0005592\n",
      "EFO_0005592 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_0000294\n",
      "EFO_0000294 - Total distinct nodes N: 49; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 47, Using 10\n",
      "\n",
      "Processing disease: EFO_0005631\n",
      "EFO_0005631 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0005631: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0003820\n",
      "EFO_0003820 - Total distinct nodes N: 16; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 14, Using 10\n",
      "\n",
      "Processing disease: EFO_0007990\n",
      "EFO_0007990 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_0009483\n",
      "EFO_0009483 - Total distinct nodes N: 26; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 24, Using 10\n",
      "\n",
      "Processing disease: EFO_0010176\n",
      "EFO_0010176 - Total distinct nodes N: 17; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 15, Using 10\n",
      "\n",
      "Processing disease: EFO_0008524\n",
      "EFO_0008524 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: MONDO_0037255\n",
      "MONDO_0037255 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "Attempt 1 failed: Cannot sample 6 negative nodes from a set of 5 negative nodes for R-HSA-162582\n",
      "Retrying with 4 negatives\n",
      "\n",
      "Processing disease: EFO_0007989\n",
      "EFO_0007989 - Total distinct nodes N: 12; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 10, Using 10\n",
      "\n",
      "Processing disease: EFO_0002428\n",
      "EFO_0002428 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: MONDO_0000956\n",
      "MONDO_0000956 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "Attempt 1 failed: Cannot sample 8 negative nodes from a set of 7 negative nodes for R-HSA-162582\n",
      "Retrying with 6 negatives\n",
      "\n",
      "Processing disease: EFO_1001512\n",
      "EFO_1001512 - Total distinct nodes N: 24; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 22, Using 10\n",
      "\n",
      "Processing disease: MONDO_0017594\n",
      "MONDO_0017594 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_0009534\n",
      "EFO_0009534 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: MONDO_0005374\n",
      "MONDO_0005374 - Total distinct nodes N: 9; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 7, Using 7\n",
      "\n",
      "Processing disease: EFO_1000613\n",
      "EFO_1000613 - Total distinct nodes N: 26; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 24, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002616\n",
      "MONDO_0002616 - Total distinct nodes N: 16; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 14, Using 10\n",
      "\n",
      "Processing disease: EFO_0000222\n",
      "EFO_0000222 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: MONDO_0003059\n",
      "MONDO_0003059 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_0006460\n",
      "EFO_0006460 - Total distinct nodes N: 22; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 20, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021138\n",
      "MONDO_0021138 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_1000657\n",
      "EFO_1000657 - Total distinct nodes N: 31; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 29, Using 10\n",
      "\n",
      "Processing disease: MONDO_0044334\n",
      "MONDO_0044334 - Total distinct nodes N: 12; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 10, Using 10\n",
      "Attempt 1 failed: Cannot sample 10 negative nodes from a set of 9 negative nodes for R-HSA-162582\n",
      "Retrying with 8 negatives\n",
      "\n",
      "Processing disease: MONDO_0001014\n",
      "MONDO_0001014 - Total distinct nodes N: 9; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 7, Using 7\n",
      "\n",
      "Processing disease: EFO_0020946\n",
      "EFO_0020946 - Total distinct nodes N: 16; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 14, Using 10\n",
      "\n",
      "Processing disease: MONDO_0016680\n",
      "MONDO_0016680 - Total distinct nodes N: 13; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 11, Using 10\n",
      "Attempt 1 failed: Cannot sample 10 negative nodes from a set of 9 negative nodes for R-HSA-162582\n",
      "Retrying with 8 negatives\n",
      "\n",
      "Processing disease: MONDO_0021043\n",
      "MONDO_0021043 - Total distinct nodes N: 19; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 17, Using 10\n",
      "\n",
      "Processing disease: EFO_0005922\n",
      "EFO_0005922 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "\n",
      "Processing disease: EFO_0004327\n",
      "EFO_0004327 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: MONDO_0002165\n",
      "MONDO_0002165 - Total distinct nodes N: 25; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 23, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000589\n",
      "MONDO_0000589 - Total distinct nodes N: 11; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 9, Using 9\n",
      "Attempt 1 failed: Cannot sample 9 negative nodes from a set of 8 negative nodes for R-HSA-168256\n",
      "Retrying with 7 negatives\n",
      "\n",
      "Processing disease: EFO_1000233\n",
      "EFO_1000233 - Total distinct nodes N: 32; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 30, Using 10\n",
      "\n",
      "Processing disease: EFO_0000519\n",
      "EFO_0000519 - Total distinct nodes N: 11; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 9, Using 9\n",
      "Attempt 1 failed: Cannot sample 9 negative nodes from a set of 7 negative nodes for R-HSA-162582\n",
      "Retrying with 7 negatives\n",
      "\n",
      "Processing disease: MONDO_0001187\n",
      "MONDO_0001187 - Total distinct nodes N: 48; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 46, Using 10\n",
      "\n",
      "Processing disease: MONDO_0100342\n",
      "MONDO_0100342 - Total distinct nodes N: 18; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 16, Using 10\n",
      "\n",
      "Processing disease: EFO_0003825\n",
      "EFO_0003825 - Total distinct nodes N: 29; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 27, Using 10\n",
      "\n",
      "Processing disease: EFO_0003868\n",
      "EFO_0003868 - Total distinct nodes N: 46; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 44, Using 10\n",
      "\n",
      "Processing disease: MONDO_0037256\n",
      "MONDO_0037256 - Total distinct nodes N: 16; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 14, Using 10\n",
      "\n",
      "Processing disease: EFO_0009804\n",
      "EFO_0009804 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: MONDO_0002512\n",
      "MONDO_0002512 - Total distinct nodes N: 13; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 11, Using 10\n",
      "Attempt 1 failed: Cannot sample 10 negative nodes from a set of 9 negative nodes for R-HSA-162582\n",
      "Retrying with 8 negatives\n",
      "\n",
      "Processing disease: EFO_0000571\n",
      "EFO_0000571 - Total distinct nodes N: 11; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 9, Using 9\n",
      "Attempt 1 failed: Cannot sample 9 negative nodes from a set of 7 negative nodes for R-HSA-162582\n",
      "Retrying with 7 negatives\n",
      "\n",
      "Processing disease: EFO_0005588\n",
      "EFO_0005588 - Total distinct nodes N: 9; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 7, Using 7\n",
      "\n",
      "Processing disease: EFO_0000199\n",
      "EFO_0000199 - Total distinct nodes N: 31; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 29, Using 10\n",
      "\n",
      "Processing disease: EFO_1001951\n",
      "EFO_1001951 - Total distinct nodes N: 9; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 7, Using 7\n",
      "\n",
      "Processing disease: EFO_1000356\n",
      "EFO_1000356 - Total distinct nodes N: 22; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 20, Using 10\n",
      "\n",
      "Processing disease: MONDO_0005499\n",
      "MONDO_0005499 - Total distinct nodes N: 12; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 10, Using 10\n",
      "Attempt 1 failed: Cannot sample 10 negative nodes from a set of 9 negative nodes for R-HSA-162582\n",
      "Retrying with 8 negatives\n",
      "\n",
      "Processing disease: MONDO_0000812\n",
      "MONDO_0000812 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: Orphanet_271847\n",
      "Orphanet_271847 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping Orphanet_271847: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_1000017\n",
      "EFO_1000017 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: EFO_0000209\n",
      "EFO_0000209 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0000209: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0020943\n",
      "EFO_0020943 - Total distinct nodes N: 13; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 11, Using 10\n",
      "\n",
      "Processing disease: MONDO_0020638\n",
      "MONDO_0020638 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: EFO_0003860\n",
      "EFO_0003860 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_0000195\n",
      "EFO_0000195 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0000195: too few nodes (2)\n",
      "\n",
      "Processing disease: MONDO_0003193\n",
      "MONDO_0003193 - Total distinct nodes N: 11; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 9, Using 9\n",
      "\n",
      "Processing disease: EFO_0001073\n",
      "EFO_0001073 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0001073: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0000640\n",
      "EFO_0000640 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0000640: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0006544\n",
      "EFO_0006544 - Total distinct nodes N: 32; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 30, Using 10\n",
      "\n",
      "Processing disease: EFO_0000546\n",
      "EFO_0000546 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0000546: too few nodes (2)\n",
      "\n",
      "Processing disease: MONDO_0000636\n",
      "MONDO_0000636 - Total distinct nodes N: 13; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 11, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021058\n",
      "MONDO_0021058 - Total distinct nodes N: 23; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 21, Using 10\n",
      "\n",
      "Processing disease: EFO_1000541\n",
      "EFO_1000541 - Total distinct nodes N: 12; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 10, Using 10\n",
      "Attempt 1 failed: Cannot sample 10 negative nodes from a set of 9 negative nodes for R-HSA-162582\n",
      "Retrying with 8 negatives\n",
      "\n",
      "Processing disease: MONDO_0007915\n",
      "MONDO_0007915 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: MONDO_0024757\n",
      "MONDO_0024757 - Total distinct nodes N: 15; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 13, Using 10\n",
      "\n",
      "Processing disease: HP_0002060\n",
      "HP_0002060 - Total distinct nodes N: 23; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 21, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002116\n",
      "MONDO_0002116 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping MONDO_0002116: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0007352\n",
      "EFO_0007352 - Total distinct nodes N: 37; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 35, Using 10\n",
      "\n",
      "Processing disease: HP_0030680\n",
      "HP_0030680 - Total distinct nodes N: 17; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 15, Using 10\n",
      "\n",
      "Processing disease: EFO_0004615\n",
      "EFO_0004615 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0004615: too few nodes (2)\n",
      "\n",
      "Processing disease: MONDO_0000648\n",
      "MONDO_0000648 - Total distinct nodes N: 24; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 22, Using 10\n",
      "\n",
      "Processing disease: MONDO_0024337\n",
      "MONDO_0024337 - Total distinct nodes N: 48; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 46, Using 10\n",
      "\n",
      "Processing disease: MONDO_0004907\n",
      "MONDO_0004907 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping MONDO_0004907: too few nodes (2)\n",
      "\n",
      "Processing disease: MONDO_0021631\n",
      "MONDO_0021631 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "Attempt 1 failed: Cannot sample 8 negative nodes from a set of 7 negative nodes for R-HSA-162582\n",
      "Retrying with 6 negatives\n",
      "\n",
      "Processing disease: EFO_0000200\n",
      "EFO_0000200 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0000200: too few nodes (2)\n",
      "\n",
      "Processing disease: MONDO_0000386\n",
      "MONDO_0000386 - Total distinct nodes N: 18; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 16, Using 10\n",
      "\n",
      "Processing disease: EFO_0000705\n",
      "EFO_0000705 - Total distinct nodes N: 16; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 14, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002928\n",
      "MONDO_0002928 - Total distinct nodes N: 28; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 26, Using 10\n",
      "\n",
      "Processing disease: EFO_0008515\n",
      "EFO_0008515 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "Attempt 1 failed: Cannot sample 5 negative nodes from a set of 3 negative nodes for R-HSA-162582\n",
      "Retrying with 3 negatives\n",
      "\n",
      "Processing disease: HP_0001939\n",
      "HP_0001939 - Total distinct nodes N: 21; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 19, Using 10\n",
      "\n",
      "Processing disease: HP_0000271\n",
      "HP_0000271 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_1000532\n",
      "EFO_1000532 - Total distinct nodes N: 17; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 15, Using 10\n",
      "\n",
      "Processing disease: EFO_1001956\n",
      "EFO_1001956 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: EFO_0021796\n",
      "EFO_0021796 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0021796: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0004705\n",
      "EFO_0004705 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0004705: too few nodes (2)\n",
      "\n",
      "Processing disease: HP_0012647\n",
      "HP_0012647 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping HP_0012647: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0020944\n",
      "EFO_0020944 - Total distinct nodes N: 17; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 15, Using 10\n",
      "\n",
      "Processing disease: EFO_0000478\n",
      "EFO_0000478 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0000478: too few nodes (2)\n",
      "\n",
      "Processing disease: MONDO_0044710\n",
      "MONDO_0044710 - Total distinct nodes N: 34; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 32, Using 10\n",
      "\n",
      "Processing disease: EFO_0003833\n",
      "EFO_0003833 - Total distinct nodes N: 19; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 17, Using 10\n",
      "\n",
      "Processing disease: MONDO_0024503\n",
      "MONDO_0024503 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping MONDO_0024503: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0000514\n",
      "EFO_0000514 - Total distinct nodes N: 12; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 10, Using 10\n",
      "\n",
      "Processing disease: MONDO_0024637\n",
      "MONDO_0024637 - Total distinct nodes N: 14; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 12, Using 10\n",
      "\n",
      "Processing disease: HP_0012649\n",
      "HP_0012649 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping HP_0012649: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0005423\n",
      "EFO_0005423 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0005423: too few nodes (2)\n",
      "\n",
      "Processing disease: MONDO_0021254\n",
      "MONDO_0021254 - Total distinct nodes N: 13; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 11, Using 10\n",
      "Attempt 1 failed: Cannot sample 10 negative nodes from a set of 9 negative nodes for R-HSA-162582\n",
      "Retrying with 8 negatives\n",
      "\n",
      "Processing disease: EFO_0003839\n",
      "EFO_0003839 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: MONDO_0000383\n",
      "MONDO_0000383 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "Attempt 1 failed: Cannot sample 8 negative nodes from a set of 7 negative nodes for R-HSA-162582\n",
      "Retrying with 6 negatives\n",
      "\n",
      "Processing disease: MONDO_0005271\n",
      "MONDO_0005271 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_0008589\n",
      "EFO_0008589 - Total distinct nodes N: 16; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 14, Using 10\n",
      "\n",
      "Processing disease: EFO_1000255\n",
      "EFO_1000255 - Total distinct nodes N: 9; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 7, Using 7\n",
      "Attempt 1 failed: Cannot sample 7 negative nodes from a set of 5 negative nodes for R-HSA-162582\n",
      "Retrying with 5 negatives\n",
      "\n",
      "Processing disease: MONDO_0000591\n",
      "MONDO_0000591 - Total distinct nodes N: 30; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 28, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000833\n",
      "MONDO_0000833 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping MONDO_0000833: too few nodes (2)\n",
      "\n",
      "Processing disease: HP_0002715\n",
      "HP_0002715 - Total distinct nodes N: 15; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 13, Using 10\n",
      "\n",
      "Processing disease: EFO_1000304\n",
      "EFO_1000304 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: MONDO_0044925\n",
      "MONDO_0044925 - Total distinct nodes N: 43; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 41, Using 10\n",
      "\n",
      "Processing disease: MONDO_0001657\n",
      "MONDO_0001657 - Total distinct nodes N: 9; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 7, Using 7\n",
      "\n",
      "Processing disease: MONDO_0004643\n",
      "MONDO_0004643 - Total distinct nodes N: 13; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 11, Using 10\n",
      "\n",
      "Processing disease: EFO_0005815\n",
      "EFO_0005815 - Total distinct nodes N: 23; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 21, Using 10\n",
      "\n",
      "Processing disease: EFO_0003841\n",
      "EFO_0003841 - Total distinct nodes N: 30; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 28, Using 10\n",
      "\n",
      "Processing disease: EFO_0000685\n",
      "EFO_0000685 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_0004280\n",
      "EFO_0004280 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_0001069\n",
      "EFO_0001069 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: MONDO_0000640\n",
      "MONDO_0000640 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_1001968\n",
      "EFO_1001968 - Total distinct nodes N: 14; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 12, Using 10\n",
      "\n",
      "Processing disease: HP_0001977\n",
      "HP_0001977 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: EFO_0002916\n",
      "EFO_0002916 - Total distinct nodes N: 21; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 19, Using 10\n",
      "\n",
      "Processing disease: EFO_0022196\n",
      "EFO_0022196 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0022196: too few nodes (2)\n",
      "\n",
      "Processing disease: MONDO_0021375\n",
      "MONDO_0021375 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping MONDO_0021375: too few nodes (2)\n",
      "\n",
      "Processing disease: MONDO_0024296\n",
      "MONDO_0024296 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "Attempt 1 failed: Cannot sample 8 negative nodes from a set of 7 negative nodes for R-HSA-162582\n",
      "Retrying with 6 negatives\n",
      "\n",
      "Processing disease: MONDO_0020663\n",
      "MONDO_0020663 - Total distinct nodes N: 14; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 12, Using 10\n",
      "\n",
      "Processing disease: EFO_1000044\n",
      "EFO_1000044 - Total distinct nodes N: 9; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 7, Using 7\n",
      "\n",
      "Processing disease: EFO_0000706\n",
      "EFO_0000706 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: MONDO_0000588\n",
      "MONDO_0000588 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping MONDO_0000588: too few nodes (2)\n",
      "\n",
      "Processing disease: HP_0001877\n",
      "HP_0001877 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "Attempt 1 failed: Cannot sample 8 negative nodes from a set of 6 negative nodes for R-HSA-8953854\n",
      "Retrying with 6 negatives\n",
      "\n",
      "Processing disease: HP_0000598\n",
      "HP_0000598 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping HP_0000598: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_1000359\n",
      "EFO_1000359 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: MONDO_0004670\n",
      "MONDO_0004670 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: MONDO_0005411\n",
      "MONDO_0005411 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping MONDO_0005411: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0004695\n",
      "EFO_0004695 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0004695: too few nodes (2)\n",
      "\n",
      "Processing disease: MONDO_0040677\n",
      "MONDO_0040677 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_1000217\n",
      "EFO_1000217 - Total distinct nodes N: 17; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 15, Using 10\n",
      "\n",
      "Processing disease: EFO_0005775\n",
      "EFO_0005775 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_0004713\n",
      "EFO_0004713 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: EFO_0004631\n",
      "EFO_0004631 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0004631: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_1001455\n",
      "EFO_1001455 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "Attempt 1 failed: Cannot sample 5 negative nodes from a set of 3 negative nodes for R-HSA-8953854\n",
      "Retrying with 3 negatives\n",
      "\n",
      "Processing disease: EFO_0004606\n",
      "EFO_0004606 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_0000538\n",
      "EFO_0000538 - Total distinct nodes N: 31; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 29, Using 10\n",
      "\n",
      "Processing disease: MONDO_0003061\n",
      "MONDO_0003061 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: MONDO_0021632\n",
      "MONDO_0021632 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "Attempt 1 failed: Cannot sample 6 negative nodes from a set of 5 negative nodes for R-HSA-162582\n",
      "Retrying with 4 negatives\n",
      "\n",
      "Processing disease: EFO_0003851\n",
      "EFO_0003851 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: EFO_0020945\n",
      "EFO_0020945 - Total distinct nodes N: 12; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 10, Using 10\n",
      "\n",
      "Processing disease: EFO_0001378\n",
      "EFO_0001378 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0001378: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0004729\n",
      "EFO_0004729 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0004729: too few nodes (2)\n",
      "\n",
      "Processing disease: MONDO_0023644\n",
      "MONDO_0023644 - Total distinct nodes N: 37; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 35, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000726\n",
      "MONDO_0000726 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping MONDO_0000726: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0000474\n",
      "EFO_0000474 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_1000307\n",
      "EFO_1000307 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_0000384\n",
      "EFO_0000384 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0000384: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_1002050\n",
      "EFO_1002050 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "Attempt 1 failed: Cannot sample 8 negative nodes from a set of 5 negative nodes for R-HSA-1474244\n",
      "Retrying with 6 negatives\n",
      "Failed after 2 attempts for EFO_1002050: Cannot sample 6 negative nodes from a set of 5 negative nodes for R-HSA-1474244\n",
      "\n",
      "Processing disease: EFO_0003872\n",
      "EFO_0003872 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0003872: too few nodes (2)\n",
      "\n",
      "Processing disease: HP_0001249\n",
      "HP_0001249 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping HP_0001249: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0000676\n",
      "EFO_0000676 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0000676: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_1000223\n",
      "EFO_1000223 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_1000223: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0006812\n",
      "EFO_0006812 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: MONDO_0021080\n",
      "MONDO_0021080 - Total distinct nodes N: 18; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 16, Using 10\n",
      "\n",
      "Processing disease: EFO_0005110\n",
      "EFO_0005110 - Total distinct nodes N: 13; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 11, Using 10\n",
      "\n",
      "Processing disease: EFO_0004273\n",
      "EFO_0004273 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0004273: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_1000636\n",
      "EFO_1000636 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_1000636: too few nodes (2)\n",
      "\n",
      "Processing disease: MONDO_0002917\n",
      "MONDO_0002917 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping MONDO_0002917: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0003888\n",
      "EFO_0003888 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0003888: too few nodes (2)\n",
      "\n",
      "Processing disease: MONDO_0001572\n",
      "MONDO_0001572 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "Attempt 1 failed: Cannot sample 6 negative nodes from a set of 5 negative nodes for R-HSA-162582\n",
      "Retrying with 4 negatives\n",
      "\n",
      "Processing disease: EFO_0003100\n",
      "EFO_0003100 - Total distinct nodes N: 9; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 7, Using 7\n",
      "\n",
      "Processing disease: EFO_1000121\n",
      "EFO_1000121 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "Attempt 1 failed: Cannot sample 6 negative nodes from a set of 5 negative nodes for R-HSA-162582\n",
      "Retrying with 4 negatives\n",
      "\n",
      "Processing disease: MONDO_0002232\n",
      "MONDO_0002232 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping MONDO_0002232: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0000662\n",
      "EFO_0000662 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0000662: too few nodes (2)\n",
      "\n",
      "Processing disease: HP_0025142\n",
      "HP_0025142 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_0000729\n",
      "EFO_0000729 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0000729: too few nodes (2)\n",
      "\n",
      "Processing disease: HP_0040064\n",
      "HP_0040064 - Total distinct nodes N: 11; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 9, Using 9\n",
      "\n",
      "Processing disease: MONDO_0000569\n",
      "MONDO_0000569 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: MONDO_0004126\n",
      "MONDO_0004126 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_0005784\n",
      "EFO_0005784 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: MONDO_0004975\n",
      "MONDO_0004975 - Total distinct nodes N: 17; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 15, Using 10\n",
      "\n",
      "Processing disease: GO_0036273\n",
      "GO_0036273 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping GO_0036273: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0000691\n",
      "EFO_0000691 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "Attempt 1 failed: Cannot sample 6 negative nodes from a set of 5 negative nodes for R-HSA-162582\n",
      "Retrying with 4 negatives\n",
      "\n",
      "Processing disease: MONDO_0021335\n",
      "MONDO_0021335 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping MONDO_0021335: too few nodes (2)\n",
      "\n",
      "Processing disease: HP_0004936\n",
      "HP_0004936 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: EFO_0006500\n",
      "EFO_0006500 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0006500: too few nodes (2)\n",
      "\n",
      "Processing disease: MONDO_0002280\n",
      "MONDO_0002280 - Total distinct nodes N: 35; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 33, Using 10\n",
      "\n",
      "Processing disease: EFO_0000365\n",
      "EFO_0000365 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "Attempt 1 failed: Cannot sample 8 negative nodes from a set of 7 negative nodes for R-HSA-162582\n",
      "Retrying with 6 negatives\n",
      "\n",
      "Processing disease: EFO_1000262\n",
      "EFO_1000262 - Total distinct nodes N: 15; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 13, Using 10\n",
      "\n",
      "Processing disease: EFO_0010118\n",
      "EFO_0010118 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_0005541\n",
      "EFO_0005541 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_0003931\n",
      "EFO_0003931 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0003931: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0020092\n",
      "EFO_0020092 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0020092: too few nodes (2)\n",
      "\n",
      "Processing disease: MONDO_0005301\n",
      "MONDO_0005301 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping MONDO_0005301: too few nodes (2)\n",
      "\n",
      "Processing disease: MONDO_0002525\n",
      "MONDO_0002525 - Total distinct nodes N: 15; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 13, Using 10\n",
      "\n",
      "Processing disease: EFO_0008595\n",
      "EFO_0008595 - Total distinct nodes N: 11; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 9, Using 9\n",
      "\n",
      "Processing disease: MONDO_0004580\n",
      "MONDO_0004580 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_0005653\n",
      "EFO_0005653 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_0005680\n",
      "EFO_0005680 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_0009608\n",
      "EFO_0009608 - Total distinct nodes N: 23; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 21, Using 10\n",
      "\n",
      "Processing disease: EFO_0004622\n",
      "EFO_0004622 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_1000870\n",
      "EFO_1000870 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_1000870: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0004145\n",
      "EFO_0004145 - Total distinct nodes N: 23; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 21, Using 10\n",
      "\n",
      "Processing disease: HP_0003124\n",
      "HP_0003124 - Total distinct nodes N: 9; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 7, Using 7\n",
      "\n",
      "Processing disease: EFO_0010226\n",
      "EFO_0010226 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_0020947\n",
      "EFO_0020947 - Total distinct nodes N: 16; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 14, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002562\n",
      "MONDO_0002562 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping MONDO_0002562: too few nodes (2)\n",
      "\n",
      "Processing disease: HP_0100790\n",
      "HP_0100790 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: HP_0003119\n",
      "HP_0003119 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "\n",
      "Processing disease: EFO_1000020\n",
      "EFO_1000020 - Total distinct nodes N: 25; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 23, Using 10\n"
     ]
    }
   ],
   "source": [
    "pem_level1 = process_hierarchy_diseases(\"/Users/polina/Pathwaganda/data/GSEA-output_filt_hier\", \n",
    "                                        max_hierLevel=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb736b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "pem_level1.write.mode(\"overwrite\").parquet(\"/Users/polina/Pathwaganda/data/pem_levels/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad28306e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering to hierarchy levels <= 2\n",
      "\n",
      "Processing disease: EFO_0000701\n",
      "EFO_0000701 - Total distinct nodes N: 39; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 37, Using 10\n",
      "\n",
      "Processing disease: EFO_0004872\n",
      "EFO_0004872 - Total distinct nodes N: 40; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 38, Using 10\n",
      "\n",
      "Processing disease: EFO_0005803\n",
      "EFO_0005803 - Total distinct nodes N: 145; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 143, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002715\n",
      "MONDO_0002715 - Total distinct nodes N: 169; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 167, Using 10\n",
      "\n",
      "Processing disease: GO_0008150\n",
      "GO_0008150 - Total distinct nodes N: 20; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 18, Using 10\n",
      "\n",
      "Processing disease: HP_0012638\n",
      "HP_0012638 - Total distinct nodes N: 19; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 17, Using 10\n",
      "\n",
      "Processing disease: MONDO_0023370\n",
      "MONDO_0023370 - Total distinct nodes N: 190; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 188, Using 10\n",
      "\n",
      "Processing disease: EFO_0004516\n",
      "EFO_0004516 - Total distinct nodes N: 29; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 27, Using 10\n",
      "\n",
      "Processing disease: EFO_0005127\n",
      "EFO_0005127 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: EFO_0803540\n",
      "EFO_0803540 - Total distinct nodes N: 19; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 17, Using 10\n",
      "\n",
      "Processing disease: EFO_0010285\n",
      "EFO_0010285 - Total distinct nodes N: 86; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 84, Using 10\n",
      "\n",
      "Processing disease: HP_0001626\n",
      "HP_0001626 - Total distinct nodes N: 67; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 65, Using 10\n",
      "\n",
      "Processing disease: MONDO_0007254\n",
      "MONDO_0007254 - Total distinct nodes N: 143; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 141, Using 10\n",
      "\n",
      "Processing disease: EFO_0004833\n",
      "EFO_0004833 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "\n",
      "Processing disease: EFO_0006843\n",
      "EFO_0006843 - Total distinct nodes N: 38; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 36, Using 10\n",
      "\n",
      "Processing disease: EFO_0010284\n",
      "EFO_0010284 - Total distinct nodes N: 18; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 16, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002039\n",
      "MONDO_0002039 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: EFO_0004574\n",
      "EFO_0004574 - Total distinct nodes N: 28; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 26, Using 10\n",
      "\n",
      "Processing disease: EFO_0002461\n",
      "EFO_0002461 - Total distinct nodes N: 124; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 122, Using 10\n",
      "\n",
      "Processing disease: EFO_1001986\n",
      "EFO_1001986 - Total distinct nodes N: 71; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 69, Using 10\n",
      "\n",
      "Processing disease: EFO_0004517\n",
      "EFO_0004517 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "\n",
      "Processing disease: EFO_0004747\n",
      "EFO_0004747 - Total distinct nodes N: 180; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 178, Using 10\n",
      "\n",
      "Processing disease: EFO_0004512\n",
      "EFO_0004512 - Total distinct nodes N: 26; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 24, Using 10\n",
      "\n",
      "Processing disease: EFO_0004269\n",
      "EFO_0004269 - Total distinct nodes N: 17; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 15, Using 10\n",
      "\n",
      "Processing disease: MONDO_0015757\n",
      "MONDO_0015757 - Total distinct nodes N: 72; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 70, Using 10\n",
      "\n",
      "Processing disease: EFO_0005771\n",
      "EFO_0005771 - Total distinct nodes N: 36; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 34, Using 10\n",
      "\n",
      "Processing disease: GO_0032501\n",
      "GO_0032501 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_0004586\n",
      "EFO_0004586 - Total distinct nodes N: 158; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 156, Using 10\n",
      "\n",
      "Processing disease: EFO_0009433\n",
      "EFO_0009433 - Total distinct nodes N: 32; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 30, Using 10\n",
      "\n",
      "Processing disease: EFO_0004527\n",
      "EFO_0004527 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_0009676\n",
      "EFO_0009676 - Total distinct nodes N: 149; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 147, Using 10\n",
      "\n",
      "Processing disease: EFO_0004260\n",
      "EFO_0004260 - Total distinct nodes N: 95; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 93, Using 10\n",
      "\n",
      "Processing disease: MONDO_0008315\n",
      "MONDO_0008315 - Total distinct nodes N: 43; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 41, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002025\n",
      "MONDO_0002025 - Total distinct nodes N: 33; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 31, Using 10\n",
      "\n",
      "Processing disease: EFO_0004530\n",
      "EFO_0004530 - Total distinct nodes N: 32; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 30, Using 10\n",
      "\n",
      "Processing disease: EFO_0004346\n",
      "EFO_0004346 - Total distinct nodes N: 19; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 17, Using 10\n",
      "\n",
      "Processing disease: MONDO_0003274\n",
      "MONDO_0003274 - Total distinct nodes N: 90; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 88, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021230\n",
      "MONDO_0021230 - Total distinct nodes N: 107; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 105, Using 10\n",
      "\n",
      "Processing disease: EFO_1001331\n",
      "EFO_1001331 - Total distinct nodes N: 154; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 152, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021066\n",
      "MONDO_0021066 - Total distinct nodes N: 169; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 167, Using 10\n",
      "\n",
      "Processing disease: EFO_0005741\n",
      "EFO_0005741 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: EFO_0003892\n",
      "EFO_0003892 - Total distinct nodes N: 35; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 33, Using 10\n",
      "\n",
      "Processing disease: EFO_0000508\n",
      "EFO_0000508 - Total distinct nodes N: 231; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 229, Using 10\n",
      "\n",
      "Processing disease: EFO_0004302\n",
      "EFO_0004302 - Total distinct nodes N: 125; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 123, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021148\n",
      "MONDO_0021148 - Total distinct nodes N: 193; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 191, Using 10\n",
      "\n",
      "Processing disease: EFO_0004611\n",
      "EFO_0004611 - Total distinct nodes N: 60; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 58, Using 10\n",
      "\n",
      "Processing disease: EFO_0004509\n",
      "EFO_0004509 - Total distinct nodes N: 51; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 49, Using 10\n",
      "\n",
      "Processing disease: EFO_0009386\n",
      "EFO_0009386 - Total distinct nodes N: 82; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 80, Using 10\n",
      "\n",
      "Processing disease: EFO_0001379\n",
      "EFO_0001379 - Total distinct nodes N: 94; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 92, Using 10\n",
      "\n",
      "Processing disease: EFO_0011008\n",
      "EFO_0011008 - Total distinct nodes N: 11; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 9, Using 9\n",
      "\n",
      "Processing disease: EFO_0000651\n",
      "EFO_0000651 - Total distinct nodes N: 127; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 125, Using 10\n",
      "\n",
      "Processing disease: EFO_0004348\n",
      "EFO_0004348 - Total distinct nodes N: 11; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 9, Using 9\n",
      "\n",
      "Processing disease: EFO_0004309\n",
      "EFO_0004309 - Total distinct nodes N: 53; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 51, Using 10\n",
      "\n",
      "Processing disease: EFO_0004614\n",
      "EFO_0004614 - Total distinct nodes N: 23; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 21, Using 10\n",
      "\n",
      "Processing disease: EFO_0005755\n",
      "EFO_0005755 - Total distinct nodes N: 22; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 20, Using 10\n",
      "\n",
      "Processing disease: EFO_0803547\n",
      "EFO_0803547 - Total distinct nodes N: 30; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 28, Using 10\n",
      "\n",
      "Processing disease: EFO_0000719\n",
      "EFO_0000719 - Total distinct nodes N: 18; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 16, Using 10\n",
      "\n",
      "Processing disease: EFO_0004995\n",
      "EFO_0004995 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_0004264\n",
      "EFO_0004264 - Total distinct nodes N: 128; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 126, Using 10\n",
      "\n",
      "Processing disease: EFO_0000574\n",
      "EFO_0000574 - Total distinct nodes N: 73; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 71, Using 10\n",
      "\n",
      "Processing disease: EFO_0006842\n",
      "EFO_0006842 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_0009431\n",
      "EFO_0009431 - Total distinct nodes N: 38; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 36, Using 10\n",
      "\n",
      "Processing disease: EFO_0005809\n",
      "EFO_0005809 - Total distinct nodes N: 42; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 40, Using 10\n",
      "\n",
      "Processing disease: EFO_0004303\n",
      "EFO_0004303 - Total distinct nodes N: 46; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 44, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021259\n",
      "MONDO_0021259 - Total distinct nodes N: 67; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 65, Using 10\n",
      "\n",
      "Processing disease: EFO_0004298\n",
      "EFO_0004298 - Total distinct nodes N: 114; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 112, Using 10\n",
      "\n",
      "Processing disease: MONDO_0001933\n",
      "MONDO_0001933 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: HP_0011842\n",
      "HP_0011842 - Total distinct nodes N: 21; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 19, Using 10\n",
      "\n",
      "Processing disease: EFO_0004198\n",
      "EFO_0004198 - Total distinct nodes N: 129; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 127, Using 10\n",
      "\n",
      "Processing disease: EFO_0005105\n",
      "EFO_0005105 - Total distinct nodes N: 117; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 115, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002259\n",
      "MONDO_0002259 - Total distinct nodes N: 52; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 50, Using 10\n",
      "\n",
      "Processing disease: MONDO_0003939\n",
      "MONDO_0003939 - Total distinct nodes N: 94; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 92, Using 10\n",
      "\n",
      "Processing disease: EFO_0007987\n",
      "EFO_0007987 - Total distinct nodes N: 23; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 21, Using 10\n",
      "\n",
      "Processing disease: EFO_0004587\n",
      "EFO_0004587 - Total distinct nodes N: 41; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 39, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002334\n",
      "MONDO_0002334 - Total distinct nodes N: 95; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 93, Using 10\n",
      "\n",
      "Processing disease: EFO_1000051\n",
      "EFO_1000051 - Total distinct nodes N: 194; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 192, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021350\n",
      "MONDO_0021350 - Total distinct nodes N: 109; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 107, Using 10\n",
      "\n",
      "Processing disease: EFO_0009682\n",
      "EFO_0009682 - Total distinct nodes N: 13; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 11, Using 10\n",
      "\n",
      "Processing disease: HP_0000924\n",
      "HP_0000924 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: EFO_0803548\n",
      "EFO_0803548 - Total distinct nodes N: 12; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 10, Using 10\n",
      "\n",
      "Processing disease: EFO_0004340\n",
      "EFO_0004340 - Total distinct nodes N: 25; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 23, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000473\n",
      "MONDO_0000473 - Total distinct nodes N: 48; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 46, Using 10\n",
      "\n",
      "Processing disease: EFO_1000999\n",
      "EFO_1000999 - Total distinct nodes N: 12; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 10, Using 10\n",
      "\n",
      "Processing disease: Orphanet_322126\n",
      "Orphanet_322126 - Total distinct nodes N: 54; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 52, Using 10\n",
      "\n",
      "Processing disease: EFO_0000684\n",
      "EFO_0000684 - Total distinct nodes N: 23; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 21, Using 10\n",
      "\n",
      "Processing disease: EFO_0001421\n",
      "EFO_0001421 - Total distinct nodes N: 17; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 15, Using 10\n",
      "\n",
      "Processing disease: EFO_0000589\n",
      "EFO_0000589 - Total distinct nodes N: 86; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 84, Using 10\n",
      "\n",
      "Processing disease: EFO_0004503\n",
      "EFO_0004503 - Total distinct nodes N: 186; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 184, Using 10\n",
      "\n",
      "Processing disease: MONDO_0044881\n",
      "MONDO_0044881 - Total distinct nodes N: 94; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 92, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002614\n",
      "MONDO_0002614 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: EFO_0004612\n",
      "EFO_0004612 - Total distinct nodes N: 57; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 55, Using 10\n",
      "\n",
      "Processing disease: EFO_0009690\n",
      "EFO_0009690 - Total distinct nodes N: 71; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 69, Using 10\n",
      "\n",
      "Processing disease: EFO_0007937\n",
      "EFO_0007937 - Total distinct nodes N: 59; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 57, Using 10\n",
      "\n",
      "Processing disease: EFO_0006841\n",
      "EFO_0006841 - Total distinct nodes N: 24; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 22, Using 10\n",
      "\n",
      "Processing disease: HP_0000118\n",
      "HP_0000118 - Total distinct nodes N: 205; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 203, Using 10\n",
      "\n",
      "Processing disease: EFO_0002919\n",
      "EFO_0002919 - Total distinct nodes N: 119; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 117, Using 10\n",
      "\n",
      "Processing disease: EFO_0005278\n",
      "EFO_0005278 - Total distinct nodes N: 111; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 109, Using 10\n",
      "\n",
      "Processing disease: EFO_0010282\n",
      "EFO_0010282 - Total distinct nodes N: 73; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 71, Using 10\n",
      "\n",
      "Processing disease: EFO_0006335\n",
      "EFO_0006335 - Total distinct nodes N: 29; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 27, Using 10\n",
      "\n",
      "Processing disease: MONDO_0045024\n",
      "MONDO_0045024 - Total distinct nodes N: 197; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 195, Using 10\n",
      "\n",
      "Processing disease: EFO_0000400\n",
      "EFO_0000400 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: EFO_0005774\n",
      "EFO_0005774 - Total distinct nodes N: 41; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 39, Using 10\n",
      "\n",
      "Processing disease: GO_0050896\n",
      "GO_0050896 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_0803546\n",
      "EFO_0803546 - Total distinct nodes N: 30; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 28, Using 10\n",
      "\n",
      "Processing disease: EFO_1002003\n",
      "EFO_1002003 - Total distinct nodes N: 69; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 67, Using 10\n",
      "\n",
      "Processing disease: MONDO_0024582\n",
      "MONDO_0024582 - Total distinct nodes N: 79; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 77, Using 10\n",
      "\n",
      "Processing disease: EFO_0007010\n",
      "EFO_0007010 - Total distinct nodes N: 35; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 33, Using 10\n",
      "\n",
      "Processing disease: EFO_0004468\n",
      "EFO_0004468 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0004468: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0001444\n",
      "EFO_0001444 - Total distinct nodes N: 153; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 151, Using 10\n",
      "\n",
      "Processing disease: HP_0000707\n",
      "HP_0000707 - Total distinct nodes N: 36; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 34, Using 10\n",
      "\n",
      "Processing disease: EFO_0007355\n",
      "EFO_0007355 - Total distinct nodes N: 61; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 59, Using 10\n",
      "\n",
      "Processing disease: EFO_0004338\n",
      "EFO_0004338 - Total distinct nodes N: 22; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 20, Using 10\n",
      "\n",
      "Processing disease: EFO_0003869\n",
      "EFO_0003869 - Total distinct nodes N: 131; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 129, Using 10\n",
      "\n",
      "Processing disease: EFO_1000363\n",
      "EFO_1000363 - Total distinct nodes N: 154; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 152, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021581\n",
      "MONDO_0021581 - Total distinct nodes N: 109; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 107, Using 10\n",
      "\n",
      "Processing disease: EFO_0004732\n",
      "EFO_0004732 - Total distinct nodes N: 76; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 74, Using 10\n",
      "\n",
      "Processing disease: EFO_0000618\n",
      "EFO_0000618 - Total distinct nodes N: 130; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 128, Using 10\n",
      "\n",
      "Processing disease: EFO_0004555\n",
      "EFO_0004555 - Total distinct nodes N: 14; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 12, Using 10\n",
      "\n",
      "Processing disease: EFO_0004980\n",
      "EFO_0004980 - Total distinct nodes N: 9; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 7, Using 7\n",
      "\n",
      "Processing disease: MONDO_0002974\n",
      "MONDO_0002974 - Total distinct nodes N: 74; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 72, Using 10\n",
      "\n",
      "Processing disease: EFO_0006858\n",
      "EFO_0006858 - Total distinct nodes N: 187; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 185, Using 10\n",
      "\n",
      "Processing disease: EFO_0011015\n",
      "EFO_0011015 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: MONDO_0002149\n",
      "MONDO_0002149 - Total distinct nodes N: 172; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 170, Using 10\n",
      "\n",
      "Processing disease: EFO_0003777\n",
      "EFO_0003777 - Total distinct nodes N: 121; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 119, Using 10\n",
      "\n",
      "Processing disease: GO_0007610\n",
      "GO_0007610 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_0001061\n",
      "EFO_0001061 - Total distinct nodes N: 95; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 93, Using 10\n",
      "\n",
      "Processing disease: EFO_0007441\n",
      "EFO_0007441 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: EFO_0000228\n",
      "EFO_0000228 - Total distinct nodes N: 72; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 70, Using 10\n",
      "\n",
      "Processing disease: EFO_0003893\n",
      "EFO_0003893 - Total distinct nodes N: 160; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 158, Using 10\n",
      "\n",
      "Processing disease: EFO_0004528\n",
      "EFO_0004528 - Total distinct nodes N: 32; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 30, Using 10\n",
      "\n",
      "Processing disease: EFO_0003966\n",
      "EFO_0003966 - Total distinct nodes N: 46; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 44, Using 10\n",
      "\n",
      "Processing disease: EFO_0005091\n",
      "EFO_0005091 - Total distinct nodes N: 32; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 30, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002654\n",
      "MONDO_0002654 - Total distinct nodes N: 21; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 19, Using 10\n",
      "\n",
      "Processing disease: Orphanet_68336\n",
      "Orphanet_68336 - Total distinct nodes N: 161; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 159, Using 10\n",
      "\n",
      "Processing disease: EFO_0000616\n",
      "EFO_0000616 - Total distinct nodes N: 188; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 186, Using 10\n",
      "\n",
      "Processing disease: EFO_0000305\n",
      "EFO_0000305 - Total distinct nodes N: 60; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 58, Using 10\n",
      "\n",
      "Processing disease: EFO_0009406\n",
      "EFO_0009406 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_0000537\n",
      "EFO_0000537 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: EFO_0004306\n",
      "EFO_0004306 - Total distinct nodes N: 55; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 53, Using 10\n",
      "\n",
      "Processing disease: EFO_0001663\n",
      "EFO_0001663 - Total distinct nodes N: 68; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 66, Using 10\n",
      "\n",
      "Processing disease: EFO_0004529\n",
      "EFO_0004529 - Total distinct nodes N: 136; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 134, Using 10\n",
      "\n",
      "Processing disease: EFO_0000677\n",
      "EFO_0000677 - Total distinct nodes N: 18; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 16, Using 10\n",
      "\n",
      "Processing disease: EFO_0000319\n",
      "EFO_0000319 - Total distinct nodes N: 151; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 149, Using 10\n",
      "\n",
      "Processing disease: EFO_0004842\n",
      "EFO_0004842 - Total distinct nodes N: 17; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 15, Using 10\n",
      "\n",
      "Processing disease: EFO_0009605\n",
      "EFO_0009605 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_0005772\n",
      "EFO_0005772 - Total distinct nodes N: 31; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 29, Using 10\n",
      "\n",
      "Processing disease: EFO_0000313\n",
      "EFO_0000313 - Total distinct nodes N: 160; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 158, Using 10\n",
      "\n",
      "Processing disease: EFO_0000540\n",
      "EFO_0000540 - Total distinct nodes N: 120; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 118, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000653\n",
      "MONDO_0000653 - Total distinct nodes N: 99; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 97, Using 10\n",
      "\n",
      "Processing disease: EFO_0005140\n",
      "EFO_0005140 - Total distinct nodes N: 55; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 53, Using 10\n",
      "\n",
      "Processing disease: EFO_0002422\n",
      "EFO_0002422 - Total distinct nodes N: 146; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 144, Using 10\n",
      "\n",
      "Processing disease: EFO_0008549\n",
      "EFO_0008549 - Total distinct nodes N: 138; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 136, Using 10\n",
      "\n",
      "Processing disease: EFO_0003086\n",
      "EFO_0003086 - Total distinct nodes N: 66; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 64, Using 10\n",
      "\n",
      "Processing disease: EFO_0004324\n",
      "EFO_0004324 - Total distinct nodes N: 59; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 57, Using 10\n",
      "\n",
      "Processing disease: EFO_0003859\n",
      "EFO_0003859 - Total distinct nodes N: 175; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 173, Using 10\n",
      "\n",
      "Processing disease: EFO_0006336\n",
      "EFO_0006336 - Total distinct nodes N: 9; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 7, Using 7\n",
      "\n",
      "Processing disease: EFO_0004627\n",
      "EFO_0004627 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_0009549\n",
      "EFO_0009549 - Total distinct nodes N: 62; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 60, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000426\n",
      "MONDO_0000426 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping MONDO_0000426: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0004725\n",
      "EFO_0004725 - Total distinct nodes N: 80; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 78, Using 10\n",
      "\n",
      "Processing disease: EFO_0004305\n",
      "EFO_0004305 - Total distinct nodes N: 29; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 27, Using 10\n",
      "\n",
      "Processing disease: EFO_0004531\n",
      "EFO_0004531 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_0004730\n",
      "EFO_0004730 - Total distinct nodes N: 20; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 18, Using 10\n",
      "\n",
      "Processing disease: EFO_0005856\n",
      "EFO_0005856 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: EFO_0007985\n",
      "EFO_0007985 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_0000512\n",
      "EFO_0000512 - Total distinct nodes N: 53; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 51, Using 10\n",
      "\n",
      "Processing disease: MONDO_0004992\n",
      "MONDO_0004992 - Total distinct nodes N: 200; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 198, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021193\n",
      "MONDO_0021193 - Total distinct nodes N: 44; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 42, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021248\n",
      "MONDO_0021248 - Total distinct nodes N: 91; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 89, Using 10\n",
      "\n",
      "Processing disease: MONDO_0017343\n",
      "MONDO_0017343 - Total distinct nodes N: 21; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 19, Using 10\n",
      "\n",
      "Processing disease: MONDO_0017595\n",
      "MONDO_0017595 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: EFO_1000627\n",
      "EFO_1000627 - Total distinct nodes N: 23; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 21, Using 10\n",
      "\n",
      "Processing disease: EFO_0000565\n",
      "EFO_0000565 - Total distinct nodes N: 34; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 32, Using 10\n",
      "\n",
      "Processing disease: EFO_1000068\n",
      "EFO_1000068 - Total distinct nodes N: 40; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 38, Using 10\n",
      "\n",
      "Processing disease: EFO_0004838\n",
      "EFO_0004838 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: HP_0000478\n",
      "HP_0000478 - Total distinct nodes N: 11; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 9, Using 9\n",
      "\n",
      "Processing disease: EFO_0009555\n",
      "EFO_0009555 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: MONDO_0004095\n",
      "MONDO_0004095 - Total distinct nodes N: 78; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 76, Using 10\n",
      "\n",
      "Processing disease: EFO_0010642\n",
      "EFO_0010642 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: HP_0002011\n",
      "HP_0002011 - Total distinct nodes N: 24; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 22, Using 10\n",
      "\n",
      "Processing disease: EFO_0005689\n",
      "EFO_0005689 - Total distinct nodes N: 19; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 17, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021583\n",
      "MONDO_0021583 - Total distinct nodes N: 55; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 53, Using 10\n",
      "\n",
      "Processing disease: EFO_0007392\n",
      "EFO_0007392 - Total distinct nodes N: 55; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 53, Using 10\n",
      "\n",
      "Processing disease: MONDO_0024276\n",
      "MONDO_0024276 - Total distinct nodes N: 51; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 49, Using 10\n",
      "\n",
      "Processing disease: EFO_1000172\n",
      "EFO_1000172 - Total distinct nodes N: 90; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 88, Using 10\n",
      "\n",
      "Processing disease: EFO_0001075\n",
      "EFO_0001075 - Total distinct nodes N: 75; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 73, Using 10\n",
      "\n",
      "Processing disease: EFO_0007991\n",
      "EFO_0007991 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: MONDO_0002406\n",
      "MONDO_0002406 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: MONDO_0008170\n",
      "MONDO_0008170 - Total distinct nodes N: 31; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 29, Using 10\n",
      "\n",
      "Processing disease: EFO_0000503\n",
      "EFO_0000503 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0000503: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0004149\n",
      "EFO_0004149 - Total distinct nodes N: 32; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 30, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021143\n",
      "MONDO_0021143 - Total distinct nodes N: 78; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 76, Using 10\n",
      "\n",
      "Processing disease: EFO_0009259\n",
      "EFO_0009259 - Total distinct nodes N: 47; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 45, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021662\n",
      "MONDO_0021662 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: MONDO_0003060\n",
      "MONDO_0003060 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: EFO_0009119\n",
      "EFO_0009119 - Total distinct nodes N: 98; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 96, Using 10\n",
      "\n",
      "Processing disease: MONDO_0008903\n",
      "MONDO_0008903 - Total distinct nodes N: 22; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 20, Using 10\n",
      "\n",
      "Processing disease: EFO_0000096\n",
      "EFO_0000096 - Total distinct nodes N: 75; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 73, Using 10\n",
      "\n",
      "Processing disease: EFO_0000708\n",
      "EFO_0000708 - Total distinct nodes N: 16; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 14, Using 10\n",
      "\n",
      "Processing disease: EFO_0000466\n",
      "EFO_0000466 - Total distinct nodes N: 56; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 54, Using 10\n",
      "\n",
      "Processing disease: EFO_0004784\n",
      "EFO_0004784 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0004784: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0004458\n",
      "EFO_0004458 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "\n",
      "Processing disease: EFO_0002427\n",
      "EFO_0002427 - Total distinct nodes N: 27; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 25, Using 10\n",
      "\n",
      "Processing disease: EFO_0004142\n",
      "EFO_0004142 - Total distinct nodes N: 28; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 26, Using 10\n",
      "\n",
      "Processing disease: MONDO_0018364\n",
      "MONDO_0018364 - Total distinct nodes N: 47; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 45, Using 10\n",
      "\n",
      "Processing disease: EFO_1002018\n",
      "EFO_1002018 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: MONDO_0007263\n",
      "MONDO_0007263 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: EFO_0000220\n",
      "EFO_0000220 - Total distinct nodes N: 59; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 57, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002229\n",
      "MONDO_0002229 - Total distinct nodes N: 57; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 55, Using 10\n",
      "\n",
      "Processing disease: EFO_0002425\n",
      "EFO_0002425 - Total distinct nodes N: 51; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 49, Using 10\n",
      "\n",
      "Processing disease: MONDO_0004867\n",
      "MONDO_0004867 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: MONDO_0002529\n",
      "MONDO_0002529 - Total distinct nodes N: 34; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 32, Using 10\n",
      "\n",
      "Processing disease: EFO_0005548\n",
      "EFO_0005548 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: EFO_0004741\n",
      "EFO_0004741 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: MONDO_0024477\n",
      "MONDO_0024477 - Total distinct nodes N: 44; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 42, Using 10\n",
      "\n",
      "Processing disease: EFO_0803539\n",
      "EFO_0803539 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_0007330\n",
      "EFO_0007330 - Total distinct nodes N: 37; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 35, Using 10\n",
      "\n",
      "Processing disease: EFO_0000681\n",
      "EFO_0000681 - Total distinct nodes N: 21; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 19, Using 10\n",
      "\n",
      "Processing disease: EFO_0004314\n",
      "EFO_0004314 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0004314: too few nodes (2)\n",
      "\n",
      "Processing disease: MONDO_0002367\n",
      "MONDO_0002367 - Total distinct nodes N: 27; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 25, Using 10\n",
      "\n",
      "Processing disease: EFO_0005952\n",
      "EFO_0005952 - Total distinct nodes N: 80; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 78, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021118\n",
      "MONDO_0021118 - Total distinct nodes N: 48; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 46, Using 10\n",
      "\n",
      "Processing disease: HP_0012443\n",
      "HP_0012443 - Total distinct nodes N: 13; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 11, Using 10\n",
      "\n",
      "Processing disease: MONDO_0005575\n",
      "MONDO_0005575 - Total distinct nodes N: 24; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 22, Using 10\n",
      "\n",
      "Processing disease: EFO_0003865\n",
      "EFO_0003865 - Total distinct nodes N: 44; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 42, Using 10\n",
      "\n",
      "Processing disease: EFO_0007993\n",
      "EFO_0007993 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: MONDO_0000429\n",
      "MONDO_0000429 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: MONDO_0002516\n",
      "MONDO_0002516 - Total distinct nodes N: 153; Chosen d: 8\n",
      "Negative sampling: Requested 10, Safe maximum 151, Using 10\n",
      "\n",
      "Processing disease: HP_0001574\n",
      "HP_0001574 - Total distinct nodes N: 9; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 7, Using 7\n",
      "\n",
      "Processing disease: EFO_0003853\n",
      "EFO_0003853 - Total distinct nodes N: 41; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 39, Using 10\n",
      "\n",
      "Processing disease: EFO_0000403\n",
      "EFO_0000403 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: EFO_0001645\n",
      "EFO_0001645 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_0003769\n",
      "EFO_0003769 - Total distinct nodes N: 84; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 82, Using 10\n",
      "\n",
      "Processing disease: EFO_0005708\n",
      "EFO_0005708 - Total distinct nodes N: 18; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 16, Using 10\n",
      "\n",
      "Processing disease: MONDO_0005148\n",
      "MONDO_0005148 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: HP_0001871\n",
      "HP_0001871 - Total distinct nodes N: 51; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 49, Using 10\n",
      "\n",
      "Processing disease: EFO_0010968\n",
      "EFO_0010968 - Total distinct nodes N: 26; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 24, Using 10\n",
      "\n",
      "Processing disease: EFO_0000756\n",
      "EFO_0000756 - Total distinct nodes N: 66; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 64, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021634\n",
      "MONDO_0021634 - Total distinct nodes N: 59; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 57, Using 10\n",
      "\n",
      "Processing disease: EFO_0004731\n",
      "EFO_0004731 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: EFO_0007984\n",
      "EFO_0007984 - Total distinct nodes N: 18; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 16, Using 10\n",
      "\n",
      "Processing disease: EFO_0000326\n",
      "EFO_0000326 - Total distinct nodes N: 59; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 57, Using 10\n",
      "\n",
      "Processing disease: EFO_0005950\n",
      "EFO_0005950 - Total distinct nodes N: 101; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 99, Using 10\n",
      "\n",
      "Processing disease: EFO_0002892\n",
      "EFO_0002892 - Total distinct nodes N: 14; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 12, Using 10\n",
      "\n",
      "Processing disease: EFO_0001642\n",
      "EFO_0001642 - Total distinct nodes N: 72; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 70, Using 10\n",
      "\n",
      "Processing disease: EFO_0003891\n",
      "EFO_0003891 - Total distinct nodes N: 20; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 18, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002532\n",
      "MONDO_0002532 - Total distinct nodes N: 54; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 52, Using 10\n",
      "\n",
      "Processing disease: EFO_0000318\n",
      "EFO_0000318 - Total distinct nodes N: 51; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 49, Using 10\n",
      "\n",
      "Processing disease: MONDO_0004979\n",
      "MONDO_0004979 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_0011011\n",
      "EFO_0011011 - Total distinct nodes N: 17; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 15, Using 10\n",
      "\n",
      "Processing disease: MONDO_0017341\n",
      "MONDO_0017341 - Total distinct nodes N: 36; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 34, Using 10\n",
      "\n",
      "Processing disease: EFO_0008550\n",
      "EFO_0008550 - Total distinct nodes N: 24; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 22, Using 10\n",
      "\n",
      "Processing disease: EFO_0000389\n",
      "EFO_0000389 - Total distinct nodes N: 38; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 36, Using 10\n",
      "\n",
      "Processing disease: EFO_1000218\n",
      "EFO_1000218 - Total distinct nodes N: 118; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 116, Using 10\n",
      "\n",
      "Processing disease: EFO_0005763\n",
      "EFO_0005763 - Total distinct nodes N: 16; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 14, Using 10\n",
      "\n",
      "Processing disease: HP_0011025\n",
      "HP_0011025 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping HP_0011025: too few nodes (2)\n",
      "\n",
      "Processing disease: MONDO_0017342\n",
      "MONDO_0017342 - Total distinct nodes N: 29; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 27, Using 10\n",
      "\n",
      "Processing disease: EFO_0005670\n",
      "EFO_0005670 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: MONDO_0003409\n",
      "MONDO_0003409 - Total distinct nodes N: 14; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 12, Using 10\n",
      "\n",
      "Processing disease: EFO_0004312\n",
      "EFO_0004312 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_1000158\n",
      "EFO_1000158 - Total distinct nodes N: 90; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 88, Using 10\n",
      "\n",
      "Processing disease: EFO_0004532\n",
      "EFO_0004532 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0004532: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0000707\n",
      "EFO_0000707 - Total distinct nodes N: 35; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 33, Using 10\n",
      "\n",
      "Processing disease: MONDO_0001627\n",
      "MONDO_0001627 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: MONDO_0002898\n",
      "MONDO_0002898 - Total distinct nodes N: 87; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 85, Using 10\n",
      "\n",
      "Processing disease: EFO_0000304\n",
      "EFO_0000304 - Total distinct nodes N: 23; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 21, Using 10\n",
      "\n",
      "Processing disease: EFO_0003767\n",
      "EFO_0003767 - Total distinct nodes N: 11; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 9, Using 9\n",
      "Attempt 1 failed: Cannot sample 9 negative nodes from a set of 8 negative nodes for R-HSA-168256\n",
      "Retrying with 7 negatives\n",
      "\n",
      "Processing disease: MONDO_0021117\n",
      "MONDO_0021117 - Total distinct nodes N: 33; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 31, Using 10\n",
      "\n",
      "Processing disease: EFO_1001938\n",
      "EFO_1001938 - Total distinct nodes N: 44; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 42, Using 10\n",
      "\n",
      "Processing disease: EFO_0002890\n",
      "EFO_0002890 - Total distinct nodes N: 14; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 12, Using 10\n",
      "\n",
      "Processing disease: EFO_0004289\n",
      "EFO_0004289 - Total distinct nodes N: 44; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 42, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002108\n",
      "MONDO_0002108 - Total distinct nodes N: 33; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 31, Using 10\n",
      "\n",
      "Processing disease: EFO_0002917\n",
      "EFO_0002917 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: HP_0000951\n",
      "HP_0000951 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping HP_0000951: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0002970\n",
      "EFO_0002970 - Total distinct nodes N: 31; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 29, Using 10\n",
      "\n",
      "Processing disease: EFO_0000348\n",
      "EFO_0000348 - Total distinct nodes N: 91; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 89, Using 10\n",
      "\n",
      "Processing disease: MONDO_0020665\n",
      "MONDO_0020665 - Total distinct nodes N: 43; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 41, Using 10\n",
      "\n",
      "Processing disease: MONDO_0005341\n",
      "MONDO_0005341 - Total distinct nodes N: 35; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 33, Using 10\n",
      "\n",
      "Processing disease: MONDO_0004805\n",
      "MONDO_0004805 - Total distinct nodes N: 12; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 10, Using 10\n",
      "Attempt 1 failed: Cannot sample 10 negative nodes from a set of 9 negative nodes for R-HSA-168256\n",
      "Retrying with 8 negatives\n",
      "\n",
      "Processing disease: MONDO_0000376\n",
      "MONDO_0000376 - Total distinct nodes N: 36; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 34, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021251\n",
      "MONDO_0021251 - Total distinct nodes N: 71; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 69, Using 10\n",
      "\n",
      "Processing disease: EFO_0005543\n",
      "EFO_0005543 - Total distinct nodes N: 36; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 34, Using 10\n",
      "\n",
      "Processing disease: EFO_0000272\n",
      "EFO_0000272 - Total distinct nodes N: 29; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 27, Using 10\n",
      "\n",
      "Processing disease: EFO_1000417\n",
      "EFO_1000417 - Total distinct nodes N: 19; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 17, Using 10\n",
      "\n",
      "Processing disease: EFO_0008528\n",
      "EFO_0008528 - Total distinct nodes N: 108; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 106, Using 10\n",
      "\n",
      "Processing disease: EFO_0004193\n",
      "EFO_0004193 - Total distinct nodes N: 16; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 14, Using 10\n",
      "\n",
      "Processing disease: MONDO_0011962\n",
      "MONDO_0011962 - Total distinct nodes N: 69; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 67, Using 10\n",
      "\n",
      "Processing disease: MONDO_0044937\n",
      "MONDO_0044937 - Total distinct nodes N: 96; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 94, Using 10\n",
      "\n",
      "Processing disease: EFO_0000673\n",
      "EFO_0000673 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0000673: too few nodes (2)\n",
      "\n",
      "Processing disease: MONDO_0004986\n",
      "MONDO_0004986 - Total distinct nodes N: 124; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 122, Using 10\n",
      "\n",
      "Processing disease: EFO_0000182\n",
      "EFO_0000182 - Total distinct nodes N: 45; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 43, Using 10\n",
      "\n",
      "Processing disease: MONDO_0003812\n",
      "MONDO_0003812 - Total distinct nodes N: 42; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 40, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002480\n",
      "MONDO_0002480 - Total distinct nodes N: 18; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 16, Using 10\n",
      "\n",
      "Processing disease: EFO_0000181\n",
      "EFO_0000181 - Total distinct nodes N: 63; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 61, Using 10\n",
      "\n",
      "Processing disease: HP_0002597\n",
      "HP_0002597 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: MONDO_0002120\n",
      "MONDO_0002120 - Total distinct nodes N: 13; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 11, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000621\n",
      "MONDO_0000621 - Total distinct nodes N: 65; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 63, Using 10\n",
      "\n",
      "Processing disease: EFO_1000646\n",
      "EFO_1000646 - Total distinct nodes N: 31; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 29, Using 10\n",
      "\n",
      "Processing disease: EFO_1001950\n",
      "EFO_1001950 - Total distinct nodes N: 15; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 13, Using 10\n",
      "\n",
      "Processing disease: EFO_0005232\n",
      "EFO_0005232 - Total distinct nodes N: 75; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 73, Using 10\n",
      "\n",
      "Processing disease: EFO_0002618\n",
      "EFO_0002618 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_0005134\n",
      "EFO_0005134 - Total distinct nodes N: 15; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 13, Using 10\n",
      "\n",
      "Processing disease: EFO_0009260\n",
      "EFO_0009260 - Total distinct nodes N: 22; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 20, Using 10\n",
      "\n",
      "Processing disease: MONDO_0018531\n",
      "MONDO_0018531 - Total distinct nodes N: 18; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 16, Using 10\n",
      "\n",
      "Processing disease: HP_0100543\n",
      "HP_0100543 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_1001463\n",
      "EFO_1001463 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_1000021\n",
      "EFO_1000021 - Total distinct nodes N: 83; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 81, Using 10\n",
      "\n",
      "Processing disease: EFO_1000416\n",
      "EFO_1000416 - Total distinct nodes N: 32; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 30, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002033\n",
      "MONDO_0002033 - Total distinct nodes N: 80; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 78, Using 10\n",
      "\n",
      "Processing disease: EFO_0006859\n",
      "EFO_0006859 - Total distinct nodes N: 95; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 93, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021096\n",
      "MONDO_0021096 - Total distinct nodes N: 29; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 27, Using 10\n",
      "\n",
      "Processing disease: EFO_0000178\n",
      "EFO_0000178 - Total distinct nodes N: 14; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 12, Using 10\n",
      "\n",
      "Processing disease: EFO_0003060\n",
      "EFO_0003060 - Total distinct nodes N: 13; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 11, Using 10\n",
      "\n",
      "Processing disease: EFO_0007861\n",
      "EFO_0007861 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: EFO_1001763\n",
      "EFO_1001763 - Total distinct nodes N: 15; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 13, Using 10\n",
      "\n",
      "Processing disease: HP_0011446\n",
      "HP_0011446 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: EFO_0005570\n",
      "EFO_0005570 - Total distinct nodes N: 98; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 96, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002038\n",
      "MONDO_0002038 - Total distinct nodes N: 78; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 76, Using 10\n",
      "\n",
      "Processing disease: MONDO_0037254\n",
      "MONDO_0037254 - Total distinct nodes N: 107; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 105, Using 10\n",
      "\n",
      "Processing disease: EFO_0000232\n",
      "EFO_0000232 - Total distinct nodes N: 45; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 43, Using 10\n",
      "\n",
      "Processing disease: MONDO_0015760\n",
      "MONDO_0015760 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: EFO_0009255\n",
      "EFO_0009255 - Total distinct nodes N: 44; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 42, Using 10\n",
      "\n",
      "Processing disease: EFO_0000275\n",
      "EFO_0000275 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: MONDO_0004251\n",
      "MONDO_0004251 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: MONDO_0021636\n",
      "MONDO_0021636 - Total distinct nodes N: 22; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 20, Using 10\n",
      "\n",
      "Processing disease: EFO_1000143\n",
      "EFO_1000143 - Total distinct nodes N: 78; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 76, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000629\n",
      "MONDO_0000629 - Total distinct nodes N: 20; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 18, Using 10\n",
      "\n",
      "Processing disease: EFO_0008591\n",
      "EFO_0008591 - Total distinct nodes N: 28; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 26, Using 10\n",
      "\n",
      "Processing disease: EFO_1000350\n",
      "EFO_1000350 - Total distinct nodes N: 51; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 49, Using 10\n",
      "\n",
      "Processing disease: EFO_0009387\n",
      "EFO_0009387 - Total distinct nodes N: 18; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 16, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021063\n",
      "MONDO_0021063 - Total distinct nodes N: 19; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 17, Using 10\n",
      "\n",
      "Processing disease: EFO_0004639\n",
      "EFO_0004639 - Total distinct nodes N: 23; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 21, Using 10\n",
      "\n",
      "Processing disease: EFO_1001902\n",
      "EFO_1001902 - Total distinct nodes N: 29; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 27, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000637\n",
      "MONDO_0000637 - Total distinct nodes N: 46; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 44, Using 10\n",
      "\n",
      "Processing disease: EFO_0006318\n",
      "EFO_0006318 - Total distinct nodes N: 55; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 53, Using 10\n",
      "\n",
      "Processing disease: HP_0012759\n",
      "HP_0012759 - Total distinct nodes N: 9; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 7, Using 7\n",
      "\n",
      "Processing disease: MONDO_0024479\n",
      "MONDO_0024479 - Total distinct nodes N: 11; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 9, Using 9\n",
      "Attempt 1 failed: Cannot sample 9 negative nodes from a set of 8 negative nodes for R-HSA-162582\n",
      "Retrying with 7 negatives\n",
      "\n",
      "Processing disease: EFO_0006545\n",
      "EFO_0006545 - Total distinct nodes N: 15; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 13, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021069\n",
      "MONDO_0021069 - Total distinct nodes N: 29; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 27, Using 10\n",
      "\n",
      "Processing disease: MONDO_0001056\n",
      "MONDO_0001056 - Total distinct nodes N: 31; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 29, Using 10\n",
      "\n",
      "Processing disease: EFO_0005540\n",
      "EFO_0005540 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: EFO_0001071\n",
      "EFO_0001071 - Total distinct nodes N: 26; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 24, Using 10\n",
      "\n",
      "Processing disease: EFO_0004570\n",
      "EFO_0004570 - Total distinct nodes N: 12; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 10, Using 10\n",
      "\n",
      "Processing disease: MONDO_0015756\n",
      "MONDO_0015756 - Total distinct nodes N: 16; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 14, Using 10\n",
      "\n",
      "Processing disease: EFO_0000246\n",
      "EFO_0000246 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_1000601\n",
      "EFO_1000601 - Total distinct nodes N: 101; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 99, Using 10\n",
      "\n",
      "Processing disease: EFO_0004343\n",
      "EFO_0004343 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: MONDO_0024476\n",
      "MONDO_0024476 - Total distinct nodes N: 93; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 91, Using 10\n",
      "\n",
      "Processing disease: EFO_1001949\n",
      "EFO_1001949 - Total distinct nodes N: 67; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 65, Using 10\n",
      "\n",
      "Processing disease: EFO_0010351\n",
      "EFO_0010351 - Total distinct nodes N: 27; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 25, Using 10\n",
      "\n",
      "Processing disease: EFO_0003897\n",
      "EFO_0003897 - Total distinct nodes N: 24; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 22, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021076\n",
      "MONDO_0021076 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: EFO_0005220\n",
      "EFO_0005220 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_0002571\n",
      "EFO_0002571 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_0004288\n",
      "EFO_0004288 - Total distinct nodes N: 24; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 22, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002691\n",
      "MONDO_0002691 - Total distinct nodes N: 22; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 20, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021355\n",
      "MONDO_0021355 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "\n",
      "Processing disease: EFO_1001901\n",
      "EFO_1001901 - Total distinct nodes N: 33; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 31, Using 10\n",
      "\n",
      "Processing disease: MONDO_0007576\n",
      "MONDO_0007576 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "\n",
      "Processing disease: EFO_0005592\n",
      "EFO_0005592 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: EFO_0000294\n",
      "EFO_0000294 - Total distinct nodes N: 119; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 117, Using 10\n",
      "\n",
      "Processing disease: EFO_0005631\n",
      "EFO_0005631 - Total distinct nodes N: 15; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 13, Using 10\n",
      "\n",
      "Processing disease: EFO_0003820\n",
      "EFO_0003820 - Total distinct nodes N: 25; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 23, Using 10\n",
      "\n",
      "Processing disease: EFO_0007990\n",
      "EFO_0007990 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: EFO_0009483\n",
      "EFO_0009483 - Total distinct nodes N: 49; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 47, Using 10\n",
      "\n",
      "Processing disease: EFO_0010176\n",
      "EFO_0010176 - Total distinct nodes N: 24; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 22, Using 10\n",
      "\n",
      "Processing disease: EFO_0008524\n",
      "EFO_0008524 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "\n",
      "Processing disease: MONDO_0037255\n",
      "MONDO_0037255 - Total distinct nodes N: 41; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 39, Using 10\n",
      "\n",
      "Processing disease: EFO_0007989\n",
      "EFO_0007989 - Total distinct nodes N: 19; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 17, Using 10\n",
      "\n",
      "Processing disease: EFO_0002428\n",
      "EFO_0002428 - Total distinct nodes N: 12; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 10, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000956\n",
      "MONDO_0000956 - Total distinct nodes N: 14; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 12, Using 10\n",
      "\n",
      "Processing disease: EFO_1001512\n",
      "EFO_1001512 - Total distinct nodes N: 45; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 43, Using 10\n",
      "\n",
      "Processing disease: MONDO_0017594\n",
      "MONDO_0017594 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: EFO_0009534\n",
      "EFO_0009534 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: MONDO_0005374\n",
      "MONDO_0005374 - Total distinct nodes N: 13; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 11, Using 10\n",
      "\n",
      "Processing disease: EFO_1000613\n",
      "EFO_1000613 - Total distinct nodes N: 75; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 73, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002616\n",
      "MONDO_0002616 - Total distinct nodes N: 24; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 22, Using 10\n",
      "\n",
      "Processing disease: EFO_0000222\n",
      "EFO_0000222 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: MONDO_0003059\n",
      "MONDO_0003059 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: EFO_0006460\n",
      "EFO_0006460 - Total distinct nodes N: 51; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 49, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021138\n",
      "MONDO_0021138 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "\n",
      "Processing disease: EFO_1000657\n",
      "EFO_1000657 - Total distinct nodes N: 88; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 86, Using 10\n",
      "\n",
      "Processing disease: MONDO_0044334\n",
      "MONDO_0044334 - Total distinct nodes N: 27; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 25, Using 10\n",
      "\n",
      "Processing disease: MONDO_0001014\n",
      "MONDO_0001014 - Total distinct nodes N: 12; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 10, Using 10\n",
      "\n",
      "Processing disease: EFO_0020946\n",
      "EFO_0020946 - Total distinct nodes N: 25; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 23, Using 10\n",
      "\n",
      "Processing disease: MONDO_0016680\n",
      "MONDO_0016680 - Total distinct nodes N: 16; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 14, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021043\n",
      "MONDO_0021043 - Total distinct nodes N: 33; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 31, Using 10\n",
      "\n",
      "Processing disease: EFO_0005922\n",
      "EFO_0005922 - Total distinct nodes N: 22; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 20, Using 10\n",
      "\n",
      "Processing disease: EFO_0004327\n",
      "EFO_0004327 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: MONDO_0002165\n",
      "MONDO_0002165 - Total distinct nodes N: 60; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 58, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000589\n",
      "MONDO_0000589 - Total distinct nodes N: 23; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 21, Using 10\n",
      "\n",
      "Processing disease: EFO_1000233\n",
      "EFO_1000233 - Total distinct nodes N: 87; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 85, Using 10\n",
      "\n",
      "Processing disease: EFO_0000519\n",
      "EFO_0000519 - Total distinct nodes N: 17; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 15, Using 10\n",
      "\n",
      "Processing disease: MONDO_0001187\n",
      "MONDO_0001187 - Total distinct nodes N: 112; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 110, Using 10\n",
      "\n",
      "Processing disease: MONDO_0100342\n",
      "MONDO_0100342 - Total distinct nodes N: 31; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 29, Using 10\n",
      "\n",
      "Processing disease: EFO_0003825\n",
      "EFO_0003825 - Total distinct nodes N: 87; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 85, Using 10\n",
      "\n",
      "Processing disease: EFO_0000095\n",
      "EFO_0000095 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0000095: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0003868\n",
      "EFO_0003868 - Total distinct nodes N: 114; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 112, Using 10\n",
      "\n",
      "Processing disease: MONDO_0037256\n",
      "MONDO_0037256 - Total distinct nodes N: 60; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 58, Using 10\n",
      "\n",
      "Processing disease: EFO_0009804\n",
      "EFO_0009804 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: MONDO_0002512\n",
      "MONDO_0002512 - Total distinct nodes N: 17; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 15, Using 10\n",
      "\n",
      "Processing disease: EFO_0000571\n",
      "EFO_0000571 - Total distinct nodes N: 21; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 19, Using 10\n",
      "\n",
      "Processing disease: EFO_0005588\n",
      "EFO_0005588 - Total distinct nodes N: 15; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 13, Using 10\n",
      "\n",
      "Processing disease: EFO_0000199\n",
      "EFO_0000199 - Total distinct nodes N: 60; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 58, Using 10\n",
      "\n",
      "Processing disease: EFO_1001951\n",
      "EFO_1001951 - Total distinct nodes N: 18; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 16, Using 10\n",
      "\n",
      "Processing disease: EFO_1000356\n",
      "EFO_1000356 - Total distinct nodes N: 45; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 43, Using 10\n",
      "\n",
      "Processing disease: MONDO_0005499\n",
      "MONDO_0005499 - Total distinct nodes N: 15; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 13, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000812\n",
      "MONDO_0000812 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: Orphanet_271847\n",
      "Orphanet_271847 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_1000017\n",
      "EFO_1000017 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: EFO_0000209\n",
      "EFO_0000209 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_0020943\n",
      "EFO_0020943 - Total distinct nodes N: 21; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 19, Using 10\n",
      "\n",
      "Processing disease: MONDO_0020638\n",
      "MONDO_0020638 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "\n",
      "Processing disease: EFO_0003860\n",
      "EFO_0003860 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: EFO_0000195\n",
      "EFO_0000195 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: MONDO_0003193\n",
      "MONDO_0003193 - Total distinct nodes N: 28; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 26, Using 10\n",
      "\n",
      "Processing disease: EFO_0001073\n",
      "EFO_0001073 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: EFO_0000640\n",
      "EFO_0000640 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_0006544\n",
      "EFO_0006544 - Total distinct nodes N: 75; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 73, Using 10\n",
      "\n",
      "Processing disease: EFO_0000546\n",
      "EFO_0000546 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: MONDO_0000636\n",
      "MONDO_0000636 - Total distinct nodes N: 20; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 18, Using 10\n",
      "\n",
      "Processing disease: MONDO_0021058\n",
      "MONDO_0021058 - Total distinct nodes N: 36; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 34, Using 10\n",
      "\n",
      "Processing disease: EFO_1000541\n",
      "EFO_1000541 - Total distinct nodes N: 20; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 18, Using 10\n",
      "\n",
      "Processing disease: MONDO_0007915\n",
      "MONDO_0007915 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: MONDO_0024757\n",
      "MONDO_0024757 - Total distinct nodes N: 21; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 19, Using 10\n",
      "\n",
      "Processing disease: HP_0002060\n",
      "HP_0002060 - Total distinct nodes N: 36; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 34, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002116\n",
      "MONDO_0002116 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_0007352\n",
      "EFO_0007352 - Total distinct nodes N: 86; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 84, Using 10\n",
      "\n",
      "Processing disease: HP_0030680\n",
      "HP_0030680 - Total distinct nodes N: 28; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 26, Using 10\n",
      "\n",
      "Processing disease: EFO_0004615\n",
      "EFO_0004615 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: MONDO_0000648\n",
      "MONDO_0000648 - Total distinct nodes N: 61; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 59, Using 10\n",
      "\n",
      "Processing disease: MONDO_0024337\n",
      "MONDO_0024337 - Total distinct nodes N: 106; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 104, Using 10\n",
      "\n",
      "Processing disease: MONDO_0004907\n",
      "MONDO_0004907 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: MONDO_0021631\n",
      "MONDO_0021631 - Total distinct nodes N: 15; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 13, Using 10\n",
      "\n",
      "Processing disease: EFO_0000200\n",
      "EFO_0000200 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0000200: too few nodes (2)\n",
      "\n",
      "Processing disease: MONDO_0000386\n",
      "MONDO_0000386 - Total distinct nodes N: 28; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 26, Using 10\n",
      "\n",
      "Processing disease: EFO_0000705\n",
      "EFO_0000705 - Total distinct nodes N: 35; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 33, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002928\n",
      "MONDO_0002928 - Total distinct nodes N: 71; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 69, Using 10\n",
      "\n",
      "Processing disease: EFO_0008515\n",
      "EFO_0008515 - Total distinct nodes N: 12; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 10, Using 10\n",
      "Attempt 1 failed: Cannot sample 10 negative nodes from a set of 8 negative nodes for R-HSA-162582\n",
      "Retrying with 8 negatives\n",
      "\n",
      "Processing disease: HP_0001939\n",
      "HP_0001939 - Total distinct nodes N: 37; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 35, Using 10\n",
      "\n",
      "Processing disease: HP_0000271\n",
      "HP_0000271 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: EFO_1000532\n",
      "EFO_1000532 - Total distinct nodes N: 49; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 47, Using 10\n",
      "\n",
      "Processing disease: EFO_1001956\n",
      "EFO_1001956 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_0021796\n",
      "EFO_0021796 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_0004705\n",
      "EFO_0004705 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: HP_0012647\n",
      "HP_0012647 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_0020944\n",
      "EFO_0020944 - Total distinct nodes N: 27; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 25, Using 10\n",
      "\n",
      "Processing disease: EFO_0000478\n",
      "EFO_0000478 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: MONDO_0044710\n",
      "MONDO_0044710 - Total distinct nodes N: 75; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 73, Using 10\n",
      "\n",
      "Processing disease: EFO_0003833\n",
      "EFO_0003833 - Total distinct nodes N: 57; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 55, Using 10\n",
      "\n",
      "Processing disease: MONDO_0024503\n",
      "MONDO_0024503 - Total distinct nodes N: 11; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 9, Using 9\n",
      "\n",
      "Processing disease: EFO_0000514\n",
      "EFO_0000514 - Total distinct nodes N: 21; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 19, Using 10\n",
      "\n",
      "Processing disease: MONDO_0024637\n",
      "MONDO_0024637 - Total distinct nodes N: 24; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 22, Using 10\n",
      "\n",
      "Processing disease: HP_0012649\n",
      "HP_0012649 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_0005423\n",
      "EFO_0005423 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0005423: too few nodes (2)\n",
      "\n",
      "Processing disease: MONDO_0021254\n",
      "MONDO_0021254 - Total distinct nodes N: 18; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 16, Using 10\n",
      "\n",
      "Processing disease: EFO_0003839\n",
      "EFO_0003839 - Total distinct nodes N: 13; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 11, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000383\n",
      "MONDO_0000383 - Total distinct nodes N: 22; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 20, Using 10\n",
      "\n",
      "Processing disease: MONDO_0005271\n",
      "MONDO_0005271 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: MONDO_0019472\n",
      "MONDO_0019472 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_0008589\n",
      "EFO_0008589 - Total distinct nodes N: 28; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 26, Using 10\n",
      "\n",
      "Processing disease: EFO_0005090\n",
      "EFO_0005090 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0005090: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_1000255\n",
      "EFO_1000255 - Total distinct nodes N: 15; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 13, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000591\n",
      "MONDO_0000591 - Total distinct nodes N: 57; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 55, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000833\n",
      "MONDO_0000833 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: MONDO_0000594\n",
      "MONDO_0000594 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping MONDO_0000594: too few nodes (2)\n",
      "\n",
      "Processing disease: HP_0002715\n",
      "HP_0002715 - Total distinct nodes N: 28; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 26, Using 10\n",
      "\n",
      "Processing disease: EFO_1000304\n",
      "EFO_1000304 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: MONDO_0044925\n",
      "MONDO_0044925 - Total distinct nodes N: 109; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 107, Using 10\n",
      "\n",
      "Processing disease: MONDO_0001657\n",
      "MONDO_0001657 - Total distinct nodes N: 15; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 13, Using 10\n",
      "\n",
      "Processing disease: MONDO_0004643\n",
      "MONDO_0004643 - Total distinct nodes N: 22; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 20, Using 10\n",
      "\n",
      "Processing disease: EFO_0005815\n",
      "EFO_0005815 - Total distinct nodes N: 47; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 45, Using 10\n",
      "\n",
      "Processing disease: EFO_0003841\n",
      "EFO_0003841 - Total distinct nodes N: 64; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 62, Using 10\n",
      "\n",
      "Processing disease: EFO_0000685\n",
      "EFO_0000685 - Total distinct nodes N: 14; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 12, Using 10\n",
      "\n",
      "Processing disease: EFO_0004280\n",
      "EFO_0004280 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: EFO_0001069\n",
      "EFO_0001069 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: MONDO_0000640\n",
      "MONDO_0000640 - Total distinct nodes N: 11; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 9, Using 9\n",
      "\n",
      "Processing disease: EFO_1001968\n",
      "EFO_1001968 - Total distinct nodes N: 25; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 23, Using 10\n",
      "\n",
      "Processing disease: HP_0001977\n",
      "HP_0001977 - Total distinct nodes N: 11; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 9, Using 9\n",
      "\n",
      "Processing disease: EFO_0002916\n",
      "EFO_0002916 - Total distinct nodes N: 46; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 44, Using 10\n",
      "\n",
      "Processing disease: EFO_0022196\n",
      "EFO_0022196 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: MONDO_0021375\n",
      "MONDO_0021375 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: MONDO_0024296\n",
      "MONDO_0024296 - Total distinct nodes N: 14; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 12, Using 10\n",
      "\n",
      "Processing disease: MONDO_0020663\n",
      "MONDO_0020663 - Total distinct nodes N: 26; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 24, Using 10\n",
      "\n",
      "Processing disease: EFO_1000044\n",
      "EFO_1000044 - Total distinct nodes N: 16; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 14, Using 10\n",
      "\n",
      "Processing disease: EFO_0004908\n",
      "EFO_0004908 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0004908: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0000706\n",
      "EFO_0000706 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: MONDO_0000588\n",
      "MONDO_0000588 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: HP_0001877\n",
      "HP_0001877 - Total distinct nodes N: 15; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 13, Using 10\n",
      "\n",
      "Processing disease: HP_0000598\n",
      "HP_0000598 - Total distinct nodes N: 4; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 2, Using 2\n",
      "\n",
      "Processing disease: EFO_1000359\n",
      "EFO_1000359 - Total distinct nodes N: 9; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 7, Using 7\n",
      "\n",
      "Processing disease: MONDO_0004670\n",
      "MONDO_0004670 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: MONDO_0005411\n",
      "MONDO_0005411 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_0004695\n",
      "EFO_0004695 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: MONDO_0040677\n",
      "MONDO_0040677 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: EFO_1000217\n",
      "EFO_1000217 - Total distinct nodes N: 24; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 22, Using 10\n",
      "\n",
      "Processing disease: EFO_0005775\n",
      "EFO_0005775 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: EFO_0004713\n",
      "EFO_0004713 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: EFO_0004631\n",
      "EFO_0004631 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0004631: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_1001455\n",
      "EFO_1001455 - Total distinct nodes N: 14; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 12, Using 10\n",
      "\n",
      "Processing disease: EFO_0004606\n",
      "EFO_0004606 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: EFO_0000538\n",
      "EFO_0000538 - Total distinct nodes N: 75; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 73, Using 10\n",
      "\n",
      "Processing disease: MONDO_0003061\n",
      "MONDO_0003061 - Total distinct nodes N: 9; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 7, Using 7\n",
      "\n",
      "Processing disease: EFO_0003851\n",
      "EFO_0003851 - Total distinct nodes N: 11; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 9, Using 9\n",
      "\n",
      "Processing disease: MONDO_0021632\n",
      "MONDO_0021632 - Total distinct nodes N: 12; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 10, Using 10\n",
      "Attempt 1 failed: Cannot sample 10 negative nodes from a set of 9 negative nodes for R-HSA-162582\n",
      "Retrying with 8 negatives\n",
      "\n",
      "Processing disease: EFO_0020945\n",
      "EFO_0020945 - Total distinct nodes N: 21; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 19, Using 10\n",
      "\n",
      "Processing disease: EFO_0001378\n",
      "EFO_0001378 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0001378: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0004729\n",
      "EFO_0004729 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0004729: too few nodes (2)\n",
      "\n",
      "Processing disease: MONDO_0023644\n",
      "MONDO_0023644 - Total distinct nodes N: 95; Chosen d: 7\n",
      "Negative sampling: Requested 10, Safe maximum 93, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000726\n",
      "MONDO_0000726 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping MONDO_0000726: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0000474\n",
      "EFO_0000474 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_1000307\n",
      "EFO_1000307 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: EFO_0000384\n",
      "EFO_0000384 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_1002050\n",
      "EFO_1002050 - Total distinct nodes N: 20; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 18, Using 10\n",
      "\n",
      "Processing disease: EFO_0003872\n",
      "EFO_0003872 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: HP_0001249\n",
      "HP_0001249 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping HP_0001249: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0000676\n",
      "EFO_0000676 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_1000223\n",
      "EFO_1000223 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_1000223: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0006812\n",
      "EFO_0006812 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: MONDO_0021080\n",
      "MONDO_0021080 - Total distinct nodes N: 30; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 28, Using 10\n",
      "\n",
      "Processing disease: EFO_0005110\n",
      "EFO_0005110 - Total distinct nodes N: 23; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 21, Using 10\n",
      "\n",
      "Processing disease: EFO_0004273\n",
      "EFO_0004273 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0004273: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_1000636\n",
      "EFO_1000636 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: MONDO_0002917\n",
      "MONDO_0002917 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping MONDO_0002917: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0003888\n",
      "EFO_0003888 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: MONDO_0001572\n",
      "MONDO_0001572 - Total distinct nodes N: 12; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 10, Using 10\n",
      "Attempt 1 failed: Cannot sample 10 negative nodes from a set of 9 negative nodes for R-HSA-162582\n",
      "Retrying with 8 negatives\n",
      "\n",
      "Processing disease: EFO_0003100\n",
      "EFO_0003100 - Total distinct nodes N: 12; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 10, Using 10\n",
      "\n",
      "Processing disease: EFO_1000121\n",
      "EFO_1000121 - Total distinct nodes N: 12; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 10, Using 10\n",
      "Attempt 1 failed: Cannot sample 10 negative nodes from a set of 9 negative nodes for R-HSA-162582\n",
      "Retrying with 8 negatives\n",
      "\n",
      "Processing disease: MONDO_0002232\n",
      "MONDO_0002232 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_0000662\n",
      "EFO_0000662 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0000662: too few nodes (2)\n",
      "\n",
      "Processing disease: HP_0025142\n",
      "HP_0025142 - Total distinct nodes N: 10; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 8, Using 8\n",
      "\n",
      "Processing disease: EFO_0000729\n",
      "EFO_0000729 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: EFO_0003756\n",
      "EFO_0003756 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0003756: too few nodes (2)\n",
      "\n",
      "Processing disease: HP_0040064\n",
      "HP_0040064 - Total distinct nodes N: 26; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 24, Using 10\n",
      "\n",
      "Processing disease: MONDO_0000569\n",
      "MONDO_0000569 - Total distinct nodes N: 11; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 9, Using 9\n",
      "\n",
      "Processing disease: MONDO_0004126\n",
      "MONDO_0004126 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: EFO_0005784\n",
      "EFO_0005784 - Total distinct nodes N: 7; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 5, Using 5\n",
      "\n",
      "Processing disease: MONDO_0004975\n",
      "MONDO_0004975 - Total distinct nodes N: 26; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 24, Using 10\n",
      "\n",
      "Processing disease: GO_0036273\n",
      "GO_0036273 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping GO_0036273: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0000691\n",
      "EFO_0000691 - Total distinct nodes N: 12; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 10, Using 10\n",
      "Attempt 1 failed: Cannot sample 10 negative nodes from a set of 9 negative nodes for R-HSA-162582\n",
      "Retrying with 8 negatives\n",
      "\n",
      "Processing disease: MONDO_0021335\n",
      "MONDO_0021335 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: HP_0004936\n",
      "HP_0004936 - Total distinct nodes N: 12; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 10, Using 10\n",
      "\n",
      "Processing disease: EFO_0006500\n",
      "EFO_0006500 - Total distinct nodes N: 3; Chosen d: 2\n",
      "Negative sampling: Requested 10, Safe maximum 1, Using 1\n",
      "\n",
      "Processing disease: MONDO_0002280\n",
      "MONDO_0002280 - Total distinct nodes N: 63; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 61, Using 10\n",
      "\n",
      "Processing disease: EFO_0000365\n",
      "EFO_0000365 - Total distinct nodes N: 13; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 11, Using 10\n",
      "\n",
      "Processing disease: EFO_1000262\n",
      "EFO_1000262 - Total distinct nodes N: 31; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 29, Using 10\n",
      "\n",
      "Processing disease: EFO_0010118\n",
      "EFO_0010118 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_0007005\n",
      "EFO_0007005 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0007005: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0005541\n",
      "EFO_0005541 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: EFO_0003931\n",
      "EFO_0003931 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0003931: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0020092\n",
      "EFO_0020092 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_0020092: too few nodes (2)\n",
      "\n",
      "Processing disease: MONDO_0005301\n",
      "MONDO_0005301 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping MONDO_0005301: too few nodes (2)\n",
      "\n",
      "Processing disease: MONDO_0002525\n",
      "MONDO_0002525 - Total distinct nodes N: 30; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 28, Using 10\n",
      "\n",
      "Processing disease: EFO_0008595\n",
      "EFO_0008595 - Total distinct nodes N: 15; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 13, Using 10\n",
      "\n",
      "Processing disease: MONDO_0004580\n",
      "MONDO_0004580 - Total distinct nodes N: 13; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 11, Using 10\n",
      "\n",
      "Processing disease: EFO_0005653\n",
      "EFO_0005653 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_0005680\n",
      "EFO_0005680 - Total distinct nodes N: 13; Chosen d: 4\n",
      "Negative sampling: Requested 10, Safe maximum 11, Using 10\n",
      "\n",
      "Processing disease: EFO_0009608\n",
      "EFO_0009608 - Total distinct nodes N: 49; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 47, Using 10\n",
      "\n",
      "Processing disease: EFO_0004622\n",
      "EFO_0004622 - Total distinct nodes N: 6; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 4, Using 4\n",
      "\n",
      "Processing disease: EFO_1000870\n",
      "EFO_1000870 - Total distinct nodes N: 2; Chosen d: 2\n",
      "Skipping EFO_1000870: too few nodes (2)\n",
      "\n",
      "Processing disease: EFO_0004145\n",
      "EFO_0004145 - Total distinct nodes N: 37; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 35, Using 10\n",
      "\n",
      "Processing disease: HP_0003124\n",
      "HP_0003124 - Total distinct nodes N: 22; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 20, Using 10\n",
      "\n",
      "Processing disease: EFO_0010226\n",
      "EFO_0010226 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: EFO_0020947\n",
      "EFO_0020947 - Total distinct nodes N: 37; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 35, Using 10\n",
      "\n",
      "Processing disease: MONDO_0002562\n",
      "MONDO_0002562 - Total distinct nodes N: 8; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 6, Using 6\n",
      "\n",
      "Processing disease: HP_0100790\n",
      "HP_0100790 - Total distinct nodes N: 5; Chosen d: 3\n",
      "Negative sampling: Requested 10, Safe maximum 3, Using 3\n",
      "\n",
      "Processing disease: HP_0003119\n",
      "HP_0003119 - Total distinct nodes N: 18; Chosen d: 5\n",
      "Negative sampling: Requested 10, Safe maximum 16, Using 10\n",
      "\n",
      "Processing disease: EFO_1000020\n",
      "EFO_1000020 - Total distinct nodes N: 64; Chosen d: 6\n",
      "Negative sampling: Requested 10, Safe maximum 62, Using 10\n"
     ]
    }
   ],
   "source": [
    "pem_level2 = process_hierarchy_diseases(\"/Users/polina/Pathwaganda/data/GSEA-output_filt_hier\", \n",
    "                                        max_hierLevel=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d72f567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pem_level2.write.mode(\"overwrite\").parquet(\"/Users/polina/Pathwaganda/data/pem_levels/2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "389bac66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+-------------------+--------------------+-------------------+--------------------+--------------------+-----------+-----+-----+\n",
      "|           ID|               dim_0|              dim_1|               dim_2|              dim_3|               dim_4|               dim_5|  diseaseId|dim_6|dim_7|\n",
      "+-------------+--------------------+-------------------+--------------------+-------------------+--------------------+--------------------+-----------+-----+-----+\n",
      "|R-HSA-1474290|  0.2979782567242389| 0.3779123131695229| 0.07479504883208896|0.06703926448642958|-0.04872236623855061| 0.48691106425357245|EFO_0000701|  NaN|  NaN|\n",
      "|R-HSA-2022090| 0.27989850816200074| 0.3564410743901681|  0.0708735905468227|0.06323375140911779|-0.04511225001842...| 0.46039601461244206|EFO_0000701|  NaN|  NaN|\n",
      "|R-HSA-6805567|-0.47368833256761567| 0.1543839617924824| 0.08013296850391213|-0.5596202237298129| 0.23809138300842622| 0.12701567595722524|EFO_0000701|  NaN|  NaN|\n",
      "|R-HSA-6809371|-0.47147090471594566| 0.1530549992103967| 0.08000756500350947|  -0.55987910497158|  0.2382879357980533| 0.12711790473271367|EFO_0000701|  NaN|  NaN|\n",
      "| R-HSA-162582| -0.3236164121280612|-0.3613311420936283|-0.25080522048988146|0.32539577084412097|-0.34127941723740385|-0.03973103116517618|EFO_0000701|  NaN|  NaN|\n",
      "+-------------+--------------------+-------------------+--------------------+-------------------+--------------------+--------------------+-----------+-----+-----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "pem_level2.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac8dd0a",
   "metadata": {},
   "source": [
    "## Weighted (Jaccard similarity index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904abf42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a8591d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1489b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e4850f9",
   "metadata": {},
   "source": [
    "# Target embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a5ba10",
   "metadata": {},
   "source": [
    "## Hierarchical (Poincare coordinates)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c829552",
   "metadata": {},
   "source": [
    "Based on tmp and pem create target coordinates (tem) in hyperbolic space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ece1b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode, split, avg, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b42478c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def create_target_embeddings(pathway_file: str, target2pathway_file: str):\n",
    "    \"\"\"\n",
    "    Generate target embeddings by averaging pathway embeddings per disease,\n",
    "    preserving original pathwayIds column.\n",
    "    \"\"\"\n",
    "\n",
    "    # Read pathway embeddings\n",
    "    pathway_df = spark.read.parquet(pathway_file).alias(\"pem\")\n",
    "    embedding_columns = [c for c in pathway_df.columns if c.startswith(\"dim_\")]\n",
    "\n",
    "    # Read target-to-pathway mapping\n",
    "    target_df = spark.read.parquet(target2pathway_file).alias(\"tpm\")\n",
    "\n",
    "    # Count missing diseases\n",
    "    missing_disease_ids = (\n",
    "        target_df.select(F.col(\"tpm.diseaseId\").alias(\"diseaseId\")).distinct()\n",
    "        .join(pathway_df.select(F.col(\"pem.diseaseId\").alias(\"diseaseId\")).distinct(), \n",
    "              on=\"diseaseId\", how=\"left_anti\")\n",
    "    )\n",
    "    print(f\"Number of diseases in TPM not found in PEM: {missing_disease_ids.count()}\")\n",
    "\n",
    "    # Explode pathwayIds, but also keep original pathwayIds\n",
    "    exploded_target_df = (\n",
    "        target_df\n",
    "        .withColumn(\"pathwayId\", F.explode(F.split(F.col(\"tpm.pathwayIds\"), \",\")))\n",
    "        .select(\"tpm.targetId\", \"tpm.approvedSymbol\", \"tpm.diseaseId\", \"tpm.pathwayIds\", \"pathwayId\")\n",
    "    ).alias(\"tpm\")\n",
    "\n",
    "    # Join with pathway embeddings\n",
    "    joined_df = (\n",
    "        exploded_target_df\n",
    "        .join(\n",
    "            pathway_df.alias(\"pem\"),\n",
    "            (F.col(\"tpm.pathwayId\") == F.col(\"pem.ID\")) &\n",
    "            (F.col(\"tpm.diseaseId\") == F.col(\"pem.diseaseId\")),\n",
    "            \"inner\"\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Average embeddings per target + disease, while keeping pathwayIds\n",
    "    averaged_df = (\n",
    "        joined_df\n",
    "        .groupBy(\"tpm.targetId\", \"tpm.approvedSymbol\", \"tpm.diseaseId\", \"tpm.pathwayIds\")\n",
    "        .agg(*[F.avg(F.col(f\"pem.{c}\")).alias(c) for c in embedding_columns])\n",
    "    )\n",
    "\n",
    "    return averaged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35e1165a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of diseases in TPM not found in PEM: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/21 15:43:53 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tem_level0 = create_target_embeddings(\n",
    "    \"/Users/polina/Pathwaganda/data/pem_levels/0\",\n",
    "    \"/Users/polina/Pathwaganda/data/tpm_levels/0\")\n",
    "\n",
    "tem_level0.write.mode(\"overwrite\").parquet(\"/Users/polina/Pathwaganda/data/tem_levels/0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e6889ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+-----------+--------------------+-------------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-----+\n",
      "|       targetId|approvedSymbol|  diseaseId|          pathwayIds|              dim_0|               dim_1|               dim_2|              dim_3|               dim_4|               dim_5|               dim_6|               dim_7|              dim_8|dim_9|\n",
      "+---------------+--------------+-----------+--------------------+-------------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-----+\n",
      "|ENSG00000000419|          DPM1|EFO_0000199|       R-HSA-1643685|0.03693195721547555|  -0.594752644697465|-0.15494661392907222|-0.4537320177105202| -0.3208464922484456|-0.42881094175268264|-0.16167811177650787|-0.22846329004855406|                NaN|  NaN|\n",
      "|ENSG00000000419|          DPM1|EFO_0000305|R-HSA-1643685,R-H...|0.09376648695321332| 0.08419231555058626| 0.13680887734149796|0.30813678828815294| 0.20100037893116812| 0.41842160173450604|-0.01416124990564395|                 NaN|                NaN|  NaN|\n",
      "|ENSG00000000419|          DPM1|EFO_0000618|R-HSA-392499,R-HS...|-0.3471677626967725|-0.22620842690797532| -0.0763026728511867| 0.2303806464920953|  0.1340544776926456|  0.2348798288359827| 0.33651105131341097|-0.07236899459580151|                NaN|  NaN|\n",
      "|ENSG00000000419|          DPM1|EFO_0000756|R-HSA-1643685,R-H...|0.13916466528058802| -0.3487285346238256|-0.21737663511470884|0.05077349011772175|-0.23887119227359255|  0.3420258285706799|-0.24347296078404895| 0.08723141213661892|                NaN|  NaN|\n",
      "|ENSG00000000419|          DPM1|EFO_0002422|R-HSA-597592,R-HS...| 0.4123221854111378|0.033515351383050784|-0.02678397259214299|0.03506686732042023|-0.30167112689769043|  0.0946721868271161|-0.13111091513713133| -0.3990981357806313|-0.1423543515409811|  NaN|\n",
      "+---------------+--------------+-----------+--------------------+-------------------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+-------------------+-----+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"/Users/polina/Pathwaganda/data/tem_levels/0\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5c2a03a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of diseases in TPM not found in PEM: 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/21 15:44:40 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "25/08/21 15:44:40 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tem_level1 = create_target_embeddings(\n",
    "    \"/Users/polina/Pathwaganda/data/pem_levels/1\",\n",
    "    \"/Users/polina/Pathwaganda/d\" \\\n",
    "    \"ata/tpm_levels/1\")\n",
    "\n",
    "tem_level1.write.mode(\"overwrite\").parquet(\"/Users/polina/Pathwaganda/data/tem_levels/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fdac6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of diseases in TPM not found in PEM: 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/21 15:44:47 WARN MemoryManager: Total allocation exceeds 95.00% (1,020,054,720 bytes) of heap memory\n",
      "Scaling row group sizes to 95.00% for 8 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tem_level2 = create_target_embeddings(\n",
    "    \"/Users/polina/Pathwaganda/data/pem_levels/2\",\n",
    "    \"/Users/polina/Pathwaganda/data/tpm_levels/2\")\n",
    "\n",
    "tem_level2.write.mode(\"overwrite\").parquet(\"/Users/polina/Pathwaganda/data/tem_levels/2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7c2868c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+-----------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|       targetId|approvedSymbol|  diseaseId|               dim_0|               dim_1|              dim_2|               dim_3|               dim_4|               dim_5|               dim_6|               dim_7|\n",
      "+---------------+--------------+-----------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|ENSG00000058404|        CAMK2B|EFO_0000313| 0.10057969602937376| 0.11319281429042716|0.09188663680975077| 0.15102883960552846|-0.15157995953640524| 0.05882703634601264| 0.08532016216936399|  0.1922430112087569|\n",
      "|ENSG00000111790|      FGFR1OP2|EFO_0000313|-4.95571735941970...|-0.03676460002274...|0.27457072469634203|   0.239685775645362| -0.3008503539177753| 0.16956199238730413| -0.5413204363683383| 0.14380154325941982|\n",
      "|ENSG00000119917|         IFIT3|EFO_0000313|  0.3006508183339845|  0.3655358708894412|0.24464591005088024| 0.11318346846139765|  0.1959611907723376| 0.08782287262746859| 0.24909371924493218|-0.43736608758082196|\n",
      "|ENSG00000204704|         OR2W1|EFO_0000313|-0.04305827764821...|  0.3908769735771731|-0.3287062763939994|-0.30884756621785675|-0.33433536051550194|  0.2493825506471467|-0.04099512840640341|0.011720783619391257|\n",
      "|ENSG00000111615|          KRR1|EFO_0000319| 0.33174393299414845| -0.2543722423527775|0.06623305728923877| -0.3134396102630644|-0.13613443505701414|-0.02490463628970...| 0.49513766514880325|-0.30626397414816137|\n",
      "+---------------+--------------+-----------+--------------------+--------------------+-------------------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tem_level0.filter(col(\"dim_7\") != \"NaN\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdc1d04",
   "metadata": {},
   "source": [
    "Ok now everything ready for score propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fafd8c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca37be6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27b3429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ef8f5011",
   "metadata": {},
   "source": [
    "## Prepare target-based metadata files with info about targets per disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f53cb33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/08/05 12:05:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f91bb3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|  approvedSymbol|                  ID|\n",
      "+----------------+--------------------+\n",
      "| complete genome|R-HSA-1643685,R-H...|\n",
      "|        18S rRNA|       R-HSA-1643685|\n",
      "|              1B|R-HSA-1643685,R-H...|\n",
      "|              1C|R-HSA-1643685,R-H...|\n",
      "|              1a|R-HSA-1643685,R-H...|\n",
      "+----------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"/Users/polina/Pathwaganda/data/target-pathway_matrix_opt/test/diseaseId=EFO_0000094\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd5bf54",
   "metadata": {},
   "source": [
    "Lets use target-pathway_matrix_opt folder to start with and parse target info from OT files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a55b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/08/05 12:08:07 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+--------------+--------------------+--------------------+--------------------+--------------------+----------------+--------------------+--------------------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----+--------------------+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "|             id|approvedSymbol|       biotype|       transcriptIds| canonicalTranscript|      canonicalExons|     genomicLocation|alternativeGenes|        approvedName|                  go|hallmarks|            synonyms|      symbolSynonyms|        nameSynonyms|functionDescriptions|subcellularLocations|         targetClass|     obsoleteSymbols|       obsoleteNames|          constraint| tep|          proteinIds|             dbXrefs|chemicalProbes|          homologues|        tractability|   safetyLiabilities|            pathways|      tss|\n",
      "+---------------+--------------+--------------+--------------------+--------------------+--------------------+--------------------+----------------+--------------------+--------------------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----+--------------------+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "|ENSG00000000003|        TSPAN6|protein_coding|[ENST00000494424,...|{ENST00000373020,...|[100632485, 10063...|{X, 100627108, 10...|            NULL|       tetraspanin 6|[{GO:0043123, PMI...|     NULL|[{Tetraspanin-6, ...|[{TSPAN6, uniprot...|[{Tetraspanin-6, ...|                  []|[{Membrane, unipr...|                NULL|    [{TM4SF6, HGNC}]|[{transmembrane 4...|[{syn, -0.47468, ...|NULL|[{O43657, uniprot...|[{11858, HGNC}, {...|          NULL|[{9606, Human, ot...|[{SM, Approved Dr...|                NULL|                NULL|100636806|\n",
      "|ENSG00000000005|          TNMD|protein_coding|[ENST00000485971,...|{ENST00000373031,...|[100585231, 10058...|{X, 100584936, 10...|            NULL|         tenomodulin|[{GO:0005515, PMI...|     NULL|[{Tenomodulin, un...|[{TNMD, uniprot},...|[{Tenomodulin, un...|[May be an angiog...|[{[Isoform 1]: Me...|                NULL|                  []|                  []|[{syn, -0.46371, ...|NULL|[{Q9H2S6, uniprot...|[{17757, HGNC}, {...|          NULL|[{9606, Human, wi...|[{SM, Approved Dr...|                NULL|                NULL|100584936|\n",
      "|ENSG00000001084|          GCLC|protein_coding|[ENST00000504525,...|{ENST00000650454,...|[53520778, 535209...|{6, 53497341, 536...|            NULL|glutamate-cystein...|[{GO:0005829, GO_...|     NULL|[{Glutamate--cyst...|[{GCLC, uniprot},...|[{Glutamate--cyst...|[Catalyzes the AT...|[{Nucleoplasm, HP...|[{645, Enzyme, l1...|[{GLCLC, HGNC}, {...|[{glutamate-cyste...|[{syn, 0.58706, 1...|NULL|[{P48506, uniprot...|[{4311, HGNC}, {C...|          NULL|[{9598, Chimpanze...|[{SM, Approved Dr...|[{regulation of t...|[{R-HSA-174403, G...| 53545101|\n",
      "|ENSG00000003137|       CYP26B1|protein_coding|[ENST00000474509,...|{ENST00000001146,...|[72147631, 721478...|{2, 72129238, 721...|            NULL|cytochrome P450 f...|[{GO:0005783, GO_...|     NULL|[{Cytochrome P450...|[{CYP26B1, unipro...|[{Cytochrome P450...|[A cytochrome P45...|[{Endoplasmic ret...|[{1075, Cytochrom...|                  []|[{cytochrome P450...|[{syn, -0.66253, ...|NULL|[{Q9NR63, uniprot...|[{20581, HGNC}, {...|          NULL|[{9606, Human, wi...|[{SM, Approved Dr...|                NULL|[{R-HSA-5579015, ...| 72147862|\n",
      "|ENSG00000004059|          ARF5|protein_coding|[ENST00000000233,...|{ENST00000000233,...|[127591213, 12759...|{7, 127588386, 12...|            NULL|        ARF GTPase 5|[{GO:0016192, GO_...|     NULL|[{ADP-ribosylatio...|[{ARF5, uniprot},...|[{ADP-ribosylatio...|[GTP-binding prot...|[{Golgi apparatus...|[{601, Unclassifi...|                  []|[{ADP ribosylatio...|[{syn, 0.6457, 42...|NULL|[{P84085, uniprot...|[{658, HGNC}, {2B...|          NULL|[{9606, Human, ot...|[{SM, Approved Dr...|                NULL|[{R-HSA-6807878, ...|127588411|\n",
      "+---------------+--------------+--------------+--------------------+--------------------+--------------------+--------------------+----------------+--------------------+--------------------+---------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----+--------------------+--------------------+--------------+--------------------+--------------------+--------------------+--------------------+---------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Take targetId from:\n",
    "\n",
    "spark.read.parquet(\"/Users/polina/Pathwaganda/data/target\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "77988951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------------+\n",
      "|       0|                  1|\n",
      "+--------+-------------------+\n",
      "|    CUX1| 0.4559480982087158|\n",
      "|  NPIPB8|0.14562438358841173|\n",
      "|    ETV5| 0.3039653988058105|\n",
      "|  NUTM2D| 0.3039653988058105|\n",
      "|DCAF12L2| 0.3039653988058105|\n",
      "+--------+-------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Take genetic evidence scores from gsea_4_inout files:\n",
    "\n",
    "spark.read.parquet(\"/Users/polina/Pathwaganda/data/input_4_gsea/diseaseId=EFO_0000094\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bafaff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+----------+-----+----------+--------------------+--------------------+---------------+--------------+--------------------+-------------+----------------+--------------------+--------------------+--------------+--------------------+--------------------+\n",
      "|     drugId|       targetId| diseaseId|phase|    status|                urls|           ancestors|          label|approvedSymbol|        approvedName|  targetClass|        prefName|          tradeNames|            synonyms|      drugType|   mechanismOfAction|          targetName|\n",
      "+-----------+---------------+----------+-----+----------+--------------------+--------------------+---------------+--------------+--------------------+-------------+----------------+--------------------+--------------------+--------------+--------------------+--------------------+\n",
      "|CHEMBL52440|ENSG00000183454|DOID_10113|  1.0| Completed|[{ClinicalTrials,...|[MONDO_0002428, E...|trypanosomiasis|        GRIN2A|glutamate ionotro...|[Ion channel]|DEXTROMETHORPHAN|                  []|[Dextromethorphan...|Small molecule|Glutamate [NMDA] ...|Glutamate [NMDA] ...|\n",
      "|  CHEMBL655|ENSG00000022355|DOID_10113|  1.0| Completed|[{ClinicalTrials,...|[MONDO_0002428, E...|trypanosomiasis|        GABRA1|gamma-aminobutyri...|[Ion channel]|       MIDAZOLAM|[Midazolam in 0.8...|[Midazolam, Midaz...|Small molecule|GABA-A receptor; ...|GABA-A receptor; ...|\n",
      "|  CHEMBL655|ENSG00000151834|DOID_10113|  1.0| Completed|[{ClinicalTrials,...|[MONDO_0002428, E...|trypanosomiasis|        GABRA2|gamma-aminobutyri...|[Ion channel]|       MIDAZOLAM|[Midazolam in 0.8...|[Midazolam, Midaz...|Small molecule|GABA-A receptor; ...|GABA-A receptor; ...|\n",
      "|  CHEMBL655|ENSG00000186297|DOID_10113|  1.0| Completed|[{ClinicalTrials,...|[MONDO_0002428, E...|trypanosomiasis|        GABRA5|gamma-aminobutyri...|[Ion channel]|       MIDAZOLAM|[Midazolam in 0.8...|[Midazolam, Midaz...|Small molecule|GABA-A receptor; ...|GABA-A receptor; ...|\n",
      "|  CHEMBL830|ENSG00000115758|DOID_10113|  2.0|Terminated|[{ClinicalTrials,...|[MONDO_0002428, E...|trypanosomiasis|          ODC1|ornithine decarbo...|     [Enzyme]|    EFLORNITHINE|                  []|[Eflornithine, Ef...|Small molecule|Ornithine decarbo...|Ornithine decarbo...|\n",
      "+-----------+---------------+----------+-----+----------+--------------------+--------------------+---------------+--------------+--------------------+-------------+----------------+--------------------+--------------------+--------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "# Take drug info from:\n",
    "\n",
    "spark.read.parquet(\"/Users/polina/Pathwaganda/data/known_drug\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87424e95",
   "metadata": {},
   "source": [
    "### Merge with known_drug from ChEMBL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e02aedad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, collect_list, concat_ws, max as spark_max\n",
    "\n",
    "def merge_files_with_target_drug_info(input_dir: str, \n",
    "                                      target_parquet: str, \n",
    "                                    #   association_parquet: str, \n",
    "                                      known_drug_parquet: str, \n",
    "                                      output_dir: str):\n",
    "    # Start Spark session\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    # Read shared dataframes\n",
    "    target_df = spark.read.parquet(target_parquet).select(\"approvedSymbol\", \"id\").distinct()\n",
    "    known_drug_df = spark.read.parquet(known_drug_parquet).select(\"phase\", \"targetId\", \"diseaseId\")\n",
    "\n",
    "    # Prepare target mapping: approvedSymbol -> comma-separated list of ids\n",
    "    target_agg_df = (\n",
    "        target_df\n",
    "        .groupBy(\"approvedSymbol\")\n",
    "        .agg(concat_ws(\",\", collect_list(\"id\")).alias(\"targetId\"))\n",
    "    )\n",
    "\n",
    "    # Iterate over folders in input_dir\n",
    "    for folder_name in os.listdir(input_dir):\n",
    "        folder_path = os.path.join(input_dir, folder_name)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "\n",
    "        # Expect folder name of format diseaseId=XXX\n",
    "        match = re.match(r\"diseaseId=(.+)\", folder_name)\n",
    "        if not match:\n",
    "            continue\n",
    "        disease_id = match.group(1)\n",
    "\n",
    "        # Read initial file (assuming single parquet in folder)\n",
    "        initial_file_path = os.path.join(folder_path)\n",
    "        initial_df = spark.read.parquet(initial_file_path)\n",
    "\n",
    "        # Join with target mapping\n",
    "        initial_with_target = (\n",
    "            initial_df\n",
    "            .join(target_agg_df, on=\"approvedSymbol\", how=\"left\")\n",
    "        )\n",
    "\n",
    "        # Filter known drug data for the current diseaseId\n",
    "        filtered_known_drug = known_drug_df.filter(col(\"diseaseId\") == disease_id)\n",
    "\n",
    "        # Join with known_drug to get phase\n",
    "        final_df = (\n",
    "            initial_with_target\n",
    "            .join(filtered_known_drug, on=\"targetId\", how=\"left\")\n",
    "        )\n",
    "\n",
    "        # Aggregate max phase for each row\n",
    "        result_df = (\n",
    "            final_df\n",
    "            .groupBy(*initial_df.columns, \"targetId\")\n",
    "            .agg(spark_max(\"phase\").alias(\"maxPhaseChEMBL\"))\n",
    "        )\n",
    "\n",
    "        # Save the result as a parquet file to output_dir with same folder name\n",
    "        output_path = os.path.join(output_dir, folder_name)\n",
    "        result_df.write.mode(\"overwrite\").parquet(output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e5bf955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "merge_files_with_target_drug_info(\n",
    "    input_dir=\"/Users/polina/Pathwaganda/data/target-pathway_matrix_opt/Reactome_Pathways_2025_diy\",\n",
    "    target_parquet=\"/Users/polina/Pathwaganda/data/target\",\n",
    "    # association_path=\"/path/to/association.parquet\",\n",
    "    known_drug_parquet=\"/Users/polina/Pathwaganda/data/known_drug\",\n",
    "    output_dir=\"/Users/polina/Pathwaganda/data/target_metadata/known_drug_merge/Reactome_Pathways_2025_diy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92eec7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+--------------------+---------------+--------------+\n",
      "|approvedSymbol|                  ID|       targetId|maxPhaseChEMBL|\n",
      "+--------------+--------------------+---------------+--------------+\n",
      "|        ANGPT1|        R-HSA-109582|ENSG00000154188|          NULL|\n",
      "|         APOOL|R-HSA-1592230,R-H...|ENSG00000155008|          NULL|\n",
      "|         CCAR1|R-HSA-72203,R-HSA...|ENSG00000060339|          NULL|\n",
      "|          CD96|        R-HSA-198933|ENSG00000153283|          NULL|\n",
      "|         CDH24|R-HSA-9759476,R-H...|ENSG00000139880|          NULL|\n",
      "+--------------+--------------------+---------------+--------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "spark.read.parquet(\"/Users/polina/Pathwaganda/diseaseId=EFO_0000094\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc6a7a6",
   "metadata": {},
   "source": [
    "### Merge with genetic association score from OT platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b44fa14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def merge_parquet_folders_with_na(spark, folder_1, folder_2, output_dir):\n",
    "    \"\"\"\n",
    "    For each subfolder in folder_1:\n",
    "    - Read parquet from folder_1 and folder_2 (same subfolder name)\n",
    "    - Rename columns in folder_2 df: '0' -> 'approvedSymbol', '1' -> 'geneticScore'\n",
    "    - Left join folder_1 df with folder_2 df on 'approvedSymbol'\n",
    "    - If folder_2 subfolder missing, write folder_1 df as is\n",
    "    - Write merged df to output_dir with same subfolder name\n",
    "    \"\"\"\n",
    "    folder_1_subdirs = [name for name in os.listdir(folder_1) \n",
    "                        if os.path.isdir(os.path.join(folder_1, name))]\n",
    "\n",
    "    for subdir in folder_1_subdirs:\n",
    "        path_1 = os.path.join(folder_1, subdir)\n",
    "        path_2 = os.path.join(folder_2, subdir)\n",
    "        output_path = os.path.join(output_dir, subdir)\n",
    "\n",
    "        df1 = spark.read.parquet(path_1)\n",
    "\n",
    "        if not os.path.exists(path_2):\n",
    "            print(f\"Folder {subdir} missing in folder_2. Writing original file from folder_1 as is.\")\n",
    "            df1.write.mode(\"overwrite\").parquet(output_path)\n",
    "            continue\n",
    "\n",
    "        df2 = spark.read.parquet(path_2)\n",
    "        df2_renamed = df2.withColumnRenamed(\"0\", \"approvedSymbol\") \\\n",
    "                         .withColumnRenamed(\"1\", \"geneticScore\")\n",
    "\n",
    "        # Left join so unmatched get null for geneticScore\n",
    "        merged_df = df1.join(df2_renamed.select(\"approvedSymbol\", \"geneticScore\"), \n",
    "                             on=\"approvedSymbol\", how=\"left\")\n",
    "\n",
    "        merged_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "        print(f\"Merged and written: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1443e4b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000503\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0011015\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0000569\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0003916\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004533\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002033\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0017343\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002691\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0001014\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0007987\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000504\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004730\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0007989\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002654\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005423\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0019472\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000707\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0000152\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0001877\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0000594\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0000951\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0007911\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004338\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0007788\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005689\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0001627\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0005041\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009282\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0003119\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1001949\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0010700\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004309\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0000598\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009086\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005680\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0018531\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0004390\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=GO_0009410\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004736\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1000233\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1000657\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000706\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002494\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0007576\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000701\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0010593\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004193\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000365\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004503\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0001071\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004731\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0001871\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0000592\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0044881\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000195\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0004907\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000708\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0001249\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0024503\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=Orphanet_68336\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0008591\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0000539\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004532\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0002890\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009676\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0017342\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0021076\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0007986\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009682\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0001642\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0008509\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0008903\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=Orphanet_322126\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000763\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0001645\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004337\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0006460\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009283\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0007789\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0024337\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1000294\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0010701\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009270\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002898\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009284\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002050\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004306\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004908\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0002461\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005220\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004280\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004274\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004273\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004617\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0021796\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0006943\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004842\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000618\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0003060\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000275\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0003863\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0003897\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005952\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002917\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000272\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000616\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0010642\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0000271\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0010226\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004289\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002928\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0002916\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0007800\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0024476\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0002571\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005592\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0003059\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0037254\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0003061\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1000941\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0007263\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005105\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005561\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000228\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000673\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0000621\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002512\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005134\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002149\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0003839\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0002715\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004627\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0003865\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004872\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0003891\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004288\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0011962\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1001455\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002320\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004875\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1000121\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1001463\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004616\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0005301\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0004643\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004611\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0003853\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009555\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0021355\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0006945\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005708\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0021193\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1002018\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000478\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000681\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0044937\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002974\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005763\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1000172\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0007254\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0006788\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0006318\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=Orphanet_271847\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0002919\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0024479\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0037255\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0003060\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1000143\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0007005\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000220\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0024477\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002525\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0002917\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0000629\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005755\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0021118\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009718\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005922\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1001047\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005784\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0005341\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005110\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0007223\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0003674\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009386\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0004251\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0004867\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005741\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0003825\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0021581\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000668\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002367\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0006500\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009910\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0003086\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002334\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0005575\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002562\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0001379\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000294\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009546\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1000307\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000400\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0003812\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0004805\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002108\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1000363\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000232\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1002050\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005116\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0022196\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1000999\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0001187\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004833\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000651\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1000532\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004695\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0007010\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0004634\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0021117\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009387\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005771\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005543\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005127\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0007442\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0044925\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005588\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0021545\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=GO_0032501\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002165\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0007825\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0006335\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1000158\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0008002\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000096\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004260\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1001471\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0001572\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0003841\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009578\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0000833\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0021143\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0001378\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1001216\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1002003\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004269\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009549\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0010724\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004324\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004586\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000589\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0020946\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000313\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000545\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004323\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004747\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009255\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0004985\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004315\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009607\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004312\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000574\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0000383\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0021054\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002480\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000349\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1000218\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004346\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0021259\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005856\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004528\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0004126\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004725\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004517\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0007392\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0004580\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1001901\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004348\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005407\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1001754\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004145\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0005499\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004713\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000181\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0021634\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0002619\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0001657\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0002428\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0021096\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0000588\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005090\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004142\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0008549\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005235\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0004379\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0000376\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0008515\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002277\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009433\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0000929\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0003767\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004314\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005232\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004784\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002614\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005803\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004574\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1000627\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0006812\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0004979\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0008524\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000771\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004587\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0003756\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004741\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0003100\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009608\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0020947\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0005090\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0003769\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0021251\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0010118\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0006841\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0007993\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0021063\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000341\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0000589\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1001938\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004527\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005091\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0021632\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009690\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1001331\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0001263\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1000217\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0003966\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004529\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000519\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004516\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000384\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004340\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1001763\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0008579\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0002427\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0030972\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000348\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0011008\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009003\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0008328\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0008170\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004949\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0006848\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0003925\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0010155\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004555\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1001512\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004303\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000565\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1000262\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0004936\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1000068\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1001986\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0005277\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0008354\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0040677\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0803548\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1001185\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1000601\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0001444\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0003777\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002256\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0003914\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0010967\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0011842\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005670\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009085\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0018364\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0803546\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0010351\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000305\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0003144\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0001421\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004705\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0011011\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0010934\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000538\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004198\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005278\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0032263\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004530\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0021080\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0002892\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009674\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0007984\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=GO_0008150\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002039\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0000707\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004995\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000199\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0011024\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0001073\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0021248\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0006858\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0001626\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004191\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002491\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009477\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009483\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002406\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0005411\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0003549\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1000255\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000304\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0803547\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0803540\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1000636\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0024757\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0012638\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004305\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004761\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0005271\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005815\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0000956\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000756\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0003124\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0004992\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1000051\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002259\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0010968\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004302\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0007915\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004190\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000702\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004732\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0006859\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0011025\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002232\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0008550\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0000591\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0011446\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0001075\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004735\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002038\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000705\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0008568\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000537\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0021043\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0023370\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000508\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004531\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004703\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0017341\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0008595\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0007985\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002009\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0002060\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004536\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004509\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0006545\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000474\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005106\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000222\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0010282\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002715\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0010088\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0000426\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0007803\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0021335\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0015756\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0003833\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0010285\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0021138\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0015760\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000677\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0007660\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002516\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000684\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002529\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0003869\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0006527\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004614\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0021350\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009550\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0002970\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004283\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1000786\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0007861\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0003851\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0100790\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0001933\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0006340\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0003893\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0000640\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0004670\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004622\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0000812\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0003860\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0000078\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0000624\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0000429\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000685\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0000478\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0003409\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000676\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0001939\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002977\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0015757\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009534\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0003832\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0030680\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0010284\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000640\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0044334\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009533\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0037256\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0006544\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004247\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005950\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0003859\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0007869\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0024276\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0003892\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0000473\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002116\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004612\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1000541\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0024885\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0000648\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000246\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002120\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0003868\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004615\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0006941\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004840\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0001977\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0044710\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004298\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0003872\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0024296\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005140\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0100543\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004639\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004606\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009119\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000094\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0000836\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0012758\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0024637\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0003843\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1000304\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0003888\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000403\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0020638\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0021148\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005541\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1000350\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000691\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=GO_0007610\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004458\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0005184\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0025142\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004838\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000662\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0005148\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005774\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0003274\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0000637\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005570\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000466\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002532\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009188\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0002597\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0000234\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0020092\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009518\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005548\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0005374\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1000359\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0020665\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009544\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0012759\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004264\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0003219\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0008039\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000095\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0007629\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002567\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0021375\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0100342\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0010467\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004631\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0000653\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004696\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0005178\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0003820\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0005147\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0004095\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000209\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0012531\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0003818\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0021583\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0020663\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004468\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0023644\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0001574\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005775\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1000356\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0006336\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0000636\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004698\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005540\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000200\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005772\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0007441\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000729\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1000223\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0002425\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009805\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004980\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000178\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0025031\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0001061\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0007396\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0002422\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0012647\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0008581\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0007991\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0006843\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002025\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0008111\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005631\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0012443\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1000020\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1000018\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0012649\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1001902\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0021066\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0021254\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0000920\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000512\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000182\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000546\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1000417\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0000545\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0010176\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1000870\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1001956\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=GO_0050896\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004327\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0001056\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1001951\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0010972\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0000726\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0020945\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=GO_0036273\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0001663\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0002011\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0004975\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005653\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0008528\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009431\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0017595\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1000613\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000326\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0004986\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1000045\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0007331\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0003765\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000319\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0024582\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002616\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004329\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009260\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0003939\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0006845\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0803539\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0021631\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0007763\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0021058\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0007352\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0002618\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000719\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0021636\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0007990\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0006842\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000389\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004729\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0001069\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000514\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0007355\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002229\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1000021\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0002624\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004149\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1000017\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0008589\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004512\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0021069\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009804\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004343\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1000646\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004541\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0040064\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000571\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0007330\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1000044\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0002814\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009605\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0000386\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0003931\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000318\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0003193\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0008315\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009259\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0003763\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0017594\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0021230\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0016680\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0002280\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009602\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000540\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0004570\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0010514\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0020944\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0009406\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=MONDO_0021662\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0000924\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1001950\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=HP_0000118\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1001968\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_1000416\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0005809\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0020943\n",
      "Merged and written: /Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0007937\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"MergeParquets\").getOrCreate()\n",
    "\n",
    "merge_parquet_folders_with_na(\n",
    "    spark,\n",
    "    folder_1=\"/Users/polina/Pathwaganda/data/target_metadata/known_drug_merge/Reactome_Pathways_2025_diy\",\n",
    "    folder_2=\"/Users/polina/Pathwaganda/data/input_4_gsea\",\n",
    "    output_dir=\"/Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9ebf37c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "283"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet(\"/Users/polina/Pathwaganda/data/target_metadata/ge_merge/test/diseaseId=MONDO_0045024\").filter(col(\"targetId\").isNull()).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe014705",
   "metadata": {},
   "source": [
    "! Need to make synonyms search !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb179e6d",
   "metadata": {},
   "source": [
    "# Prepare file with umap coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4953fbd3",
   "metadata": {},
   "source": [
    "## Case 1: user hasn't specified list of genes (show only genetics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83bd5c5",
   "metadata": {},
   "source": [
    "Steps: \n",
    "- Take coordinate file and run umap and clustering\n",
    "- Write coordinates and clusters into correspondent metadata file\n",
    "- Run gsea to put labels for each pathway (opt)\n",
    "- Filter out genes without genetic evidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9325044",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import umap\n",
    "import hdbscan\n",
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d6288b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_poincare_distance_matrix(embedding_matrix):\n",
    "    \"\"\"Vectorized computation of pairwise Poincar distances.\"\"\"\n",
    "    def poincare_dist(u, v):\n",
    "        norm_u = np.linalg.norm(u)\n",
    "        norm_v = np.linalg.norm(v)\n",
    "        norm_diff = np.linalg.norm(u - v)\n",
    "\n",
    "        denom = (1 - norm_u ** 2) * (1 - norm_v ** 2)\n",
    "        if denom <= 0:\n",
    "            return float('inf')\n",
    "\n",
    "        argument = 1 + 2 * (norm_diff ** 2) / denom\n",
    "        return np.arccosh(argument)\n",
    "\n",
    "    return squareform(pdist(embedding_matrix, metric=poincare_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942bec2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_umap_clustering_parquet(\n",
    "    metadata_parquet_dir,\n",
    "    coordinates_parquet_dir,\n",
    "    output_dir,\n",
    "    n_neighbors=10,\n",
    "    min_dist=0.5,\n",
    "    min_cluster_size=12,\n",
    "    umap_dimensions=2\n",
    "):\n",
    "    \"\"\"\n",
    "    Performs UMAP dimensionality reduction and HDBSCAN clustering using Poincar distance.\n",
    "    Aligns metadata and coordinates by 'approvedSymbol', saves final result as TSV.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load metadata and coordinates\n",
    "    metadata = pd.read_parquet(metadata_parquet_dir).query(\"geneticScore.notnull()\")\n",
    "    coords_df = pd.read_parquet(coordinates_parquet_dir)\n",
    "\n",
    "    # Sanity checks\n",
    "    assert 'approvedSymbol' in metadata.columns, \"Metadata must contain 'approvedSymbol'\"\n",
    "    assert coords_df.shape[1] > 1, \"Coordinates must have approvedSymbol + at least one dimension\"\n",
    "\n",
    "    # Rename first column to 'approvedSymbol' if needed\n",
    "    coords_df = coords_df.rename(columns={coords_df.columns[0]: 'approvedSymbol'})\n",
    "\n",
    "    # Convert coordinate columns to float\n",
    "    coord_columns = coords_df.columns[1:]\n",
    "    coords_df[coord_columns] = coords_df[coord_columns].astype(float)\n",
    "\n",
    "    # Merge metadata and coordinates on approvedSymbol\n",
    "    merged_df = pd.merge(metadata, coords_df, on='approvedSymbol', how='inner')\n",
    "    print(f\"Merged metadata and coordinates: {merged_df.shape[0]} entries.\")\n",
    "\n",
    "    # Extract embedding matrix (in correct order)\n",
    "    embedding_matrix = merged_df[coord_columns].values\n",
    "\n",
    "    def compute_poincare_distance_matrix(embedding_matrix):\n",
    "    \"\"\"Vectorized computation of pairwise Poincar distances.\"\"\"\n",
    "    def poincare_dist(u, v):\n",
    "        norm_u = np.linalg.norm(u)\n",
    "        norm_v = np.linalg.norm(v)\n",
    "        norm_diff = np.linalg.norm(u - v)\n",
    "\n",
    "        denom = (1 - norm_u ** 2) * (1 - norm_v ** 2)\n",
    "        if denom <= 0:\n",
    "            return float('inf')\n",
    "\n",
    "        argument = 1 + 2 * (norm_diff ** 2) / denom\n",
    "        return np.arccosh(argument)\n",
    "\n",
    "    return squareform(pdist(embedding_matrix, metric=poincare_dist))\n",
    "\n",
    "    # Check that all embeddings lie within the unit ball\n",
    "    norms = np.linalg.norm(embedding_matrix, axis=1)\n",
    "    if np.any(norms >= 1):\n",
    "        raise ValueError(\"Some embeddings lie outside the Poincar ball (norm >= 1).\")\n",
    "\n",
    "    distance_matrix = compute_poincare_distance_matrix(embedding_matrix)\n",
    "\n",
    "    # UMAP dimensionality reduction\n",
    "    reducer = umap.UMAP(\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        n_components=umap_dimensions,\n",
    "        metric='precomputed',\n",
    "        random_state=42\n",
    "    )\n",
    "    embedding_umap = reducer.fit_transform(distance_matrix)\n",
    "\n",
    "    # HDBSCAN clustering\n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        min_samples=1,\n",
    "        metric='precomputed'\n",
    "    )\n",
    "    cluster_labels = clusterer.fit_predict(distance_matrix)\n",
    "\n",
    "    # Add UMAP and cluster results to merged_df\n",
    "    for dim in range(umap_dimensions):\n",
    "        merged_df[f'UMAP {dim+1}'] = embedding_umap[:, dim]\n",
    "    merged_df['cluster'] = cluster_labels\n",
    "\n",
    "    # Drop original embedding dimensions before saving\n",
    "    output_df = merged_df.drop(columns=coord_columns)\n",
    "\n",
    "    # Output directory and file\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_file = os.path.join(output_dir, 'metadata_clusters_poincare_fast_ge.tsv')\n",
    "    output_df.to_csv(output_file, sep='\\t', index=False)\n",
    "\n",
    "\n",
    "    print(f\" Updated metadata with clusters saved to: {output_file}\")\n",
    "    return output_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43032e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged metadata and coordinates: 528 entries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/umap/umap_.py:1865: UserWarning: using precomputed metric; inverse_transform will be unavailable\n",
      "  warn(\"using precomputed metric; inverse_transform will be unavailable\")\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/umap/spectral.py:548: UserWarning: Spectral initialisation failed! The eigenvector solver\n",
      "failed. This is likely due to too small an eigengap. Consider\n",
      "adding some noise or jitter to your data.\n",
      "\n",
      "Falling back to random initialisation!\n",
      "  warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Updated metadata with clusters saved to: /Users/polina/Pathwaganda/data/umap/test/metadata_clusters_poincare_fast_ge.tsv\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/Users/polina/Pathwaganda/data/umap/test/metadata_clusters_poincare_fast_ge.tsv'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perform_umap_clustering_parquet(\n",
    "    metadata_parquet_dir=\"/Users/polina/Pathwaganda/data/target_metadata/ge_merge/Reactome_Pathways_2025_diy/diseaseId=EFO_0000094/\",\n",
    "    coordinates_parquet_dir=\"/Users/polina/Pathwaganda/data/target_embeddings/Reactome_Pathways_2025_diy/diseaseId=EFO_0000094/\",\n",
    "    output_dir=\"/Users/polina/Pathwaganda/data/umap/test\",\n",
    "    n_neighbors=5,\n",
    "    min_dist=0.7,\n",
    "    min_cluster_size=5,\n",
    "    umap_dimensions=2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7da84d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7d2ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
