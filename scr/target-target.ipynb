{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "856135c1",
   "metadata": {},
   "source": [
    "This code is pasted from IBD_pathway_to_cell from similarity_mvp and should be rewritten to spark to process data more effeciently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7644035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, split, array_distinct, udf\n",
    "from pyspark.ml.feature import MinHashLSH\n",
    "from pyspark.sql.types import FloatType\n",
    "import pyspark.sql.functions as F\n",
    "import os\n",
    "import numpy as np\n",
    "import gcsfs\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2799bc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a94715",
   "metadata": {},
   "source": [
    "# Similarity matrix for propagation results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d897a006",
   "metadata": {},
   "source": [
    "## Misnhash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10f12f9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "For MVP: \n",
    "\n",
    "presence or absence of target in pathways (0/1)\n",
    "\n",
    "Jaccard for distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a275554",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_jaccard_similarity_spark_minhash(input_gcs_dir, output_gcs_dir, folders_to_process, num_hash_tables=5):\n",
    "    \"\"\"\n",
    "    Process CSV files in specified folders within a GCS directory and calculate Jaccard similarity matrices using Spark MinHashLSH (approximate).\n",
    "\n",
    "    Args:\n",
    "        input_gcs_dir (str): Input GCS directory path.\n",
    "        output_gcs_dir (str): Output GCS directory path.\n",
    "        folders_to_process (list): List of folder names within the input directory to process.\n",
    "        num_hash_tables (int): Number of hash tables for MinHash (higher = more accuracy, slower).\n",
    "\n",
    "    Output:\n",
    "        Saves approximate similarity results into output GCS directory.\n",
    "    \"\"\"\n",
    "\n",
    "    input_gcs_dir = input_gcs_dir.rstrip(\"/\")\n",
    "    output_gcs_dir = output_gcs_dir.rstrip(\"/\")\n",
    "\n",
    "    for folder_name in folders_to_process:\n",
    "        folder_path = f\"{input_gcs_dir}/{folder_name}\"\n",
    "        output_folder_path = f\"{output_gcs_dir}/{folder_name}\"\n",
    "\n",
    "        # Read all CSVs inside the folder\n",
    "        df = spark.read.option(\"header\", True).csv(f\"{folder_path}/*.csv\")\n",
    "\n",
    "        # Check if required columns exist\n",
    "        expected_cols = {'propagated_edge', 'Term'}\n",
    "        if not expected_cols.issubset(set(df.columns)):\n",
    "            print(f\"Skipping folder {folder_name}: missing required columns.\")\n",
    "            continue\n",
    "\n",
    "        # Explode propagated_edge column\n",
    "        df_exploded = df.withColumn(\n",
    "            \"propagated_edge_exploded\", \n",
    "            explode(split(col(\"propagated_edge\"), \",\"))\n",
    "        ).dropna(subset=[\"propagated_edge_exploded\"])\n",
    "\n",
    "        # Group by target and collect associated terms\n",
    "        target_terms = df_exploded.groupBy(\"propagated_edge_exploded\") \\\n",
    "            .agg(F.collect_set(\"Term\").alias(\"terms\"))\n",
    "\n",
    "        # Index the terms (since MinHash needs fixed-size numeric vectors)\n",
    "        # Create a dictionary: term -> index\n",
    "        all_terms = df_exploded.select(\"Term\").distinct().rdd.map(lambda r: r[0]).collect()\n",
    "        term_to_index = {term: idx for idx, term in enumerate(all_terms)}\n",
    "\n",
    "        # Transform terms into SparseVectors\n",
    "        def terms_to_vector(terms):\n",
    "            indices = [term_to_index[term] for term in terms if term in term_to_index]\n",
    "            values = [1.0] * len(indices)\n",
    "            size = len(term_to_index)\n",
    "            return Vectors.sparse(size, list(zip(indices, values)))\n",
    "\n",
    "        terms_to_vector_udf = udf(terms_to_vector, VectorUDT())\n",
    "\n",
    "\n",
    "        target_terms = target_terms.withColumn(\"features\", terms_to_vector_udf(col(\"terms\")))\n",
    "\n",
    "        # Initialize MinHashLSH model\n",
    "        mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=num_hash_tables)\n",
    "        model = mh.fit(target_terms)\n",
    "\n",
    "        # Compute pairwise similarities\n",
    "        similarities = model.approxSimilarityJoin(target_terms, target_terms, threshold=1.0, distCol=\"JaccardDistance\")\n",
    "\n",
    "        # Select relevant columns and format\n",
    "        similarity_df = similarities.select(\n",
    "            col(\"datasetA.propagated_edge_exploded\").alias(\"target1\"),\n",
    "            col(\"datasetB.propagated_edge_exploded\").alias(\"target2\"),\n",
    "            (1 - col(\"JaccardDistance\")).alias(\"similarity\")\n",
    "        ).filter(\"target1 <= target2\")  # Avoid duplicates (symmetry)\n",
    "\n",
    "        # Save the similarity results\n",
    "        similarity_df.write.mode('overwrite').option(\"header\", True).csv(f\"{output_folder_path}\")\n",
    "\n",
    "        print(f\"Output: {output_folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "135d9160",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed (MinHash) and uploaded: gs://ot-team/polina/pathway_propagation_validation_v2/similarity_mtx/jaccard_spark/test\n"
     ]
    }
   ],
   "source": [
    "gsea_dir = \"gs://ot-team/polina/pathway_propagation_validation_v2/gsea_output\"\n",
    "output_dir = \"gs://ot-team/polina/pathway_propagation_validation_v2/similarity_mtx/jaccard_spark\"\n",
    "\n",
    "# library = [\"KEGG_2021_Human\"]\n",
    "library = [\"test\"]\n",
    "\n",
    "calculate_jaccard_similarity_spark_minhash(gsea_dir, output_dir, library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1abed3a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"gs://ot-team/polina/pathway_propagation_validation_v2/similarity_mtx/jaccard_spark/test\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0076cf72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------------+\n",
      "|target1|target2|         similarity|\n",
      "+-------+-------+-------------------+\n",
      "|  ABCA1|  MYD88|0.06666666666666665|\n",
      "|  ACTG1|  MED16| 0.1428571428571428|\n",
      "|  ACTN1|   CDK6|0.04166666666666663|\n",
      "|  ACTN4| GTF2A1|                0.5|\n",
      "|  ACTN4| ARPC5L|0.33333333333333326|\n",
      "+-------+-------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "786a2768",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "513150"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97406007",
   "metadata": {},
   "source": [
    "### MinHashLSH vs Exact Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "80ddce63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(set1, set2):\n",
    "    if not set1 or not set2:\n",
    "        return 0.0\n",
    "    intersection = len(set(set1).intersection(set(set2)))\n",
    "    union = len(set(set1).union(set(set2)))\n",
    "    return float(intersection) / float(union)\n",
    "\n",
    "def compare_jaccard_exact_vs_minhash(df_input, num_hash_tables=5):\n",
    "    spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "    # Explode propagated_edge\n",
    "    df = df_input.withColumn(\n",
    "        \"propagated_edge_exploded\",\n",
    "        explode(split(col(\"propagated_edge\"), \",\"))\n",
    "    ).dropna(subset=[\"propagated_edge_exploded\"])\n",
    "\n",
    "    # Group: each node â†’ list of terms\n",
    "    target_terms = df.groupBy(\"propagated_edge_exploded\") \\\n",
    "        .agg(F.collect_set(\"Term\").alias(\"terms\"))\n",
    "\n",
    "    # Get all unique terms and assign indices\n",
    "    all_terms = df.select(\"Term\").distinct().rdd.map(lambda r: r[0]).collect()\n",
    "    term_to_index = {term: idx for idx, term in enumerate(all_terms)}\n",
    "\n",
    "    def terms_to_sparse_vector(terms):\n",
    "        indices = [term_to_index[t] for t in terms if t in term_to_index]\n",
    "        return Vectors.sparse(len(term_to_index), [(i, 1.0) for i in indices])\n",
    "\n",
    "    terms_to_vector_udf = udf(terms_to_sparse_vector, VectorUDT())\n",
    "\n",
    "    target_terms = target_terms.withColumn(\"features\", terms_to_vector_udf(col(\"terms\")))\n",
    "\n",
    "    # Fit MinHashLSH\n",
    "    mh = MinHashLSH(inputCol=\"features\", outputCol=\"hashes\", numHashTables=num_hash_tables)\n",
    "    model = mh.fit(target_terms)\n",
    "\n",
    "    # Compute MinHash similarities\n",
    "    approx_sim = model.approxSimilarityJoin(target_terms, target_terms, threshold=1, distCol=\"JaccardDistance\") \\\n",
    "        .filter(\"datasetA.propagated_edge_exploded <= datasetB.propagated_edge_exploded\") \\\n",
    "        .select(\n",
    "            col(\"datasetA.propagated_edge_exploded\").alias(\"target1\"),\n",
    "            col(\"datasetA.terms\").alias(\"terms1\"),\n",
    "            col(\"datasetB.propagated_edge_exploded\").alias(\"target2\"),\n",
    "            col(\"datasetB.terms\").alias(\"terms2\"),\n",
    "            (1 - col(\"JaccardDistance\")).alias(\"approx_jaccard\")\n",
    "        )\n",
    "\n",
    "    # Compute exact Jaccard\n",
    "    jaccard_udf = udf(jaccard_similarity, FloatType())\n",
    "    with_exact = approx_sim.withColumn(\"exact_jaccard\", jaccard_udf(col(\"terms1\"), col(\"terms2\")))\n",
    "\n",
    "    # Calculate % error\n",
    "    with_error = with_exact.withColumn(\n",
    "        \"percent_error\",\n",
    "        F.when(col(\"exact_jaccard\") > 0,\n",
    "               F.abs(col(\"approx_jaccard\") - col(\"exact_jaccard\")) / col(\"exact_jaccard\") * 100)\n",
    "         .otherwise(None)\n",
    "    )\n",
    "\n",
    "    # Clean output\n",
    "    final_result = with_error.select(\"target1\", \"target2\", \"approx_jaccard\", \"exact_jaccard\", \"percent_error\")\n",
    "\n",
    "    return final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8de62c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare = spark.read.option(\"header\", True).csv(\"gs://ot-team/polina/pathway_propagation_validation_v2/gsea_output/test/*.csv\")\n",
    "result_df_compare = compare_jaccard_exact_vs_minhash(df_compare, num_hash_tables=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "babf4751",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 106:======================================>                  (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------+--------------------+-------------+--------------------+\n",
      "|target1 |target2|approx_jaccard      |exact_jaccard|percent_error       |\n",
      "+--------+-------+--------------------+-------------+--------------------+\n",
      "|CALCOCO2|HRAS   |0.015873015873015817|0.015873017  |5.774199984473481E-6|\n",
      "|KRAS    |SNW1   |0.015873015873015817|0.015873017  |5.774199984473481E-6|\n",
      "|CALCOCO2|KRAS   |0.015873015873015817|0.015873017  |5.774199984473481E-6|\n",
      "|HRAS    |SNW1   |0.015873015873015817|0.015873017  |5.774199984473481E-6|\n",
      "|HDAC1   |NRAS   |0.12698412698412698 |0.12698413   |5.774199634753249E-6|\n",
      "|KRAS    |RBPJ   |0.031746031746031744|0.031746034  |5.774199634753249E-6|\n",
      "|HLA-E   |KRAS   |0.12698412698412698 |0.12698413   |5.774199634753249E-6|\n",
      "|HDAC2   |NRAS   |0.12698412698412698 |0.12698413   |5.774199634753249E-6|\n",
      "|NRAS    |SKP2   |0.06349206349206349 |0.06349207   |5.774199634753249E-6|\n",
      "|CAMK2A  |PIK3R1 |0.12698412698412698 |0.12698413   |5.774199634753249E-6|\n",
      "|HLA-A   |HRAS   |0.12698412698412698 |0.12698413   |5.774199634753249E-6|\n",
      "|MAP2K1  |NOS3   |0.06349206349206349 |0.06349207   |5.774199634753249E-6|\n",
      "|HLA-G   |KRAS   |0.12698412698412698 |0.12698413   |5.774199634753249E-6|\n",
      "|CAMK2G  |PIK3R3 |0.12698412698412698 |0.12698413   |5.774199634753249E-6|\n",
      "|HRAS    |RBPJL  |0.031746031746031744|0.031746034  |5.774199634753249E-6|\n",
      "|CAMK2G  |PIK3R1 |0.12698412698412698 |0.12698413   |5.774199634753249E-6|\n",
      "|KRAS    |RAB7A  |0.031746031746031744|0.031746034  |5.774199634753249E-6|\n",
      "|HLA-B   |HRAS   |0.12698412698412698 |0.12698413   |5.774199634753249E-6|\n",
      "|HRAS    |LTBR   |0.031746031746031744|0.031746034  |5.774199634753249E-6|\n",
      "|HLA-C   |HRAS   |0.12698412698412698 |0.12698413   |5.774199634753249E-6|\n",
      "|CAMK2A  |PIK3R2 |0.12698412698412698 |0.12698413   |5.774199634753249E-6|\n",
      "|HLA-C   |KRAS   |0.12698412698412698 |0.12698413   |5.774199634753249E-6|\n",
      "|CDK6    |MAP2K2 |0.25396825396825395 |0.25396827   |5.774199634753249E-6|\n",
      "|PIK3R3  |RELA   |0.5079365079365079  |0.50793654   |5.774199634753249E-6|\n",
      "|KRAS    |LTBR   |0.031746031746031744|0.031746034  |5.774199634753249E-6|\n",
      "|PIK3R1  |RELA   |0.5079365079365079  |0.50793654   |5.774199634753249E-6|\n",
      "|CAMK2D  |PIK3R3 |0.12698412698412698 |0.12698413   |5.774199634753249E-6|\n",
      "|RAF1    |TLR4   |0.12698412698412698 |0.12698413   |5.774199634753249E-6|\n",
      "|CAMK2D  |PIK3R1 |0.12698412698412698 |0.12698413   |5.774199634753249E-6|\n",
      "|CAMK2D  |PIK3R2 |0.12698412698412698 |0.12698413   |5.774199634753249E-6|\n",
      "|MAP2K2  |TLR4   |0.12698412698412698 |0.12698413   |5.774199634753249E-6|\n",
      "|PIK3R2  |RELA   |0.5079365079365079  |0.50793654   |5.774199634753249E-6|\n",
      "|HLA-G   |HRAS   |0.12698412698412698 |0.12698413   |5.774199634753249E-6|\n",
      "|CDK1    |KRAS   |0.06349206349206349 |0.06349207   |5.774199634753249E-6|\n",
      "|CAMK2B  |PIK3R2 |0.12698412698412698 |0.12698413   |5.774199634753249E-6|\n",
      "|CTNNB1  |NRAS   |0.25396825396825395 |0.25396827   |5.774199634753249E-6|\n",
      "|CAMK2G  |PIK3R2 |0.12698412698412698 |0.12698413   |5.774199634753249E-6|\n",
      "|HRAS    |RBPJ   |0.031746031746031744|0.031746034  |5.774199634753249E-6|\n",
      "|CAMK2B  |PIK3R1 |0.12698412698412698 |0.12698413   |5.774199634753249E-6|\n",
      "|CDK1    |HRAS   |0.06349206349206349 |0.06349207   |5.774199634753249E-6|\n",
      "|HLA-B   |KRAS   |0.12698412698412698 |0.12698413   |5.774199634753249E-6|\n",
      "|CDK6    |RAF1   |0.25396825396825395 |0.25396827   |5.774199634753249E-6|\n",
      "|MAP2K1  |RBX1   |0.06349206349206349 |0.06349207   |5.774199634753249E-6|\n",
      "|FOS     |MAP2K1 |0.25396825396825395 |0.25396827   |5.774199634753249E-6|\n",
      "|KRAS    |RBPJL  |0.031746031746031744|0.031746034  |5.774199634753249E-6|\n",
      "|CAMK2B  |PIK3R3 |0.12698412698412698 |0.12698413   |5.774199634753249E-6|\n",
      "|CHEK1   |KRAS   |0.06349206349206349 |0.06349207   |5.774199634753249E-6|\n",
      "|CAMK2A  |PIK3R3 |0.12698412698412698 |0.12698413   |5.774199634753249E-6|\n",
      "|CHEK1   |HRAS   |0.06349206349206349 |0.06349207   |5.774199634753249E-6|\n",
      "|HLA-E   |HRAS   |0.12698412698412698 |0.12698413   |5.774199634753249E-6|\n",
      "+--------+-------+--------------------+-------------+--------------------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show comparison\n",
    "result_df_compare.orderBy(\"percent_error\", ascending=False).show(50, truncate=False)\n",
    "\n",
    "# # Save\n",
    "# result_df_compare.write.mode(\"overwrite\").option(\"header\", True).csv(\"gs://ot-team/polina/pathway_propagation_validation_v2/similarity_mtx/jaccard_spark/compare_exact_vs_minhash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "924033e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "513150"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df_compare.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "54db58af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 155:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------------------+-------------+--------------------+\n",
      "|target1|target2|     approx_jaccard|exact_jaccard|       percent_error|\n",
      "+-------+-------+-------------------+-------------+--------------------+\n",
      "|  ABCA1|  MYD88|0.06666666666666665|   0.06666667|5.215406168046513E-6|\n",
      "|  ACTG1|  MED16| 0.1428571428571428|   0.14285715| 4.47034820272308E-6|\n",
      "|  ACTN1|   CDK6|0.04166666666666663|  0.041666668|2.980232238769531...|\n",
      "|  ACTN4| GTF2A1|                0.5|          0.5|                 0.0|\n",
      "|  ACTN4| ARPC5L|0.33333333333333326|   0.33333334|2.980232172156152E-6|\n",
      "| ACTR10|  DCTN6|                1.0|          1.0|                 0.0|\n",
      "| ACTR1A|   GNAQ|0.05882352941176472|   0.05882353|3.725290062539522...|\n",
      "|  ACTR2|S100A10|                0.5|          0.5|                 0.0|\n",
      "| ACTR3B|   IRF3|0.06666666666666665|   0.06666667|5.215406168046513E-6|\n",
      "| ADAM17| NOTCH4|0.16666666666666663|   0.16666667|2.980232172156152E-6|\n",
      "|  ADCY4|  PDGFA|0.19999999999999996|          0.2|1.490116119384765...|\n",
      "|  ADCY9|SLC25A6|0.11764705882352944|   0.11764706|3.725290062539522...|\n",
      "|  ADCY9|  CDC23|0.06666666666666665|   0.06666667|5.215406168046513E-6|\n",
      "|  ADCY9|   JAG1|0.17647058823529416|    0.1764706|2.483526781757761E-6|\n",
      "| ADRA1B|   TRHR|                1.0|          1.0|                 0.0|\n",
      "| ADRA1D|   FGF8|              0.125|        0.125|                 0.0|\n",
      "|  ADRB2|  EDNRA|0.33333333333333326|   0.33333334|2.980232172156152E-6|\n",
      "|  ADRB3| CYP3A4|                0.5|          0.5|                 0.0|\n",
      "| AGPAT3|     F2|                0.5|          0.5|                 0.0|\n",
      "|   AKT3|   RFX5|0.01754385964912286|   0.01754386|7.450577432788231E-7|\n",
      "+-------+-------+-------------------+-------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "result_df_compare.filter(col(\"percent_error\") != 5.774199634753249E-6).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3da04d",
   "metadata": {},
   "source": [
    "Results seem identical so we can preliminarly use Minhash for similarity calculation. But lets just check if without its too slow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaa0c96",
   "metadata": {},
   "source": [
    "## Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d0beb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_jaccard_similarity_spark_exact(input_gcs_dir, output_gcs_dir, folders_to_process):\n",
    "    \"\"\"\n",
    "    Process CSV files in specified folders within a GCS directory and calculate exact Jaccard similarity matrices.\n",
    "\n",
    "    Args:\n",
    "        input_gcs_dir (str): Input GCS directory path.\n",
    "        output_gcs_dir (str): Output GCS directory path.\n",
    "        folders_to_process (list): List of folder names within the input directory to process.\n",
    "\n",
    "    Output:\n",
    "        Saves exact similarity results into output GCS directory.\n",
    "    \"\"\"\n",
    "\n",
    "    input_gcs_dir = input_gcs_dir.rstrip(\"/\")\n",
    "    output_gcs_dir = output_gcs_dir.rstrip(\"/\")\n",
    "\n",
    "    def jaccard_similarity(set1, set2):\n",
    "        if not set1 or not set2:\n",
    "            return 0.0\n",
    "        intersection = len(set(set1).intersection(set(set2)))\n",
    "        union = len(set(set1).union(set(set2)))\n",
    "        return float(intersection) / float(union)\n",
    "\n",
    "    jaccard_udf = udf(jaccard_similarity, FloatType())\n",
    "\n",
    "    for folder_name in folders_to_process:\n",
    "        folder_path = f\"{input_gcs_dir}/{folder_name}\"\n",
    "        output_folder_path = f\"{output_gcs_dir}/{folder_name}\"\n",
    "\n",
    "        # Read all CSVs inside the folder\n",
    "        df = spark.read.option(\"header\", True).csv(f\"{folder_path}/*.csv\")\n",
    "\n",
    "        # Check if required columns exist\n",
    "        expected_cols = {'propagated_edge', 'Term'}\n",
    "        if not expected_cols.issubset(set(df.columns)):\n",
    "            print(f\"Skipping folder {folder_name}: missing required columns.\")\n",
    "            continue\n",
    "\n",
    "        # Explode propagated_edge column\n",
    "        df_exploded = df.withColumn(\n",
    "            \"propagated_edge_exploded\", \n",
    "            explode(split(col(\"propagated_edge\"), \",\"))\n",
    "        ).dropna(subset=[\"propagated_edge_exploded\"])\n",
    "\n",
    "        # Group by target and collect associated terms\n",
    "        target_terms = df_exploded.groupBy(\"propagated_edge_exploded\") \\\n",
    "            .agg(F.collect_set(\"Term\").alias(\"terms\"))\n",
    "\n",
    "        # Create a DataFrame with all pairs of targets\n",
    "        target_terms_alias1 = target_terms.alias(\"target1\")\n",
    "        target_terms_alias2 = target_terms.alias(\"target2\")\n",
    "        \n",
    "        # Cross join to get all pairs, then filter to avoid duplicates\n",
    "        pairs = target_terms_alias1.crossJoin(target_terms_alias2) \\\n",
    "            .filter(\"target1.propagated_edge_exploded <= target2.propagated_edge_exploded\")\n",
    "\n",
    "        # Calculate exact Jaccard similarity for each pair\n",
    "        similarity_df = pairs.select(\n",
    "            col(\"target1.propagated_edge_exploded\").alias(\"target1\"),\n",
    "            col(\"target2.propagated_edge_exploded\").alias(\"target2\"),\n",
    "            jaccard_udf(col(\"target1.terms\"), col(\"target2.terms\")).alias(\"similarity\")\n",
    "        )\n",
    "\n",
    "        # Save the similarity results\n",
    "        similarity_df.write.mode('overwrite').option(\"header\", True).csv(f\"{output_folder_path}\")\n",
    "\n",
    "        print(f\"Output: {output_folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "17fbfc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: gs://ot-team/polina/pathway_propagation_validation_v2/similarity_mtx/jaccard_spark/test\n"
     ]
    }
   ],
   "source": [
    "gsea_dir = \"gs://ot-team/polina/pathway_propagation_validation_v2/gsea_output\"\n",
    "output_dir = \"gs://ot-team/polina/pathway_propagation_validation_v2/similarity_mtx/jaccard_spark\"\n",
    "\n",
    "# library = [\"KEGG_2021_Human\"]\n",
    "library = [\"test\"]\n",
    "\n",
    "calculate_jaccard_similarity_spark_exact(gsea_dir, output_dir, library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ce28d046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_show = spark.read.csv(\"gs://ot-team/polina/pathway_propagation_validation_v2/similarity_mtx/jaccard_spark/test\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "36e65f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 174:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+----------+\n",
      "|target1|target2|similarity|\n",
      "+-------+-------+----------+\n",
      "|   CDK4|   CDK4|       1.0|\n",
      "|   CDK4|   CDK5|       0.0|\n",
      "|   CDK4| CDK5R1|       0.0|\n",
      "|   CDK4|   CDK6|0.84615386|\n",
      "|   CDK4|   CDK7|      0.04|\n",
      "|   CDK4| CDKN1A| 0.5945946|\n",
      "|   CDK4| CDKN1B| 0.3548387|\n",
      "|   CDK4| CDKN1C|      0.04|\n",
      "|   CDK4| CDKN2A| 0.5769231|\n",
      "|   CDK4| CDKN2B|0.25925925|\n",
      "+-------+-------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_show.filter(col(\"target1\") == \"CDK4\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6014351b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c614ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
