{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7644035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.functions import col, explode, split, array_distinct, udf\n",
    "# from pyspark.ml.feature import MinHashLSH\n",
    "# from pyspark.sql.types import FloatType\n",
    "# import pyspark.sql.functions as F\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import gcsfs\n",
    "# from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "# from pyspark.sql.functions import udf\n",
    "# from itertools import combinations\n",
    "\n",
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.functions import col, split, explode, collect_set, lit, array, udf\n",
    "# from pyspark.sql.types import DoubleType\n",
    "# from itertools import combinations\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, split, collect_list, flatten, \n",
    "    array_intersect, array_union, size,\n",
    "    when, first\n",
    ")\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.storagelevel import StorageLevel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2799bc78",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a94715",
   "metadata": {},
   "source": [
    "# Similarity matrix for propagation results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef3da04d",
   "metadata": {},
   "source": [
    "For future speed optiomisation use Minhash for similarity calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adaa0c96",
   "metadata": {},
   "source": [
    "## Jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d0beb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_jaccard_similarity(base_gcs_path, folders_to_process, output_gcs_path):\n",
    "    spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "    \n",
    "    for folder in folders_to_process:\n",
    "        input_path = f\"{base_gcs_path}/{folder}/**/*.parquet\"\n",
    "        df = spark.read.option(\"recursiveFileLookup\", \"true\").parquet(input_path)\n",
    "        \n",
    "        # Process terms more efficiently\n",
    "        df = df.withColumn(\"term\", split(col(\"term\"), \",\"))\n",
    "        \n",
    "        grouped = df.groupBy(\"approvedSymbol\").agg(\n",
    "            flatten(collect_list(\"term\")).alias(\"terms\"),\n",
    "            first(\"taregtId\").alias(\"taregtId\")\n",
    "        ).cache()\n",
    "        \n",
    "        # Self-join with filtering\n",
    "        pairs = grouped.alias(\"a\").join(\n",
    "            grouped.alias(\"b\"),\n",
    "            col(\"a.approvedSymbol\") < col(\"b.approvedSymbol\")\n",
    "        ).filter(\n",
    "            size(array_intersect(col(\"a.terms\"), col(\"b.terms\"))) > 0\n",
    "        )\n",
    "        \n",
    "        # Calculate Jaccard without UDF\n",
    "        result = pairs.withColumn(\n",
    "            \"jaccardSimilarity\",\n",
    "            size(array_intersect(col(\"a.terms\"), col(\"b.terms\"))) / \n",
    "            size(array_union(col(\"a.terms\"), col(\"b.terms\")))\n",
    "        ).select(\n",
    "            col(\"a.approvedSymbol\").alias(\"approvedSymbolA\"),\n",
    "            col(\"b.approvedSymbol\").alias(\"approvedSymbolB\"),\n",
    "            col(\"a.taregtId\").alias(\"taregtIdA\"),\n",
    "            col(\"b.taregtId\").alias(\"taregtIdB\"),\n",
    "            col(\"jaccardSimilarity\")\n",
    "        )\n",
    "        \n",
    "        output_path = f\"{output_gcs_path}/{folder}/jaccard_output\"\n",
    "        result.write.mode(\"overwrite\").parquet(output_path)\n",
    "        grouped.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fbfc0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/01 11:09:13 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "[Stage 0:>                  (0 + 0) / 1][Stage 1:>                  (0 + 0) / 1]\r"
     ]
    }
   ],
   "source": [
    "gsea_dir = \"gs://ot-team/polina/pathway_propagation_validation_v2/gsea_output_spark\"\n",
    "output_dir = \"gs://ot-team/polina/pathway_propagation_validation_v2/similarity_mtx/jaccard_spark\"\n",
    "\n",
    "calculate_jaccard_similarity(\n",
    "    base_gcs_path=gsea_dir,\n",
    "    folders_to_process=[\"test\"],\n",
    "    output_gcs_path=output_dir\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ce28d046",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_show = spark.read.csv(\"gs://ot-team/polina/pathway_propagation_validation_v2/similarity_mtx/jaccard_spark/KEGG_2021_Human/EFO_0000565_ge_mm_som_gsea_KEGG_2021_Human_pval0.05\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "36e65f31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 476:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+-----------+\n",
      "|targetA|   targetB| jaccardSim|\n",
      "+-------+----------+-----------+\n",
      "|   CDK2|      CDK2|        1.0|\n",
      "|   CDK2|      CDK4| 0.41935483|\n",
      "|   CDK2|      CDK5|        0.0|\n",
      "|   CDK2|    CDK5R1|        0.0|\n",
      "|   CDK2|      CDK6| 0.41379312|\n",
      "|   CDK2|      CDK7|0.055555556|\n",
      "|   CDK2|      CDK9|        0.0|\n",
      "|   CDK2|    CDKN1A|  0.3902439|\n",
      "|   CDK2|    CDKN1B|        0.5|\n",
      "|   CDK2|    CDKN1C|0.055555556|\n",
      "|   CDK2|    CDKN2A| 0.25925925|\n",
      "|   CDK2|    CDKN2B|        0.5|\n",
      "|   CDK2|    CDKN2C| 0.15789473|\n",
      "|   CDK2|    CDKN2D| 0.11111111|\n",
      "|   CDK2|      CDX2|0.055555556|\n",
      "|   CDK2|     CEBPA|       0.05|\n",
      "|   CDK2|     CEBPB|        0.0|\n",
      "|   CDK2|     CEBPE|        0.0|\n",
      "|   CDK2|     CENPS|        0.0|\n",
      "|   CDK2|CENPS-CORT|        0.0|\n",
      "+-------+----------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_show.filter(col(\"targetA\") == \"CDK2\").show(20)\n",
    "# df_show.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6014351b",
   "metadata": {},
   "source": [
    "Ok it's too long. Lets return to hash-tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c614ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
