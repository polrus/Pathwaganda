{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1be007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install blitzgsea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805139b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql import functions as f\n",
    "# from pyspark.sql.functions import collect_list, concat_ws, col, when, udf, row_number, sum as spark_sum, max as spark_max, create_map, lit, min as spark_min\n",
    "# from pyspark.sql.types import DoubleType\n",
    "# from pyspark.sql import Window\n",
    "# from itertools import chain\n",
    "# from pyspark.sql import DataFrame\n",
    "# from pyspark.sql import Row\n",
    "# import pandas as pd\n",
    "# import os\n",
    "# import numpy as np\n",
    "# from sklearn.metrics import jaccard_score\n",
    "# from scipy.spatial.distance import pdist, squareform\n",
    "# from scipy.stats import spearmanr, kendalltau\n",
    "# import gcsfs\n",
    "# from pathlib import Path\n",
    "# import blitzgsea as blitz\n",
    "# from functools import reduce\n",
    "# from sklearn.metrics import roc_auc_score, precision_recall_curve\n",
    "# from sklearn.utils import resample\n",
    "# import statsmodels.api as sm\n",
    "# from scipy.stats import ttest_1samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9e26fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2e9b47",
   "metadata": {},
   "source": [
    "## Prepare input for blitzGSEA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3b7a9c",
   "metadata": {},
   "source": [
    "For each diseaseId with >= 500 genes:\n",
    "- take columns approvedSymbol, overallScore \n",
    "- sort by overallScore\n",
    "- convert each column name 'overallScore': '1', 'approvedSymbol': '0'\n",
    "- and saves each partition named after diseaseID into one parquet directory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3dd8f33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gcsfs\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, countDistinct\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0a7570",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1391:>                                                       (0 + 2) / 2]\r"
     ]
    }
   ],
   "source": [
    "# Input and output GCS paths\n",
    "INPUT_PATH  = \"gs://ot-team/polina/pathwaganda/processed_diseases/oncology\"\n",
    "OUTPUT_BASE = \"gs://ot-team/polina/pathwaganda/input_4_gsea/oncology\"\n",
    "\n",
    "# ─── Initialize GCS filesystem and check input ────────────────────────────────\n",
    "\n",
    "fs = gcsfs.GCSFileSystem()\n",
    "if not fs.exists(INPUT_PATH):\n",
    "    raise FileNotFoundError(f\"Input path not found: {INPUT_PATH}\")\n",
    "\n",
    "# ─── Read all Parquet files from GCS ──────────────────────────────────────────\n",
    "\n",
    "df = spark.read.parquet(INPUT_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3ee6c2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_TARGETS = 500\n",
    "\n",
    "# only pull diseaseIds whose countDistinct(approvedSymbol) >= MIN_TARGETS\n",
    "valid_diseases = (\n",
    "    df\n",
    "    .groupBy(\"diseaseId\")\n",
    "    .agg(countDistinct(\"approvedSymbol\").alias(\"uniqueCount\"))\n",
    "    .filter(col(\"uniqueCount\") >= MIN_TARGETS)\n",
    "    .select(\"diseaseId\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "285f9ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Inner‐join back to keep only those diseases\n",
    "df_filtered = df.join(valid_diseases, on=\"diseaseId\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d96e0074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Select & rename once up‑front:\n",
    "df2 = (\n",
    "    df_filtered\n",
    "    .select(\"diseaseId\", \"approvedSymbol\", \"overallScore\")\n",
    "    .withColumnRenamed(\"approvedSymbol\", \"0\")\n",
    "    .withColumnRenamed(\"overallScore\",     \"1\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aec04ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Repartition by diseaseId and sort within each partition **descending** by score:\n",
    "df2 = (\n",
    "    df2\n",
    "    .repartition(\"diseaseId\")\n",
    "    # .sortWithinPartitions(col(\"1\").desc())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad28d42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 3) Write out in one go, partitioned by diseaseId:\n",
    "df2.write \\\n",
    "   .mode(\"overwrite\") \\\n",
    "   .partitionBy(\"diseaseId\") \\\n",
    "   .parquet(OUTPUT_BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abe078d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "815"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1391:>                                                       (0 + 2) / 2]\r"
     ]
    }
   ],
   "source": [
    "# spark.read.parquet(\"gs://ot-team/polina/pathwaganda/input_4_gsea/non_oncology/diseaseId=EFO_0000195\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26fd220",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f85a60f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31a9012",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04595636",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da23fda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91270bda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c44025f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "import blitzgsea as blitz\n",
    "\n",
    "def blitzgsea_with_fileprep_spark(\n",
    "    input_gcs_dir: str,\n",
    "    output_gcs_dir: str,\n",
    "    libraries: list = None,\n",
    "    fdr_cutoff: float = 0.1\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Perform scalable GSEA on a Parquet dataset with PySpark, grouping by diseaseId and writing partitioned results.\n",
    "\n",
    "    Args:\n",
    "        input_gcs_dir (str): GCS path to the Parquet input (file or directory).\n",
    "        output_gcs_dir (str): GCS path for output library folders.\n",
    "        libraries (list of str, optional): List of blitz Enrichr library names. If None, fetches all available libraries.\n",
    "        fdr_cutoff (float): FDR threshold for significant terms.\n",
    "    \"\"\"\n",
    "    # Initialize Spark with GCS connector settings\n",
    "    spark = (\n",
    "        SparkSession\n",
    "        .builder\n",
    "        .appName(\"BlitzGSEA\")\n",
    "        # ensure Google Cloud Storage support\n",
    "        .config(\"spark.hadoop.fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "\n",
    "    # Read Parquet dataset directly from GCS\n",
    "    df = spark.read.parquet(input_gcs_dir)\n",
    "\n",
    "    # Select and rename columns for GSEA\n",
    "    df2 = df.select(\n",
    "        col(\"diseaseId\"),\n",
    "        col(\"targetId\"),\n",
    "        col(\"approvedSymbol\").alias(\"0\"),\n",
    "        col(\"overallScore\").alias(\"1\")\n",
    "    )\n",
    "\n",
    "    # Determine libraries to process\n",
    "    if libraries is None:\n",
    "        libraries = blitz.enrichr.get_libraries()\n",
    "\n",
    "    # Pre-load all library sets\n",
    "    library_sets = {lib: blitz.enrichr.get_library(lib) for lib in libraries}\n",
    "\n",
    "    # Collect distinct disease IDs\n",
    "    disease_ids = [row.diseaseId for row in df2.select(\"diseaseId\").distinct().collect()]\n",
    "\n",
    "    for disease_id in disease_ids:\n",
    "        # Filter, sort, and convert to pandas for blitz.gsea\n",
    "        pdf = (\n",
    "            df2\n",
    "            .filter(col(\"diseaseId\") == disease_id)\n",
    "            .orderBy(col(\"1\").desc())\n",
    "            .select(\"0\", \"1\")\n",
    "            .toPandas()\n",
    "        )\n",
    "        if pdf.empty:\n",
    "            continue\n",
    "\n",
    "        for lib in libraries:\n",
    "            library = library_sets[lib]\n",
    "\n",
    "            # Run GSEA\n",
    "            result = blitz.gsea(pdf, library, processes=4)\n",
    "            sig = result[result.get(\"FDR\", result.get(\"fdr\", result.get(\"fdr_qvalue\"))) <= fdr_cutoff].copy()\n",
    "            if sig.empty:\n",
    "                continue\n",
    "\n",
    "            # Propagation step\n",
    "            sig[\"propagated_edge\"] = sig.index.map(lambda term: \",\".join(library.get(term, [])))\n",
    "            sig[\"diseaseId\"] = disease_id\n",
    "\n",
    "            # Convert back to Spark and write to GCS as Parquet\n",
    "            sdf = spark.createDataFrame(\n",
    "                sig.reset_index().rename(columns={\"index\": \"Term\"})\n",
    "            )\n",
    "            output_path = f\"{output_gcs_dir.rstrip('/')}/{lib}/{disease_id}\"\n",
    "            sdf.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "    spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f036ff4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/22 11:31:16 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "                                                                                \r"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "float division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/opt/conda/miniconda3/lib/python3.11/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n                    ^^^^^^^^^^^^^^^^^^^\n  File \"/home/polina/.local/lib/python3.11/site-packages/blitzgsea/__init__.py\", line 39, in estimate_anchor_star\n    return estimate_anchor(*args)\n           ^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/polina/.local/lib/python3.11/site-packages/blitzgsea/__init__.py\", line 42, in estimate_anchor\n    es = np.array(get_peak_size_adv(abs_signature, set_size, permutations, int(seed)))\n                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/polina/.local/lib/python3.11/site-packages/blitzgsea/__init__.py\", line 153, in get_peak_size_adv\n    es_val = enrichment_score_null(abs_signature, hit_indicator.copy(), number_hits)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/polina/.local/lib/python3.11/site-packages/blitzgsea/__init__.py\", line 111, in enrichment_score_null\n    norm_no_hit = 1.0 / number_miss\n                  ~~~~^~~~~~~~~~~~~\nZeroDivisionError: float division by zero\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 10\u001b[0m\n\u001b[1;32m      2\u001b[0m output_gcs_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgs://ot-team/polina/pathwaganda/gsea_results/non_oncology\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m libraries \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKEGG_2021_Human\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      5\u001b[0m             \u001b[38;5;66;03m# \"Reactome_Pathways_2024\",\u001b[39;00m\n\u001b[1;32m      6\u001b[0m             \u001b[38;5;66;03m# \"WikiPathways_2024_Human\", \u001b[39;00m\n\u001b[1;32m      7\u001b[0m             \u001b[38;5;66;03m# \"GO_Biological_Process_2025\"]\u001b[39;00m\n\u001b[1;32m      8\u001b[0m             ]\n\u001b[0;32m---> 10\u001b[0m \u001b[43mblitzgsea_with_fileprep_spark\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_gcs_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_gcs_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlibraries\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 67\u001b[0m, in \u001b[0;36mblitzgsea_with_fileprep_spark\u001b[0;34m(input_gcs_dir, output_gcs_dir, libraries, fdr_cutoff)\u001b[0m\n\u001b[1;32m     64\u001b[0m library \u001b[38;5;241m=\u001b[39m library_sets[lib]\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# Run GSEA\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mblitz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgsea\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlibrary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocesses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m sig \u001b[38;5;241m=\u001b[39m result[result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFDR\u001b[39m\u001b[38;5;124m\"\u001b[39m, result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfdr\u001b[39m\u001b[38;5;124m\"\u001b[39m, result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfdr_qvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m))) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m fdr_cutoff]\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sig\u001b[38;5;241m.\u001b[39mempty:\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/blitzgsea/__init__.py:354\u001b[0m, in \u001b[0;36mgsea\u001b[0;34m(signature, library, permutations, anchors, min_size, max_size, processes, plotting, verbose, progress, symmetric, signature_cache, kl_threshold, kl_bins, shared_null, seed, add_noise, accuracy, deep_accuracy, center, ks_disable)\u001b[0m\n\u001b[1;32m    352\u001b[0m     f_alpha_pos, f_beta_pos, f_pos_ratio, f_alpha_neg, f_beta_neg, ks_pos, ks_neg \u001b[38;5;241m=\u001b[39m pdf_cache[sig_hash][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 354\u001b[0m     f_alpha_pos, f_beta_pos, f_pos_ratio, f_alpha_neg, f_beta_neg, ks_pos, ks_neg \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mabs_signature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlibrary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpermutations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpermutations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcalibration_anchors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manchors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocesses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocesses\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msymmetric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msymmetric\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplotting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mplotting\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mks_disable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mks_disable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    355\u001b[0m     xv, pdf \u001b[38;5;241m=\u001b[39m create_pdf(np\u001b[38;5;241m.\u001b[39marray(signature[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mv\u001b[39m\u001b[38;5;124m\"\u001b[39m]), kl_bins)\n\u001b[1;32m    356\u001b[0m     pdf_cache[sig_hash] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    357\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m: xv,\n\u001b[1;32m    358\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpdf\u001b[39m\u001b[38;5;124m\"\u001b[39m: pdf,\n\u001b[1;32m    359\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: (f_alpha_pos, f_beta_pos, f_pos_ratio, f_alpha_neg, f_beta_neg, ks_pos, ks_neg),\n\u001b[1;32m    360\u001b[0m     }\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/blitzgsea/__init__.py:183\u001b[0m, in \u001b[0;36mestimate_parameters\u001b[0;34m(signature, abs_signature, signature_map, library, permutations, max_size, symmetric, calibration_anchors, plotting, processes, verbose, progress, seed, ks_disable)\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m multiprocessing\u001b[38;5;241m.\u001b[39mPool(processes) \u001b[38;5;28;01mas\u001b[39;00m pool:\n\u001b[1;32m    182\u001b[0m         args \u001b[38;5;241m=\u001b[39m [(signature, abs_signature, signature_map, xx, permutations, symmetric, \u001b[38;5;28mint\u001b[39m(seed\u001b[38;5;241m+\u001b[39mxx), ks_disable) \u001b[38;5;28;01mfor\u001b[39;00m xx \u001b[38;5;129;01min\u001b[39;00m anchor_set_sizes]\n\u001b[0;32m--> 183\u001b[0m         results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(tqdm(pool\u001b[38;5;241m.\u001b[39mimap(estimate_anchor_star, args), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m, total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(args), disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m progress))\n\u001b[1;32m    185\u001b[0m alpha_pos \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    186\u001b[0m beta_pos \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/site-packages/tqdm/std.py:1166\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[38;5;66;03m# If the bar is disabled, then just walk the iterable\u001b[39;00m\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;66;03m# (note: keep this check outside the loop for performance)\u001b[39;00m\n\u001b[1;32m   1165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable:\n\u001b[0;32m-> 1166\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1167\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1168\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/multiprocessing/pool.py:873\u001b[0m, in \u001b[0;36mIMapIterator.next\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m value\n\u001b[0;32m--> 873\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m value\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"
     ]
    }
   ],
   "source": [
    "input_gcs_dir = \"gs://ot-team/polina/pathwaganda/processed_diseases/non_oncology\"\n",
    "output_gcs_dir = \"gs://ot-team/polina/pathwaganda/gsea_results/non_oncology\"\n",
    "\n",
    "libraries = [\"KEGG_2021_Human\"\n",
    "            # \"Reactome_Pathways_2024\",\n",
    "            # \"WikiPathways_2024_Human\", \n",
    "            # \"GO_Biological_Process_2025\"]\n",
    "            ]\n",
    "\n",
    "blitzgsea_with_fileprep_spark(input_gcs_dir, output_gcs_dir, libraries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f4b00e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
