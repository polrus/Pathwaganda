{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e19fa0ca",
   "metadata": {},
   "source": [
    "# Target-disease genetic evidence from Open Targets Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7723f216",
   "metadata": {},
   "source": [
    "This code is meant to prepare ranked lists of genes for all diseases from Open Targets platform with amount of genetically supported genes (genetically and somatic mutations for oncological traits) >= 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b66823",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01margparse\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession, DataFrame, Window\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     col, countDistinct, row_number, \u001b[38;5;28msum\u001b[39m \u001b[38;5;28;01mas\u001b[39;00m spark_sum,\n\u001b[1;32m      5\u001b[0m     broadcast\n\u001b[1;32m      6\u001b[0m )\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgcsfs\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "from pyspark.sql import SparkSession, DataFrame, Window\n",
    "from pyspark.sql.functions import (\n",
    "    col, countDistinct, row_number, sum as spark_sum,\n",
    "    broadcast, array_contains\n",
    ")\n",
    "import gcsfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc6613fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    # \"ot_genetics_portal\": 1,\n",
    "    \"gwas_credible_sets\": 1,\n",
    "    \"gene_burden\": 1,\n",
    "    \"eva\": 1,\n",
    "    \"genomics_england\": 1,\n",
    "    \"gene2phenotype\": 1,\n",
    "    \"uniprot_literature\": 1,\n",
    "    \"uniprot_variants\": 1,\n",
    "    \"orphanet\": 1,\n",
    "    \"clingen\": 1,\n",
    "    \"cancer_gene_census\": 1,\n",
    "    \"intogen\": 1,\n",
    "    \"eva_somatic\": 1,\n",
    "    \"cancer_biomarkers\": 1,\n",
    "    \"chembl\": 1,\n",
    "    \"crispr_screen\": 1,\n",
    "    \"crispr\": 1,\n",
    "    \"slapenrich\": 0.5,\n",
    "    \"progeny\": 0.5,\n",
    "    \"reactome\": 1,\n",
    "    \"sysbio\": 0.5,\n",
    "    \"europepmc\": 0.2,\n",
    "    \"expression_atlas\": 0.2,\n",
    "    \"impc\": 0.2,\n",
    "    \"ot_crispr_validation\": 0.5,\n",
    "    \"ot_crispr\": 0.5,\n",
    "    \"encore\": 0.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed0c42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oncology MONDO code constant\n",
    "ONCOLOGY_ID = \"MONDO_0045024\"\n",
    "\n",
    "\n",
    "def process_oncology(\n",
    "    evidence_sour: DataFrame,\n",
    "    weights: dict,\n",
    "    disease_df: DataFrame,\n",
    "    output_dir: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Process oncology diseases (contain ONCOLOGY_ID) and include somatic_mutation evidence.\n",
    "    \"\"\"\n",
    "    spark = evidence_sour.sparkSession\n",
    "    # Broadcast weights for efficient lookup\n",
    "    weights_df = spark.createDataFrame(\n",
    "        list(weights.items()), schema=[\"datasourceId\", \"weight\"]\n",
    "    )\n",
    "\n",
    "    # Filter to oncology diseases and evidence types\n",
    "    ev = (\n",
    "        evidence_sour\n",
    "        .join(broadcast(disease_df), on=\"diseaseId\")\n",
    "        .filter(array_contains(col(\"therapeuticArea\"), ONCOLOGY_ID))\n",
    "        .filter(col(\"datatypeId\").rlike(\"genetic_association|animal_model|somatic_mutation\"))\n",
    "    )\n",
    "\n",
    "    # Keep diseases with >=500 unique genes\n",
    "    valid = (\n",
    "        ev.groupBy(\"diseaseId\")\n",
    "          .agg(countDistinct(\"approvedSymbol\").alias(\"nGenes\"))\n",
    "          .filter(col(\"nGenes\") >= 500)\n",
    "    )\n",
    "\n",
    "    # Join back valid diseases and compute weighted scores\n",
    "    df = (\n",
    "        ev.join(valid.select(\"diseaseId\"), on=\"diseaseId\")\n",
    "          .join(broadcast(weights_df), on=\"datasourceId\", how=\"left\")\n",
    "          .withColumn(\"score_weighted\", col(\"score\") * col(\"weight\"))\n",
    "    )\n",
    "\n",
    "    # First-level window aggregation: per (disease, datatype, gene, target)\n",
    "    win1 = Window.partitionBy(\n",
    "        \"diseaseId\", \"datatypeId\", \"approvedSymbol\", \"targetId\"\n",
    "    ).orderBy(col(\"score_weighted\").desc())\n",
    "\n",
    "    df1 = (\n",
    "        df.withColumn(\"rank1\", row_number().over(win1))\n",
    "          .withColumn(\"term1\", col(\"score_weighted\") / (col(\"rank1\") ** 2))\n",
    "          .groupBy(\"diseaseId\", \"datatypeId\", \"approvedSymbol\", \"targetId\")\n",
    "          .agg(spark_sum(\"term1\").alias(\"sourceSum\"))\n",
    "    )\n",
    "\n",
    "    # Second-level window aggregation: per (disease, gene, target)\n",
    "    win2 = Window.partitionBy(\n",
    "        \"diseaseId\", \"approvedSymbol\", \"targetId\"\n",
    "    ).orderBy(col(\"sourceSum\").desc())\n",
    "\n",
    "    df2 = (\n",
    "        df1.withColumn(\"rank2\", row_number().over(win2))\n",
    "           .withColumn(\"overallScore\", col(\"sourceSum\") / (col(\"rank2\") ** 2))\n",
    "           .filter(col(\"overallScore\").isNotNull())\n",
    "    )\n",
    "\n",
    "    # Write Parquet files partitioned by diseaseId under oncology subdir\n",
    "    df2.select(\"diseaseId\", \"approvedSymbol\", \"targetId\", \"overallScore\")\n",
    "       .repartition(\"diseaseId\")\n",
    "       .write.mode(\"overwrite\")\n",
    "       .partitionBy(\"diseaseId\")\n",
    "       .parquet(f\"{output_dir}/oncology\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d771fa2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_non_oncology(\n",
    "    evidence_sour: DataFrame,\n",
    "    weights: dict,\n",
    "    disease_df: DataFrame,\n",
    "    output_dir: str\n",
    "):\n",
    "    \"\"\"\n",
    "    Process non-oncology diseases (not containing ONCOLOGY_ID) and exclude somatic_mutation evidence.\n",
    "    \"\"\"\n",
    "    spark = evidence_sour.sparkSession\n",
    "    # Broadcast weights\n",
    "    weights_df = spark.createDataFrame(\n",
    "        list(weights.items()), schema=[\"datasourceId\", \"weight\"]\n",
    "    )\n",
    "\n",
    "    # Filter to non-oncology diseases and evidence types\n",
    "    ev = (\n",
    "        evidence_sour\n",
    "        .join(broadcast(disease_df), on=\"diseaseId\")\n",
    "        .filter(~array_contains(col(\"therapeuticArea\"), ONCOLOGY_ID))\n",
    "        .filter(col(\"datatypeId\").rlike(\"genetic_association|animal_model\"))\n",
    "    )\n",
    "\n",
    "    # Keep diseases with >=500 unique genes\n",
    "    valid = (\n",
    "        ev.groupBy(\"diseaseId\")\n",
    "          .agg(countDistinct(\"approvedSymbol\").alias(\"nGenes\"))\n",
    "          .filter(col(\"nGenes\") >= 500)\n",
    "    )\n",
    "\n",
    "    # Join back and compute weighted scores\n",
    "    df = (\n",
    "        ev.join(valid.select(\"diseaseId\"), on=\"diseaseId\")\n",
    "          .join(broadcast(weights_df), on=\"datasourceId\", how=\"left\")\n",
    "          .withColumn(\"score_weighted\", col(\"score\") * col(\"weight\"))\n",
    "    )\n",
    "\n",
    "    # First-level window aggregation\n",
    "    win1 = Window.partitionBy(\n",
    "        \"diseaseId\", \"datatypeId\", \"approvedSymbol\", \"targetId\"\n",
    "    ).orderBy(col(\"score_weighted\").desc())\n",
    "\n",
    "    df1 = (\n",
    "        df.withColumn(\"rank1\", row_number().over(win1))\n",
    "          .withColumn(\"term1\", col(\"score_weighted\") / (col(\"rank1\") ** 2))\n",
    "          .groupBy(\"diseaseId\", \"datatypeId\", \"approvedSymbol\", \"targetId\")\n",
    "          .agg(spark_sum(\"term1\").alias(\"sourceSum\"))\n",
    "    )\n",
    "\n",
    "    # Second-level window aggregation\n",
    "    win2 = Window.partitionBy(\n",
    "        \"diseaseId\", \"approvedSymbol\", \"targetId\"\n",
    "    ).orderBy(col(\"sourceSum\").desc())\n",
    "\n",
    "    df2 = (\n",
    "        df1.withColumn(\"rank2\", row_number().over(win2))\n",
    "           .withColumn(\"overallScore\", col(\"sourceSum\") / (col(\"rank2\") ** 2))\n",
    "           .filter(col(\"overallScore\").isNotNull())\n",
    "    )\n",
    "\n",
    "    # Write Parquet files partitioned by diseaseId under non_oncology subdir\n",
    "    df2.select(\"diseaseId\", \"approvedSymbol\", \"targetId\", \"overallScore\")\n",
    "       .repartition(\"diseaseId\")\n",
    "       .write.mode(\"overwrite\")\n",
    "       .partitionBy(\"diseaseId\")\n",
    "       .parquet(f\"{output_dir}/non_oncology\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ca5f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    parser = argparse.ArgumentParser(description=\"Process disease evidence by area.\")\n",
    "    parser.add_argument(\n",
    "        \"--evidence-path\", required=True,\n",
    "        help=\"GCS path to association_by_datasource_indirect parquet file\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--target-path\", required=True,\n",
    "        help=\"GCS path to target parquet file\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--disease-path\", required=True,\n",
    "        help=\"GCS path to disease-to-area parquet file\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output-dir\", required=True,\n",
    "        help=\"GCS output prefix for Parquet files\"\n",
    "    )\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    spark = SparkSession.builder.appName(\"ProcessDiseases\").getOrCreate()\n",
    "\n",
    "    # Load evidence and enrich with gene names\n",
    "    evidence = spark.read.parquet(args.evidence_path)\n",
    "    target_df = spark.read.parquet(args.target_path).select(\n",
    "        col(\"id\").alias(\"targetId\"), col(\"approvedSymbol\")\n",
    "    )\n",
    "    evidence = evidence.join(\n",
    "        broadcast(target_df), on=\"targetId\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    # Load disease-to-area mapping\n",
    "    disease_df = spark.read.parquet(args.disease_path).select(\n",
    "        col(\"id\").alias(\"diseaseId\"), col(\"therapeuticArea\")\n",
    "    )\n",
    "\n",
    "    # Define your data source weights mapping\n",
    "    weights = {\n",
    "        # \"datasourceA\": 1.0,\n",
    "        # \"datasourceB\": 0.5,\n",
    "        # ...\n",
    "    }\n",
    "\n",
    "    # Execute processing pipelines\n",
    "    process_oncology(evidence, weights, disease_df, args.output_dir)\n",
    "    process_non_oncology(evidence, weights, disease_df, args.output_dir)\n",
    "\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e00e498",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a360b49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
