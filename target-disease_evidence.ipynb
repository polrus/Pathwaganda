{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e19fa0ca",
   "metadata": {},
   "source": [
    "# Target-disease genetic evidence from Open Targets Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7723f216",
   "metadata": {},
   "source": [
    "This code is meant to prepare ranked lists of genes for all diseases from Open Targets platform with amount of genetically supported genes (genetically and somatic mutations for oncological traits) >= 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6b66823",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/08 15:32:11 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame, Window\n",
    "from pyspark.sql.functions import (\n",
    "    col, countDistinct, row_number, sum as spark_sum,\n",
    "    broadcast, array_contains\n",
    ")\n",
    "import gcsfs\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ProcessDiseasesNotebook\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df4da9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Oncology MONDO code constant\n",
    "ONCOLOGY_ID = \"MONDO_0045024\"\n",
    "\n",
    "\n",
    "# 1. Load inputs by specifying your GCS paths directly\n",
    "evidence_path = \"gs://open-targets-data-releases/25.06/output/association_by_datasource_indirect\"\n",
    "target_path   = \"gs://open-targets-data-releases/25.06/output/target\"\n",
    "disease_path  = \"gs://open-targets-data-releases/25.06/output/disease\"\n",
    "output_dir    = \"gs://ot-team/polina/pathwaganda/processed_diseases\"\n",
    "include_animal_models = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce9941e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/08 15:33:31 WARN YarnScheduler: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "ERROR:root:KeyboardInterrupt while sending command.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/miniconda3/lib/python3.11/socket.py\", line 706, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisease_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mprintSchema()\n",
      "File \u001b[0;32m/usr/lib/spark/python/pyspark/sql/readwriter.py:544\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    533\u001b[0m int96RebaseMode \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint96RebaseMode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m    535\u001b[0m     mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema,\n\u001b[1;32m    536\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     int96RebaseMode\u001b[38;5;241m=\u001b[39mint96RebaseMode,\n\u001b[1;32m    542\u001b[0m )\n\u001b[0;32m--> 544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1039\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mreadline()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/miniconda3/lib/python3.11/socket.py:706\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 706\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    708\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "spark.read.parquet(disease_path).printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed0c42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Read and prepare DataFrames\n",
    "\n",
    "evidence = spark.read.parquet(evidence_path)\n",
    "target_df = (\n",
    "    spark.read.parquet(target_path)\n",
    "         .select(col(\"id\").alias(\"targetId\"), col(\"approvedSymbol\"))\n",
    ")\n",
    "evidence = evidence.join(broadcast(target_df), on=\"targetId\", how=\"left\")\n",
    "\n",
    "disease_df = (\n",
    "    spark.read.parquet(disease_path)\n",
    "         .select(col(\"id\").alias(\"diseaseId\"), col(\"therapeuticArea\"))\n",
    ")\n",
    "\n",
    "# 3. Define your data source weights\n",
    "weights = {\n",
    "    # \"ot_genetics_portal\": 1,\n",
    "    \"gwas_credible_sets\": 1,\n",
    "    \"gene_burden\": 1,\n",
    "    \"eva\": 1,\n",
    "    \"genomics_england\": 1,\n",
    "    \"gene2phenotype\": 1,\n",
    "    \"uniprot_literature\": 1,\n",
    "    \"uniprot_variants\": 1,\n",
    "    \"orphanet\": 1,\n",
    "    \"clingen\": 1,\n",
    "    \"cancer_gene_census\": 1,\n",
    "    \"intogen\": 1,\n",
    "    \"eva_somatic\": 1,\n",
    "    \"cancer_biomarkers\": 1,\n",
    "    \"chembl\": 1,\n",
    "    \"crispr_screen\": 1,\n",
    "    \"crispr\": 1,\n",
    "    \"slapenrich\": 0.5,\n",
    "    \"progeny\": 0.5,\n",
    "    \"reactome\": 1,\n",
    "    \"sysbio\": 0.5,\n",
    "    \"europepmc\": 0.2,\n",
    "    \"expression_atlas\": 0.2,\n",
    "    \"impc\": 0.2,\n",
    "    \"ot_crispr_validation\": 0.5,\n",
    "    \"ot_crispr\": 0.5,\n",
    "    \"encore\": 0.5,\n",
    "}\n",
    "\n",
    "# 4. Build evidence type lists based on animal-model flag\n",
    "oncology_types     = [\"genetic_association\", \"somatic_mutation\"]\n",
    "non_oncology_types = [\"genetic_association\"]\n",
    "if include_animal_models:\n",
    "    oncology_types.append(\"animal_model\")\n",
    "    non_oncology_types.append(\"animal_model\")\n",
    "\n",
    "# 5. Define processing functions\n",
    "\n",
    "def _compute_scores(ev: DataFrame, evidence_types: list) -> DataFrame:\n",
    "    # Build regex from types\n",
    "    pattern = \"|\".join(evidence_types)\n",
    "    # Filter by types and by â‰¥500 genes\n",
    "    valid = (\n",
    "        ev.filter(col(\"datatypeId\").rlike(pattern))\n",
    "          .groupBy(\"diseaseId\")\n",
    "          .agg(countDistinct(\"approvedSymbol\").alias(\"nGenes\"))\n",
    "          .filter(col(\"nGenes\") >= 500)\n",
    "    )\n",
    "    weights_df = spark.createDataFrame(\n",
    "        list(weights.items()), schema=[\"datasourceId\", \"weight\"]\n",
    "    )\n",
    "    df = (\n",
    "        ev.join(valid.select(\"diseaseId\"), on=\"diseaseId\")\n",
    "          .join(broadcast(weights_df), on=\"datasourceId\", how=\"left\")\n",
    "          .withColumn(\"score_weighted\", col(\"score\") * col(\"weight\"))\n",
    "    )\n",
    "    # First-level window\n",
    "    win1 = Window.partitionBy(\"diseaseId\",\"datatypeId\",\"approvedSymbol\",\"targetId\").orderBy(col(\"score_weighted\").desc())\n",
    "    df1 = (\n",
    "        df.withColumn(\"rank1\", row_number().over(win1))\n",
    "          .withColumn(\"term1\", col(\"score_weighted\") / (col(\"rank1\")**2))\n",
    "          .groupBy(\"diseaseId\",\"datatypeId\",\"approvedSymbol\",\"targetId\")\n",
    "          .agg(spark_sum(\"term1\").alias(\"sourceSum\"))\n",
    "    )\n",
    "    # Second-level window\n",
    "    win2 = Window.partitionBy(\"diseaseId\",\"approvedSymbol\",\"targetId\").orderBy(col(\"sourceSum\").desc())\n",
    "    df2 = (\n",
    "        df1.withColumn(\"rank2\", row_number().over(win2))\n",
    "           .withColumn(\"overallScore\", col(\"sourceSum\") / (col(\"rank2\")**2))\n",
    "           .filter(col(\"overallScore\").isNotNull())\n",
    "    )\n",
    "    return df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e00e498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Process oncology\n",
    "\n",
    "ev_onc = (\n",
    "    evidence.join(broadcast(disease_df), on=\"diseaseId\")\n",
    "            .filter(array_contains(col(\"therapeuticArea\"), ONCOLOGY_ID))\n",
    ")\n",
    "result_onc = _compute_scores(ev_onc, oncology_types)\n",
    "# Write out\n",
    "result_onc.select(\"diseaseId\",\"approvedSymbol\",\"targetId\",\"overallScore\") \\\n",
    "           .repartition(\"diseaseId\") \\\n",
    "           .write.mode(\"overwrite\") \\\n",
    "           .partitionBy(\"diseaseId\") \\\n",
    "           .parquet(f\"{output_dir}/oncology\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a360b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Process non-oncology\n",
    "\n",
    "ev_non = (\n",
    "    evidence.join(broadcast(disease_df), on=\"diseaseId\")\n",
    "            .filter(~array_contains(col(\"therapeuticArea\"), ONCOLOGY_ID))\n",
    ")\n",
    "result_non = _compute_scores(ev_non, non_oncology_types)\n",
    "# Write out\n",
    "result_non.select(\"diseaseId\",\"approvedSymbol\",\"targetId\",\"overallScore\") \\\n",
    "           .repartition(\"diseaseId\") \\\n",
    "           .write.mode(\"overwrite\") \\\n",
    "           .partitionBy(\"diseaseId\") \\\n",
    "           .parquet(f\"{output_dir}/non_oncology\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
